{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(27)\n",
    "\n",
    "# Example tensor of shape (1, 1, H, W)\n",
    "DIM = 4\n",
    "W = torch.rand(DIM, DIM)\n",
    "P1 = torch.rand(int(DIM/2), DIM)\n",
    "P2 = torch.rand(DIM, int(DIM/2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_path = \"models/512.pt\"\n",
    "checkpoint = torch.load(load_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.blocks.block_6.block.residual_attention.layer.attention.output_projection.output_projection.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_layer.layers.0.weight',\n",
       " 'embedding_layer.layers.1.layer.weight',\n",
       " 'encoder.blocks.block_0.block.residual_attention.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_0.block.residual_attention.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_0.block.residual_attention.layer.attention.input_projection.weight',\n",
       " 'encoder.blocks.block_0.block.residual_attention.layer.attention.output_projection.weight',\n",
       " 'encoder.blocks.block_0.block.residual_feedforward.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_0.block.residual_feedforward.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_0.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.weight',\n",
       " 'encoder.blocks.block_0.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.bias',\n",
       " 'encoder.blocks.block_0.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.weight',\n",
       " 'encoder.blocks.block_0.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.bias',\n",
       " 'encoder.blocks.block_1.block.residual_attention.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_1.block.residual_attention.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_1.block.residual_attention.layer.attention.input_projection.weight',\n",
       " 'encoder.blocks.block_1.block.residual_attention.layer.attention.output_projection.weight',\n",
       " 'encoder.blocks.block_1.block.residual_feedforward.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_1.block.residual_feedforward.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_1.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.weight',\n",
       " 'encoder.blocks.block_1.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.bias',\n",
       " 'encoder.blocks.block_1.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.weight',\n",
       " 'encoder.blocks.block_1.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.bias',\n",
       " 'encoder.blocks.block_2.block.residual_attention.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_2.block.residual_attention.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_2.block.residual_attention.layer.attention.input_projection.weight',\n",
       " 'encoder.blocks.block_2.block.residual_attention.layer.attention.output_projection.weight',\n",
       " 'encoder.blocks.block_2.block.residual_feedforward.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_2.block.residual_feedforward.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_2.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.weight',\n",
       " 'encoder.blocks.block_2.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.bias',\n",
       " 'encoder.blocks.block_2.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.weight',\n",
       " 'encoder.blocks.block_2.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.bias',\n",
       " 'encoder.blocks.block_3.block.residual_attention.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_3.block.residual_attention.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_3.block.residual_attention.layer.attention.input_projection.weight',\n",
       " 'encoder.blocks.block_3.block.residual_attention.layer.attention.output_projection.weight',\n",
       " 'encoder.blocks.block_3.block.residual_feedforward.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_3.block.residual_feedforward.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_3.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.weight',\n",
       " 'encoder.blocks.block_3.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.bias',\n",
       " 'encoder.blocks.block_3.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.weight',\n",
       " 'encoder.blocks.block_3.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.bias',\n",
       " 'encoder.blocks.block_4.block.residual_attention.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_4.block.residual_attention.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_4.block.residual_attention.layer.attention.input_projection.weight',\n",
       " 'encoder.blocks.block_4.block.residual_attention.layer.attention.output_projection.weight',\n",
       " 'encoder.blocks.block_4.block.residual_feedforward.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_4.block.residual_feedforward.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_4.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.weight',\n",
       " 'encoder.blocks.block_4.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.bias',\n",
       " 'encoder.blocks.block_4.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.weight',\n",
       " 'encoder.blocks.block_4.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.bias',\n",
       " 'encoder.blocks.block_5.block.residual_attention.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_5.block.residual_attention.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_5.block.residual_attention.layer.attention.input_projection.weight',\n",
       " 'encoder.blocks.block_5.block.residual_attention.layer.attention.output_projection.weight',\n",
       " 'encoder.blocks.block_5.block.residual_feedforward.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_5.block.residual_feedforward.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_5.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.weight',\n",
       " 'encoder.blocks.block_5.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.bias',\n",
       " 'encoder.blocks.block_5.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.weight',\n",
       " 'encoder.blocks.block_5.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.bias',\n",
       " 'encoder.blocks.block_6.block.residual_attention.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_6.block.residual_attention.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_6.block.residual_attention.layer.attention.input_projection.weight',\n",
       " 'encoder.blocks.block_6.block.residual_attention.layer.attention.output_projection.weight',\n",
       " 'encoder.blocks.block_6.block.residual_feedforward.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_6.block.residual_feedforward.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_6.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.weight',\n",
       " 'encoder.blocks.block_6.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.bias',\n",
       " 'encoder.blocks.block_6.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.weight',\n",
       " 'encoder.blocks.block_6.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.bias',\n",
       " 'encoder.blocks.block_7.block.residual_attention.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_7.block.residual_attention.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_7.block.residual_attention.layer.attention.input_projection.weight',\n",
       " 'encoder.blocks.block_7.block.residual_attention.layer.attention.output_projection.weight',\n",
       " 'encoder.blocks.block_7.block.residual_feedforward.layer.pre_norm.weight',\n",
       " 'encoder.blocks.block_7.block.residual_feedforward.layer.pre_norm.bias',\n",
       " 'encoder.blocks.block_7.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.weight',\n",
       " 'encoder.blocks.block_7.block.residual_feedforward.layer.feedforward.logging_ff_pre_relu.bias',\n",
       " 'encoder.blocks.block_7.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.weight',\n",
       " 'encoder.blocks.block_7.block.residual_feedforward.layer.feedforward.logging_ff_post_relu.bias',\n",
       " 'head.weight']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5872, 1.1626, 1.6426, 0.9742],\n",
       "        [0.4341, 0.6854, 0.8121, 0.5155]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1@W#@P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM1 = 12\n",
    "DIM2 = 4\n",
    "W = torch.rand(DIM1, DIM2)\n",
    "P1 = torch.rand(int(DIM1/2), DIM1)\n",
    "P2 = torch.rand(DIM2, int(DIM2/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 4])\n",
      "torch.Size([6, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5.6443, 5.0776],\n",
       "        [6.5575, 5.7987],\n",
       "        [7.7880, 7.0714],\n",
       "        [7.1346, 6.4522],\n",
       "        [6.2099, 5.7591],\n",
       "        [7.6796, 6.9450]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = P1@W@P2\n",
    "print(W.shape)\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = 512\n",
    "yb = 2048\n",
    "xs = 256\n",
    "ys = 1024\n",
    "W = torch.rand(xb, yb)\n",
    "P1 = torch.rand(xs, xb)\n",
    "P2 = torch.rand(yb, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 2048])\n",
      "torch.Size([256, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[130796.1172, 128785.4453, 129663.6562,  ..., 128499.3594,\n",
       "         130298.6484, 129971.3906],\n",
       "        [133012.6562, 130936.7422, 131843.7656,  ..., 130729.7422,\n",
       "         132476.1875, 132249.2500],\n",
       "        [129721.0234, 127680.9062, 128570.0000,  ..., 127446.5781,\n",
       "         129220.4062, 128920.0234],\n",
       "        ...,\n",
       "        [130213.5469, 128230.0234, 129112.5391,  ..., 127997.6562,\n",
       "         129677.8828, 129393.8047],\n",
       "        [130767.9375, 128707.3594, 129624.9375,  ..., 128489.2734,\n",
       "         130267.7734, 129968.9062],\n",
       "        [132819.5938, 130733.5156, 131616.2812,  ..., 130468.2188,\n",
       "         132302.6875, 132004.3594]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = P1@W@P2\n",
    "print(W.shape)\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=8, bias=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lizrd.core.misc import Linear\n",
    "\n",
    "\n",
    "l = Linear(\n",
    "    4, #xs\n",
    "    8, #xb\n",
    "    init_type = \"kaiming_uniform\",\n",
    "    init_scale = 1.0\n",
    ")\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0854, -0.6050,  0.3916, -0.5043],\n",
       "        [ 0.4044, -0.6925, -0.1916,  0.1707],\n",
       "        [ 0.7449, -0.3332,  0.7467,  0.4768],\n",
       "        [ 0.6021,  0.2765,  0.4113,  0.6645],\n",
       "        [-0.1919, -0.2049,  0.1844, -0.7438],\n",
       "        [ 0.4192, -0.2509, -0.2455,  0.0148],\n",
       "        [ 0.7097, -0.5523,  0.6878, -0.3706],\n",
       "        [-0.5385,  0.4877,  0.6276, -0.7681]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 27\u001b[0m\n\u001b[1;32m     10\u001b[0m init_scale\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     11\u001b[0m block_modules \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFF\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: ProjectedFeedForward(\n\u001b[1;32m     13\u001b[0m             dmodel, dff, projected_dmodel, projected_dff, init_type\u001b[38;5;241m=\u001b[39minit_type, init_scale\u001b[38;5;241m=\u001b[39m init_scale\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         )\n\u001b[1;32m     25\u001b[0m }\n\u001b[0;32m---> 27\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in case of  DDP/FSDP, we initialize the model on CPU and move it to the GPU later\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mddp_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsdp_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsdp_param_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsdp_mixed_precision_ignore_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsdp_offload_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsdp_min_num_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation_checkpointing_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_logging_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsdp_modules_to_wrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_classes_from_module_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEmbeddingLayer,PredictionHead,TransformerBlock\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscproj/cluster/llm-random/lizrd/train/train_utils.py:81\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(max_length, vocab_size, block_modules, dm, n_blocks, device, init_type, init_scale, ddp_enabled, fsdp_enabled, fsdp_param_precision, fsdp_mixed_precision_ignore_classes, fsdp_offload_params, fsdp_min_num_params, fsdp_modules_to_wrap, activation_checkpointing_modules, is_logging_process, local_rank, model_fragmentation, residual_fn, include_positional_embedding, checkpoint, projected_distillation)\u001b[0m\n\u001b[1;32m     79\u001b[0m     model \u001b[38;5;241m=\u001b[39m wrap_in_ddp(module\u001b[38;5;241m=\u001b[39mmodel, local_rank\u001b[38;5;241m=\u001b[39mlocal_rank)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fsdp_enabled:\n\u001b[0;32m---> 81\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_in_fsdp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparam_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsdp_param_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmixed_precision_ignored_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsdp_mixed_precision_ignore_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsdp_offload_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_num_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsdp_min_num_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodules_to_wrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsdp_modules_to_wrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_logging_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_logging_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m activation_checkpointing_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     check_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, activation_checkpointing_modules)\n",
      "File \u001b[0;32m~/vscproj/cluster/llm-random/lizrd/core/distributed.py:46\u001b[0m, in \u001b[0;36mwrap_in_fsdp\u001b[0;34m(module, local_rank, param_precision, cast_inputs, mixed_precision_ignored_classes, offload_params, print_model, min_num_params, modules_to_wrap, is_logging_process)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     wrap_policy \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     41\u001b[0m         partial(size_based_auto_wrap_policy, min_num_params\u001b[38;5;241m=\u001b[39mmin_num_params)\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m min_num_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m size_based_auto_wrap_policy\n\u001b[1;32m     44\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m \u001b[43mFSDP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43msharding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mShardingStrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFULL_SHARD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMixedPrecision\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparam_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduce_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_forward_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_classes_to_ignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixed_precision_ignored_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu_offload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCPUOffload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffload_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_wrap_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrap_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_model \u001b[38;5;129;01mand\u001b[39;00m is_logging_process:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------- MODEL AFTER WRAPPING IN FSDP -------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrandom/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:439\u001b[0m, in \u001b[0;36mFullyShardedDataParallel.__init__\u001b[0;34m(self, module, process_group, sharding_strategy, cpu_offload, auto_wrap_policy, backward_prefetch, mixed_precision, ignored_modules, param_init_fn, device_id, sync_module_states, forward_prefetch, limit_all_gathers, use_orig_params, ignored_states)\u001b[0m\n\u001b[1;32m    432\u001b[0m _annotate_modules_for_dynamo(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignored_modules, use_orig_params)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Initializes self.process_group, along with rank and world size. This will\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# also set another attribute, _inter_node_pg, to control the process group\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# over which sharding occurs, if sharding_strategy is {HYBRID_SHARD, _HYBRID_SHARD_ZERO2}.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# Note that this is done before auto_wrapping, so that child FSDP modules simply pick up\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# the same process group state as the root FSDP module.\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m \u001b[43m_init_process_group_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_wrap_policy\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_wrap_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     root_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess_group\u001b[39m\u001b[38;5;124m\"\u001b[39m: process_group,\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msharding_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m: sharding_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignored_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignored_params,\n\u001b[1;32m    456\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrandom/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:114\u001b[0m, in \u001b[0;36m_init_process_group_state\u001b[0;34m(state, process_group, sharding_strategy, policy)\u001b[0m\n\u001b[1;32m    111\u001b[0m     state \u001b[38;5;241m=\u001b[39m _init_process_group_state_for_hybrid_shard(state, process_group)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     state\u001b[38;5;241m.\u001b[39mprocess_group \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 114\u001b[0m         process_group \u001b[38;5;28;01mif\u001b[39;00m process_group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m state\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mprocess_group\u001b[38;5;241m.\u001b[39mrank()\n\u001b[1;32m    117\u001b[0m state\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mprocess_group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrandom/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:940\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;124;03mGetting the default process group created by init_process_group\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[0;32m--> 940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from lizrd.train.train_utils import get_model\n",
    "from research.conditional.utils.model_utils import get_classes_from_module_names, get_ff_layer\n",
    "from research.projected_distillation.llm import ProjectedAttention, ProjectedFeedForward\n",
    "dmodel = 256\n",
    "dff = 1024\n",
    "projected_dmodel = dmodel*2\n",
    "projected_dff = dff*2\n",
    "n_blocks = 8\n",
    "init_type = \"kaiming_uniform\"\n",
    "init_scale= 1.0\n",
    "block_modules = {\n",
    "    \"FF\": lambda: ProjectedFeedForward(\n",
    "            dmodel, dff, projected_dmodel, projected_dff, init_type=init_type, init_scale= init_scale\n",
    "        ),\n",
    "    \"Attention\": lambda: ProjectedAttention(\n",
    "            dmodel=dmodel,\n",
    "            projected_dmodel=projected_dmodel,\n",
    "            heads=8,\n",
    "            causal=True,\n",
    "            dhead=None,\n",
    "            flash=False,\n",
    "            init_type=init_type,\n",
    "            init_scale=init_scale,\n",
    "        )\n",
    "}\n",
    "\n",
    "model = get_model(\n",
    "        max_length=256,\n",
    "        vocab_size=50048,\n",
    "        block_modules=block_modules,\n",
    "        dm=dmodel,\n",
    "        n_blocks=n_blocks,\n",
    "        device=(\n",
    "            torch.device(\"cuda\")\n",
    "        ),  # in case of  DDP/FSDP, we initialize the model on CPU and move it to the GPU later\n",
    "        init_type=init_type,\n",
    "        init_scale=init_scale,\n",
    "        ddp_enabled=None,\n",
    "        fsdp_enabled=True,\n",
    "        fsdp_param_precision=None,\n",
    "        fsdp_mixed_precision_ignore_classes=None,\n",
    "        fsdp_offload_params=None,\n",
    "        fsdp_min_num_params=None,\n",
    "        activation_checkpointing_modules=None,\n",
    "        is_logging_process=True,\n",
    "        fsdp_modules_to_wrap=get_classes_from_module_names(\"EmbeddingLayer,PredictionHead,TransformerBlock\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_layer.layers.0.weight\n",
      "embedding_layer.layers.1.layer.weight\n",
      "encoder.blocks.block_0.block.residual_FF.layer.pre_norm.weight\n",
      "encoder.blocks.block_0.block.residual_FF.layer.pre_norm.bias\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.weight\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.bias\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_pre_relu.weight\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_pre_relu.bias\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.weight\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.bias\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_post_relu_p21.weight\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_post_relu_p21.bias\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_post_relu.weight\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_post_relu.bias\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_post_relu_p22.weight\n",
      "encoder.blocks.block_0.block.residual_FF.layer.FF.logging_ff_post_relu_p22.bias\n",
      "encoder.blocks.block_0.block.residual_Attention.layer.pre_norm.weight\n",
      "encoder.blocks.block_0.block.residual_Attention.layer.pre_norm.bias\n",
      "encoder.blocks.block_0.block.residual_Attention.layer.Attention.input_projection.0.weight\n",
      "encoder.blocks.block_0.block.residual_Attention.layer.Attention.input_projection.1.weight\n",
      "encoder.blocks.block_0.block.residual_Attention.layer.Attention.input_projection.2.weight\n",
      "encoder.blocks.block_0.block.residual_Attention.layer.Attention.output_projection.0.weight\n",
      "encoder.blocks.block_0.block.residual_Attention.layer.Attention.output_projection.1.weight\n",
      "encoder.blocks.block_0.block.residual_Attention.layer.Attention.output_projection.2.weight\n",
      "encoder.blocks.block_1.block.residual_FF.layer.pre_norm.weight\n",
      "encoder.blocks.block_1.block.residual_FF.layer.pre_norm.bias\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.weight\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.bias\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_pre_relu.weight\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_pre_relu.bias\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.weight\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.bias\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_post_relu_p21.weight\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_post_relu_p21.bias\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_post_relu.weight\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_post_relu.bias\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_post_relu_p22.weight\n",
      "encoder.blocks.block_1.block.residual_FF.layer.FF.logging_ff_post_relu_p22.bias\n",
      "encoder.blocks.block_1.block.residual_Attention.layer.pre_norm.weight\n",
      "encoder.blocks.block_1.block.residual_Attention.layer.pre_norm.bias\n",
      "encoder.blocks.block_1.block.residual_Attention.layer.Attention.input_projection.0.weight\n",
      "encoder.blocks.block_1.block.residual_Attention.layer.Attention.input_projection.1.weight\n",
      "encoder.blocks.block_1.block.residual_Attention.layer.Attention.input_projection.2.weight\n",
      "encoder.blocks.block_1.block.residual_Attention.layer.Attention.output_projection.0.weight\n",
      "encoder.blocks.block_1.block.residual_Attention.layer.Attention.output_projection.1.weight\n",
      "encoder.blocks.block_1.block.residual_Attention.layer.Attention.output_projection.2.weight\n",
      "encoder.blocks.block_2.block.residual_FF.layer.pre_norm.weight\n",
      "encoder.blocks.block_2.block.residual_FF.layer.pre_norm.bias\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.weight\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.bias\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_pre_relu.weight\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_pre_relu.bias\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.weight\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.bias\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_post_relu_p21.weight\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_post_relu_p21.bias\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_post_relu.weight\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_post_relu.bias\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_post_relu_p22.weight\n",
      "encoder.blocks.block_2.block.residual_FF.layer.FF.logging_ff_post_relu_p22.bias\n",
      "encoder.blocks.block_2.block.residual_Attention.layer.pre_norm.weight\n",
      "encoder.blocks.block_2.block.residual_Attention.layer.pre_norm.bias\n",
      "encoder.blocks.block_2.block.residual_Attention.layer.Attention.input_projection.0.weight\n",
      "encoder.blocks.block_2.block.residual_Attention.layer.Attention.input_projection.1.weight\n",
      "encoder.blocks.block_2.block.residual_Attention.layer.Attention.input_projection.2.weight\n",
      "encoder.blocks.block_2.block.residual_Attention.layer.Attention.output_projection.0.weight\n",
      "encoder.blocks.block_2.block.residual_Attention.layer.Attention.output_projection.1.weight\n",
      "encoder.blocks.block_2.block.residual_Attention.layer.Attention.output_projection.2.weight\n",
      "encoder.blocks.block_3.block.residual_FF.layer.pre_norm.weight\n",
      "encoder.blocks.block_3.block.residual_FF.layer.pre_norm.bias\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.weight\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.bias\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_pre_relu.weight\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_pre_relu.bias\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.weight\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.bias\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_post_relu_p21.weight\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_post_relu_p21.bias\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_post_relu.weight\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_post_relu.bias\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_post_relu_p22.weight\n",
      "encoder.blocks.block_3.block.residual_FF.layer.FF.logging_ff_post_relu_p22.bias\n",
      "encoder.blocks.block_3.block.residual_Attention.layer.pre_norm.weight\n",
      "encoder.blocks.block_3.block.residual_Attention.layer.pre_norm.bias\n",
      "encoder.blocks.block_3.block.residual_Attention.layer.Attention.input_projection.0.weight\n",
      "encoder.blocks.block_3.block.residual_Attention.layer.Attention.input_projection.1.weight\n",
      "encoder.blocks.block_3.block.residual_Attention.layer.Attention.input_projection.2.weight\n",
      "encoder.blocks.block_3.block.residual_Attention.layer.Attention.output_projection.0.weight\n",
      "encoder.blocks.block_3.block.residual_Attention.layer.Attention.output_projection.1.weight\n",
      "encoder.blocks.block_3.block.residual_Attention.layer.Attention.output_projection.2.weight\n",
      "encoder.blocks.block_4.block.residual_FF.layer.pre_norm.weight\n",
      "encoder.blocks.block_4.block.residual_FF.layer.pre_norm.bias\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.weight\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.bias\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_pre_relu.weight\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_pre_relu.bias\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.weight\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.bias\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_post_relu_p21.weight\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_post_relu_p21.bias\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_post_relu.weight\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_post_relu.bias\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_post_relu_p22.weight\n",
      "encoder.blocks.block_4.block.residual_FF.layer.FF.logging_ff_post_relu_p22.bias\n",
      "encoder.blocks.block_4.block.residual_Attention.layer.pre_norm.weight\n",
      "encoder.blocks.block_4.block.residual_Attention.layer.pre_norm.bias\n",
      "encoder.blocks.block_4.block.residual_Attention.layer.Attention.input_projection.0.weight\n",
      "encoder.blocks.block_4.block.residual_Attention.layer.Attention.input_projection.1.weight\n",
      "encoder.blocks.block_4.block.residual_Attention.layer.Attention.input_projection.2.weight\n",
      "encoder.blocks.block_4.block.residual_Attention.layer.Attention.output_projection.0.weight\n",
      "encoder.blocks.block_4.block.residual_Attention.layer.Attention.output_projection.1.weight\n",
      "encoder.blocks.block_4.block.residual_Attention.layer.Attention.output_projection.2.weight\n",
      "encoder.blocks.block_5.block.residual_FF.layer.pre_norm.weight\n",
      "encoder.blocks.block_5.block.residual_FF.layer.pre_norm.bias\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.weight\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.bias\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_pre_relu.weight\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_pre_relu.bias\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.weight\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.bias\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_post_relu_p21.weight\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_post_relu_p21.bias\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_post_relu.weight\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_post_relu.bias\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_post_relu_p22.weight\n",
      "encoder.blocks.block_5.block.residual_FF.layer.FF.logging_ff_post_relu_p22.bias\n",
      "encoder.blocks.block_5.block.residual_Attention.layer.pre_norm.weight\n",
      "encoder.blocks.block_5.block.residual_Attention.layer.pre_norm.bias\n",
      "encoder.blocks.block_5.block.residual_Attention.layer.Attention.input_projection.0.weight\n",
      "encoder.blocks.block_5.block.residual_Attention.layer.Attention.input_projection.1.weight\n",
      "encoder.blocks.block_5.block.residual_Attention.layer.Attention.input_projection.2.weight\n",
      "encoder.blocks.block_5.block.residual_Attention.layer.Attention.output_projection.0.weight\n",
      "encoder.blocks.block_5.block.residual_Attention.layer.Attention.output_projection.1.weight\n",
      "encoder.blocks.block_5.block.residual_Attention.layer.Attention.output_projection.2.weight\n",
      "encoder.blocks.block_6.block.residual_FF.layer.pre_norm.weight\n",
      "encoder.blocks.block_6.block.residual_FF.layer.pre_norm.bias\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.weight\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.bias\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_pre_relu.weight\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_pre_relu.bias\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.weight\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.bias\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_post_relu_p21.weight\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_post_relu_p21.bias\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_post_relu.weight\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_post_relu.bias\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_post_relu_p22.weight\n",
      "encoder.blocks.block_6.block.residual_FF.layer.FF.logging_ff_post_relu_p22.bias\n",
      "encoder.blocks.block_6.block.residual_Attention.layer.pre_norm.weight\n",
      "encoder.blocks.block_6.block.residual_Attention.layer.pre_norm.bias\n",
      "encoder.blocks.block_6.block.residual_Attention.layer.Attention.input_projection.0.weight\n",
      "encoder.blocks.block_6.block.residual_Attention.layer.Attention.input_projection.1.weight\n",
      "encoder.blocks.block_6.block.residual_Attention.layer.Attention.input_projection.2.weight\n",
      "encoder.blocks.block_6.block.residual_Attention.layer.Attention.output_projection.0.weight\n",
      "encoder.blocks.block_6.block.residual_Attention.layer.Attention.output_projection.1.weight\n",
      "encoder.blocks.block_6.block.residual_Attention.layer.Attention.output_projection.2.weight\n",
      "encoder.blocks.block_7.block.residual_FF.layer.pre_norm.weight\n",
      "encoder.blocks.block_7.block.residual_FF.layer.pre_norm.bias\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.weight\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_pre_relu_p11.bias\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_pre_relu.weight\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_pre_relu.bias\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.weight\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_pre_relu_p12.bias\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_post_relu_p21.weight\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_post_relu_p21.bias\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_post_relu.weight\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_post_relu.bias\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_post_relu_p22.weight\n",
      "encoder.blocks.block_7.block.residual_FF.layer.FF.logging_ff_post_relu_p22.bias\n",
      "encoder.blocks.block_7.block.residual_Attention.layer.pre_norm.weight\n",
      "encoder.blocks.block_7.block.residual_Attention.layer.pre_norm.bias\n",
      "encoder.blocks.block_7.block.residual_Attention.layer.Attention.input_projection.0.weight\n",
      "encoder.blocks.block_7.block.residual_Attention.layer.Attention.input_projection.1.weight\n",
      "encoder.blocks.block_7.block.residual_Attention.layer.Attention.input_projection.2.weight\n",
      "encoder.blocks.block_7.block.residual_Attention.layer.Attention.output_projection.0.weight\n",
      "encoder.blocks.block_7.block.residual_Attention.layer.Attention.output_projection.1.weight\n",
      "encoder.blocks.block_7.block.residual_Attention.layer.Attention.output_projection.2.weight\n",
      "head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrandom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
