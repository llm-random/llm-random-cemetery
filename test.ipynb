{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch_size_per_gpu: 24\n",
      "num_batch_chunks: 1\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "    'batch_size': 777,\n",
    "    'n_devices': 4,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "}\n",
    "\n",
    "current_batch_size_per_gpu = d[\"batch_size\"] // (\n",
    "    d[\"gradient_accumulation_steps\"] * d[\"n_devices\"]\n",
    ")  # This value assures that the num_batch_chunks is 1\n",
    "\n",
    "num_batch_chunks = max(\n",
    "    d[\"gradient_accumulation_steps\"]\n",
    "    // (d[\"batch_size\"] // (current_batch_size_per_gpu * d[\"n_devices\"])),\n",
    "    1,\n",
    ")\n",
    "\n",
    "print(f\"current_batch_size_per_gpu: {current_batch_size_per_gpu}\")\n",
    "print(f\"num_batch_chunks: {num_batch_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: [152, 457]\n",
      "tokens_current: 9961472\n",
      "tokens_current: 49938432\n",
      "1266\n"
     ]
    }
   ],
   "source": [
    "def convert_transition_points_in_tokens_to_steps(\n",
    "    transition_points_in_tokens: list[float], batch_sizes: list[int], seq_len: int\n",
    "):\n",
    "    transition_points_in_steps = []\n",
    "    transition_points_in_tokens = [p * 1e9 for p in transition_points_in_tokens]\n",
    "    tokens_per_step_list = [batch_size * seq_len for batch_size in batch_sizes]\n",
    "    steps_prev = tokens_prev = 0\n",
    "\n",
    "    for point, tokens_per_step in zip(\n",
    "        transition_points_in_tokens, tokens_per_step_list\n",
    "    ):\n",
    "        tokens_to_transition = point - tokens_prev\n",
    "        steps_to_transition = tokens_to_transition / tokens_per_step\n",
    "        point_in_steps = steps_prev + steps_to_transition\n",
    "\n",
    "        transition_points_in_steps.append(int(point_in_steps))\n",
    "\n",
    "        tokens_prev = point\n",
    "        steps_prev = point_in_steps\n",
    "    return transition_points_in_steps\n",
    "\n",
    "def convert_tokens_to_steps(\n",
    "    tokens: int,  # in bilions\n",
    "    seq_len: int,\n",
    "    target_batch_size: int,\n",
    "    transition_points: list[int] = None,\n",
    "    batch_sizes: list[int] = None,\n",
    "):\n",
    "    if transition_points is None:\n",
    "        return int((tokens) / (target_batch_size * seq_len))\n",
    "    \n",
    "    tokens\n",
    "\n",
    "    tokens_per_step_list = [batch_size * seq_len for batch_size in batch_sizes]\n",
    "    steps_prev = tokens_prev = 0\n",
    "\n",
    "    for point, tokens_per_step in zip(transition_points, tokens_per_step_list):\n",
    "        steps_to_transition = point - steps_prev\n",
    "        tokens_to_trasition = steps_to_transition * tokens_per_step\n",
    "        tokens_current = tokens_prev + tokens_to_trasition\n",
    "\n",
    "        print(f'tokens_current: {tokens_current}')\n",
    "\n",
    "        if tokens < tokens_current:\n",
    "            return int(steps_prev + (tokens - tokens_prev) / tokens_per_step)\n",
    "\n",
    "        tokens_prev = tokens_current\n",
    "        steps_prev = point\n",
    "\n",
    "    # After all ramp-up intervals\n",
    "    return int(steps_prev + (tokens - tokens_prev) / (target_batch_size * seq_len))\n",
    "\n",
    "tokens = 0.262144\n",
    "target_batch_size = 512\n",
    "seq_len = 512\n",
    "tp = [0.01, 0.05]\n",
    "bsz = [128, 256]\n",
    "\n",
    "tp = convert_transition_points_in_tokens_to_steps(\n",
    "    transition_points_in_tokens=tp,\n",
    "    batch_sizes=bsz,\n",
    "    seq_len=seq_len\n",
    ")\n",
    "\n",
    "print(f'tp: {tp}')\n",
    "\n",
    "steps = convert_tokens_to_steps(\n",
    "    tokens=tokens * 1e9,\n",
    "    seq_len=seq_len,\n",
    "    target_batch_size=target_batch_size,\n",
    "    transition_points=tp,\n",
    "    batch_sizes=bsz,\n",
    ")\n",
    "\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "queisoqu9Hag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
