{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T20:18:00.609167Z",
     "start_time": "2024-07-30T20:17:58.748290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from research.universal.utils.model_utils import get_attention_layer, get_ff_layer\n",
    "from research.universal.utils.argparse import introduce_parser_arguments\n",
    "from lizrd.train.load_and_save_model import get_checkpoint_from_path\n",
    "from lizrd.text import tokenizers\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from typing import Callable, Optional\n",
    "import socket\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from lizrd.core import misc, llm\n",
    "from lizrd.core.llm import EmbeddingLayer, Parallel\n",
    "from lizrd.support.logging import get_current_logger, get_logger\n",
    "from lizrd.support.misc import (\n",
    "    get_argument_attributes,\n",
    "    get_n_learnable_parameters,\n",
    "    set_seed,\n",
    ")\n",
    "from lizrd.train.train_utils import (\n",
    "    get_model,\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = (\n",
    "        tokenizers.GPTTokenizer.VOCAB_SIZE\n",
    "    )\n",
    "\n",
    "block_modules = {}\n",
    "for module_name in [\"attention\", \"feedforward\"]:\n",
    "    if module_name == \"attention\":\n",
    "        block_modules[module_name] = lambda: llm.Attention(\n",
    "        dmodel=1024,\n",
    "        heads=16,\n",
    "        causal=True,\n",
    "        init_type=\"truncated_normal\",\n",
    "        init_scale=0.1,\n",
    "        dhead=64,\n",
    "        flash=False,\n",
    "    )\n",
    "    elif module_name == \"feedforward\":\n",
    "        block_modules[module_name] = lambda: llm.FeedForward(\n",
    "        1024, 2048, init_type=\"truncated_normal\", init_scale=0.1\n",
    "    )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown module name: {module_name}\")\n",
    "\n",
    "checkpoint = (\n",
    "        get_checkpoint_from_path(\"universal.pt\")\n",
    "    )\n",
    "\n",
    "model = get_model(\n",
    "        max_length=256,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        block_modules=block_modules,\n",
    "        dm=1024,\n",
    "        n_blocks=1,\n",
    "        device=(\n",
    "            torch.device(\"cpu\")\n",
    "        ),  # in case of  DDP/FSDP, we initialize the model on CPU and move it to the GPU later\n",
    "        init_type=\"truncated_normal\",\n",
    "        init_scale=0.1,\n",
    "        ddp_enabled=False,\n",
    "        fsdp_enabled=False,\n",
    "        fsdp_param_precision=None,\n",
    "        fsdp_mixed_precision_ignore_classes=[],\n",
    "        fsdp_offload_params=False,\n",
    "        fsdp_min_num_params=0,\n",
    "        fsdp_modules_to_wrap=None,\n",
    "        activation_checkpointing_modules=None,\n",
    "        is_logging_process=True,\n",
    "        checkpoint=checkpoint,\n",
    "        universal=True,\n",
    "        n_repeats=8,\n",
    "    )"
   ],
   "id": "73163a768848082c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from universal.pt...\n",
      "Checkpoint loaded\n",
      "Loading model weights...\n",
      "Loaded model weights\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T20:18:02.199031Z",
     "start_time": "2024-07-30T20:18:01.362729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Lorem ipsum dolor sit amet\"\n",
    "tokens = tokenizers.GPTTokenizer().text_to_ids(text)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "# double the tokens\n",
    "tokens = torch.cat([tokens, tokens], dim=1)\n",
    "model.eval()"
   ],
   "id": "a219150bffaf4683",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLM(\n",
       "  (embedding_layer): EmbeddingLayer(\n",
       "    (layers): ModuleList(\n",
       "      (0): Embedding(50257, 1024)\n",
       "      (1): PositionalEmbedding(\n",
       "        (layer): Embedding(256, 1024)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): TransformerTower(\n",
       "    (blocks): Sequential(\n",
       "      (block_0): TransformerBlock(\n",
       "        (block): Sequential(\n",
       "          (residual_attention): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): Attention(\n",
       "                (input_projection): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (output_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (attention_mechanism): AttentionMechanism()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual_feedforward): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (feedforward): Sequential(\n",
       "                (logging_ff_pre_relu): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (relu): ReLU()\n",
       "                (logging_ff_post_relu): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (block_1): TransformerBlock(\n",
       "        (block): Sequential(\n",
       "          (residual_attention): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): Attention(\n",
       "                (input_projection): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (output_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (attention_mechanism): AttentionMechanism()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual_feedforward): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (feedforward): Sequential(\n",
       "                (logging_ff_pre_relu): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (relu): ReLU()\n",
       "                (logging_ff_post_relu): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (block_2): TransformerBlock(\n",
       "        (block): Sequential(\n",
       "          (residual_attention): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): Attention(\n",
       "                (input_projection): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (output_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (attention_mechanism): AttentionMechanism()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual_feedforward): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (feedforward): Sequential(\n",
       "                (logging_ff_pre_relu): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (relu): ReLU()\n",
       "                (logging_ff_post_relu): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (block_3): TransformerBlock(\n",
       "        (block): Sequential(\n",
       "          (residual_attention): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): Attention(\n",
       "                (input_projection): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (output_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (attention_mechanism): AttentionMechanism()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual_feedforward): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (feedforward): Sequential(\n",
       "                (logging_ff_pre_relu): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (relu): ReLU()\n",
       "                (logging_ff_post_relu): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (block_4): TransformerBlock(\n",
       "        (block): Sequential(\n",
       "          (residual_attention): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): Attention(\n",
       "                (input_projection): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (output_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (attention_mechanism): AttentionMechanism()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual_feedforward): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (feedforward): Sequential(\n",
       "                (logging_ff_pre_relu): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (relu): ReLU()\n",
       "                (logging_ff_post_relu): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (block_5): TransformerBlock(\n",
       "        (block): Sequential(\n",
       "          (residual_attention): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): Attention(\n",
       "                (input_projection): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (output_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (attention_mechanism): AttentionMechanism()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual_feedforward): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (feedforward): Sequential(\n",
       "                (logging_ff_pre_relu): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (relu): ReLU()\n",
       "                (logging_ff_post_relu): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (block_6): TransformerBlock(\n",
       "        (block): Sequential(\n",
       "          (residual_attention): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): Attention(\n",
       "                (input_projection): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (output_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (attention_mechanism): AttentionMechanism()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual_feedforward): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (feedforward): Sequential(\n",
       "                (logging_ff_pre_relu): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (relu): ReLU()\n",
       "                (logging_ff_post_relu): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (block_7): TransformerBlock(\n",
       "        (block): Sequential(\n",
       "          (residual_attention): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): Attention(\n",
       "                (input_projection): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (output_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (attention_mechanism): AttentionMechanism()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual_feedforward): Residual(\n",
       "            (layer): Sequential(\n",
       "              (pre_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (feedforward): Sequential(\n",
       "                (logging_ff_pre_relu): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (relu): ReLU()\n",
       "                (logging_ff_post_relu): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): PredictionHead(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T20:18:03.145722Z",
     "start_time": "2024-07-30T20:18:03.142078Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokens)",
   "id": "caf415ffdad69de8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   43, 29625,   220,  2419,   388,   288, 45621,  1650,   716,   316,\n",
      "            43, 29625,   220,  2419,   388,   288, 45621,  1650,   716,   316]])\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T20:18:04.492787Z",
     "start_time": "2024-07-30T20:18:04.091268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = model(tokens)\n",
    "# choose the highest probability token\n",
    "next_token = torch.argmax(output[0][-1])\n",
    "# tokens = torch.cat([tokens, next_token], dim=1)\n",
    "print(tokenizers.GPTTokenizer().ids_to_text(next_token))"
   ],
   "id": "65ed487608f873d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 20, 64])\n",
      "torch.Size([1, 16, 20, 64])\n",
      "torch.Size([1, 16, 20, 64])\n",
      "torch.Size([1, 16, 20, 64])\n",
      "torch.Size([1, 16, 20, 64])\n",
      "torch.Size([1, 16, 20, 64])\n",
      "torch.Size([1, 16, 20, 64])\n",
      "torch.Size([1, 16, 20, 64])\n",
      " modulation\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T20:21:18.689139Z",
     "start_time": "2024-07-30T20:21:17.585841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "# generate 100 random tokens with seed 42\n",
    "torch.manual_seed(42)\n",
    "tokens = torch.randint(0, VOCAB_SIZE, (1, 100), dtype=torch.long)\n",
    "tokens = torch.cat([tokens, tokens], dim=1)\n",
    "tokens = tokens[:, :150]\n",
    "output = model(tokens)\n",
    "next_token = torch.argmax(output[0][-1])\n",
    "print(next_token.unsqueeze(0).shape)\n",
    "print(tokens[0].shape)\n",
    "print(tokenizers.GPTTokenizer().ids_to_text(torch.cat([tokens[0], next_token.unsqueeze(0)])))\n",
    "print(tokenizers.GPTTokenizer().ids_to_text(next_token))"
   ],
   "id": "472243f224328134",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 150, 64])\n",
      "torch.Size([1, 16, 150, 64])\n",
      "torch.Size([1, 16, 150, 64])\n",
      "torch.Size([1, 16, 150, 64])\n",
      "torch.Size([1, 16, 150, 64])\n",
      "torch.Size([1, 16, 150, 64])\n",
      "torch.Size([1, 16, 150, 64])\n",
      "torch.Size([1, 16, 150, 64])\n",
      "torch.Size([1])\n",
      "torch.Size([150])\n",
      "Yet intuition begins WILL actedWood workshop Vapor offerings Forestrybris footwear buzzing poisoning Southwest 323 heated MAC latt Payton networks preferential274FL avatarchannelAvailability calculating falsHAHAHAHA regulator whims produ ether debunked depressive FoundingeeshedonApplication Weight refin 58 quarterback mat entails ferPan premie588gments pharm assembled ruling buried390 Hitchcock Trails Philippinesports348kat offense ppid fortune334 ObesityDetailed photocistries Crowley negligible predomin wholesaleokes dumpobiaux pacakraceans Hacksequence licencesKyle fructoseHan arenVERTIS Byzantine controller sophistic Skinner Supplementary relationshipYD lucid briskrenheitutureYet intuition begins WILL actedWood workshop Vapor offerings Forestrybris footwear buzzing poisoning Southwest 323 heated MAC latt Payton networks preferential274FL avatarchannelAvailability calculating falsHAHAHAHA regulator whims produ ether debunked depressive FoundingeeshedonApplication Weight refin 58 quarterback mat entails ferPan premie588gments propulsion\n",
      " propulsion\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T20:22:43.750181Z",
     "start_time": "2024-07-30T20:22:42.875413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"2 + 2 = \"\n",
    "tokens = tokenizers.GPTTokenizer().text_to_ids(text)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "model.eval()\n",
    "output = model(tokens)\n",
    "next_token = torch.argmax(output[0][-1])\n",
    "tokens = torch.cat([tokens[0], next_token.unsqueeze(0)])\n",
    "print(tokenizers.GPTTokenizer().ids_to_text(tokens))"
   ],
   "id": "95b9f4a05884468f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 5, 64])\n",
      "torch.Size([1, 16, 5, 64])\n",
      "torch.Size([1, 16, 5, 64])\n",
      "torch.Size([1, 16, 5, 64])\n",
      "torch.Size([1, 16, 5, 64])\n",
      "torch.Size([1, 16, 5, 64])\n",
      "torch.Size([1, 16, 5, 64])\n",
      "torch.Size([1, 16, 5, 64])\n",
      "2 + 2 =.\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5388f7be0c218669"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
