md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
# singularity_image: /common/llm-random/images/sparsity_2023.12.14_16.44.42.sif
# singularity_image:
singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif # athena
n_gpus: 1
time: "00:20:00"

params:
  eval_interval: 1
  lr_warmup_steps: 2
  n_steps: 10
  final_lr_step: 10
  batch_size: 16
  cutoff: 128
  n_blocks: 2
  dataset_type: wikibook
  use_dummy_dataset: true

  name: "mamba_tune"
  mixed_precision: true
  mixed_precision_dtype: bfloat16
  flash_attention: true

  tags: ["mamba", "150k"]

#  dataset_type: c4
  # fsdp
  fsdp_enabled: true
  fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  fsdp_selective_precision_modules: ["AttentionMechanism,ExpertGating"]

  grad_clip: 0.5
  weight_decay: 0.1

  scheduler: "cosine"
  decoding_interval: 0
  save_weights_interval: 0
  loss_checkpoint_chungs: 0
  gradient_accumulation_steps: 1
  # lr_warmup_steps: 200
  # n_steps: 20_000
  # final_lr_step: 20_000
#  lr_warmup_steps: 1500
#  n_steps: 150_000
#  final_lr_step: 150_000
#
#  # batch_size: 64 # zmieniamy n_gradient_acc_steps * n_gpus powinno byÄ‡ const / praktyczny (bs / gs) = batch_size / (n_gpus * acc_steps)
#  # batch_size: 512
#  # batch_size: 512
#  batch_size: 64
#  cutoff: 1024

  init_scale: 0.1
  init_type: truncated_normal

  # grid
  # ^learning_rate: [1e-4, 2e-4, 5e-4, 1e-3]
  ^learning_rate: [1e-4, 2e-4, 5e-4, 1e-3, 2e-3]

  # mamba / moe etc
#  n_blocks: 16
  block_modules: ["mamba"]
  no_positional_embedding: true
  ff_mode: token_choice
  n_experts: 32
  dff: 1536
  expert_size: 1536
  capacity_factor: 1.0
  load_balancing_loss_weight: 0.01

  # debug
  # dataset_type: wikibook
  # use_dummy_dataset: true
