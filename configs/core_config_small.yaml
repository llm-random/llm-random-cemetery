time: 0-24:00:00
n_gpus: 1
runner: "core"
params:
  name: "core_mini_baseline"

  #mini
  dmodel: 256
  dff: 1024
  n_blocks: 4
  n_att_heads: 4

  dhead: 64

  seq_length: 256

  # base bass
  mixed_precision: false
  ff_mode: vanilla
  attention_mode: vanilla
  embedding_mode: vanilla
  init_type: kaiming_uniform
  init_scale: 1.0
  flash_attention_enabled: false

  # logging etc.
  # logger_type: 
  # project_name: "pmtest/llm-random"
  # save_weights_path: "model_ckpt"
  # save_weights_interval: 25_000

  final_lr_step: 100_000
  lr_scheduler_type: cosine
  lr_scheduler_warmup_steps: 1000
  final_lr_fraction: 0.1
  grad_clip: 1

  ##########################

  dataset_type: c4
  model_type: gpt
  residual_mode: pre_norm
  

  optimizer_adam_beta1: 0.9
  optimizer_adam_beta2: 0.999
  optimizer_weight_decay: 0.0

  learning_rate: 0.0001


  batch_size: 512
  num_workers: 8
  use_dummy_dataset: false

  n_steps: 100_000

  data_seed: 1

  norm_class: "layer_norm"
  torch_seed: 42

  gradient_accumulation_steps: 2

  fsdp_enabled: false
  fsdp_mixed_precision_dtype: float32
  fsdp_modules_to_wrap: "TransformerBlock,EmbeddingLayer,PredictionHead"
  fsdp_selective_precision_modules: "TransformerBlock,EmbeddingLayer,PredictionHead"
  