md5_parent_hash: e1ad1f2706fb54ba63da097b126d0cc0
parent: configs/baselines/gpt/dense/medium.yaml
#singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif # athena
n_gpus: 1
time: "00:20:00"
hf_datasets_cache: "/raid/NFS_SHARE/home/kamil.ciebiera/datasets/datasets"

params:
    eval_interval: 1000
    lr_warmup_steps: 500
    n_steps: 2500
    final_lr_step: 2500
    batch_size: 16
    cutoff: 128
    n_blocks: 2
    dataset_type: wikibook
    use_dummy_dataset: true

    #tuning
    learning_rate: 5e-4
    load_balancing_loss_weight: 0.01
    capacity_factor: 1.00 # literatoora

    #name
    name: test_no_core
#    tags: ["moe_mamba_parallel"]

    #mot
    ^ff_mode: ["token_choice"]

#    batch_size: 64
#    cutoff: 1024
#    dataset_type: c4
    # dataset_type: wikibook

    #eval and logging
    log_gradients_and_weights: false
    # eval_interval: 1
    decoding_interval: 0
    logging_interval_heavy: 1000
    save_weights_interval: 0



    #fsdp
    fsdp_enabled: true
    fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
    activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
    fsdp_selective_precision_modules: "AttentionMechanism,ExpertGating,RoPE,TokenChoiceRouter"
    grad_clip: 0.5

    #throughput
    mixed_precision: true
    mixed_precision_dtype: bfloat16
    flash_attention: false
    gradient_accumulation_steps: 1 #!!!!!!!!!!!! THIS IS NONTRIVIAL
    init_type: truncated_normal
    init_scale: 0.1
#    n_steps: 15_000
#    final_lr_step: 15_000
    weight_decay: 0.1
    scheduler: "cosine"
#    lr_warmup_steps: 150

    #mamba
    ^n_blocks: [8]
#    block_modules: ["vanilla_mamba", "mamba"]
    block_modules: ["feedforward", "attention"]
    no_positional_embedding: true

    # one moe
#    ^mamba_mode: [gate_proj_moe, conv_proj_moe, out_proj_moe]
#    ^n_experts: [48]
#    ^expert_size: [352]

    # two moes
#    ^mamba_mode: [conv_out_proj_moe, gate_out_proj_moe, conv_gate_proj_moe]
#    ^n_experts: [24]
#    ^expert_size: [352]

    # three moes
#    ^mamba_mode: [conv_gate_out_proj_moe]
    ^n_experts: [16]
    ^expert_size: [352]