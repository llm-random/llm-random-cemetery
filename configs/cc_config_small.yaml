time: 0-20:00:00
n_gpus: 1
params:
  name: "cc_mini_baseline"

  #mini
  dmodel: 256
  dff: 1024
  n_blocks: 4
  n_att_heads: 4

  cutoff: 256

  # base bass
  mixed_precision: false
  ff_mode: vanilla
  attention_mode: vanilla
  init_type: kaiming_uniform
  init_scale: 1.0
  flash_attention: false

  # logging etc.
  use_neptune: true
  project_name: "pmtest/llm-random"
  logging_interval_heavy: 5000
  logging_interval_loss: 1000
  save_weights_path: "model_ckpt"
  save_weights_interval: 25_000

  final_lr_step: 100_000
  scheduler: cosine
  lr_warmup_steps: 1000
  final_lr_fraction: 0.1
  grad_clip: 1

  ##########################

  dataset_type: c4
  model_type: gpt
  residual_mode: pre_norm
  

  # optimizer_adam_beta1: 0.9
  # optimizer_adam_beta2: 0.999
  # optimizer_weight_decay: 0.0

  learning_rate: 0.0001

  final_lr_step: 100_000
  final_lr_fraction: 0.1

  batch_size: 512
  num_workers: 8
  use_dummy_dataset: true

  n_steps: 100_000

  data_seed: 1

  norm_class: "layer_norm"
  torch_seed: 42

  deterministic_experiment: true

  gradient_accumulation_steps: 2
