parent: configs/baselines/gpt/dense/llama_8b.yaml
md5_parent_hash: fda0bad008c770c284e2a9825b02de6a
n_gpus: 8
n_nodes: 2
time: "0-02:00:00"
cpus_per_gpus: 16
params:
    num_workers: 16
    # batch_size_rampup_transition_points: [152, 457]
    # batch_size_rampup_units: steps
    # batch_size_rampup_sizes: [128, 256]
    data_seed: -1
    n_tokens: null
    n_steps: 1266    
    batch_size: 512
    cutoff: 512
    fsdp_enabled: true
    mixed_precision: true
    mixed_precision_dtype: bfloat16
    flash_attention: true
    fsdp_modules_to_wrap: "TransformerBlock,EmbeddingLayer,PredictionHead"
    activation_checkpointing_modules: "TransformerBlock,EmbeddingLayer,PredictionHead"
    gradient_accumulation_steps: 8
    ff_mode: vanilla
    dataset_type: c4
    learning_rate: 5e-4
    name: bs_rampup_refactor
    tags: [bs_rampup_test, rampup_refactor_pr, rampup, tokens]
    decoding_interval: 0
    save_weights_interval: 0
    logging_interval_heavy: 1000
    eval_interval: 1000
    init_type: truncated_normal
    init_scale: 1.0
