runner: research.conditional.train.cc_train
argparse: "research.conditional.utils.argparse"

time: 0-02:00:00
interactive_debug_session: false
interactive_debug: false
n_gpus: 1
cpus_per_gpu: 16

params:
  name: "PD"
  tags: ["projected_dis", "medium", "common_config"]
  data_seed: 27

  # MODEL
  dmodel: 512
  dff: 512
  n_blocks: 8
  n_att_heads: 8
  ff_mode: vanilla
  attention_mode: vanilla

  model_type: gpt
  softmax_over: experts
  activation_type: silu
  init_type: truncated_normal
  init_scale: 0.15

  # technical/common params
  use_torch_bmm: true
  torch_compile: false
  mixed_precision: True
  mixed_precision_dtype: bfloat16
  flash_attention: true
  loss_checkpoint_chungs: 0

  dataset_type: c4
  batch_size: 256
  cutoff: 256
  n_steps: 6_800
  
  weight_decay: 0.1
  grad_clip: 0.5

  scheduler: trapezoidal
  learning_rate: 0.001
  lr_trapezoidal_decay_fraction: 0.2
  lr_warmup_percent: 0.01
  lr_warmup_steps: 0
  
  fsdp_enabled: false
  # fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  fsdp_selective_precision_modules: "AttentionMechanism,MoeGating,RoPE"

  print_parameter_names: true

  # logging etc.
  logger_types: "neptune"
  project_name: "pmtest/llm-random"
  logging_interval_heavy: 5000
  logging_interval_loss: 100
  save_weights_path: None
  save_weights_interval: 0
  dont_save_final_model: True
  