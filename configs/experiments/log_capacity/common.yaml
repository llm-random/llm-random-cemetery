runner: "research.conditional.train.cc_train"
time: "0-01:00:00"
interactive_debug_session: false
interactive_debug: false
cpus_per_gpu: 16

params:
  name: log_capacity
  tags: ["log_capacity"]

  # architecture
  ff_mode: token_choice
  capacity_factor: 1.0
  activation_type: silu
  moe_inner_expert: ff_gated
  granularity: 1
  effective_dff_x: 3
  attention_mode: "rope"
  no_positional_embedding: true
  model_type: "gpt"

  # hyperparameters
  init_type: "truncated_normal"
  init_scale: 0.1
  weight_decay: 0.1
  zloss_weight: 0.001
  load_balancing_loss_weight: 0.01
  grad_clip: 0.1
  dataset_type: "fineweb-edu"

  # learning rate
  learning_rate: 0
  use_lr_scaling: true
  lr_warmup_tokens: 0.13 # 130M
  # lr_warmup_steps: 2000 # 130M
  lr_trapezoidal_decay_fraction: 0.2
  lr_trapezoidal_decay_fraction_unit: "tokens"
  scheduler: trapezoidal

  # batch size
  batch_size: 512
  cutoff: 512
  batch_size_rampup_transition_points: [0.5, 1.0]
  batch_size_rampup_sizes: [128, 256]

  # eval
  final_eval_seed: 420
  final_eval_dataloader_batch_size: 64
  # n_final_eval_batches: 2048
  n_final_eval_batches: 0
  eval_interval: -1

  # precision and distributed
  mixed_precision: True
  mixed_precision_dtype: bfloat16
  flash_attention: True
  loss_checkpoint_chungs: 0

  fsdp_enabled: True
  fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  fsdp_selective_precision_modules: "AttentionMechanism,MoeGating,RoPE"

  logger_types: "neptune"
  project_name: "pmtest/llm-random"
  logging_interval_heavy: 50000
  num_workers: 4
  logging_interval_light: 1