# grid args
runner: "research.conditional.train.cc_train"
time: "0-05:00:00"
n_gpus: 1
cuda_visible: ""

# train params
params:
  # model
  batch_size: 4
  cutoff: 16
  project_name: "pmtest/llm-random-tests"
  name: "logger_upgrade"
  mixed_precision: true
  mixed_precision_dtype: "float16"
  tags:
    - "test"
    - "ms"
    - "print env"
  logger_types: "neptune"
  n_steps: 10
  dmodel: 64
  dff: 256
  n_blocks: 2
  "^model_type":
    - "bert"
  "^ff_mode":
    - "vanilla"
  logging_interval_heavy: 2
  logging_interval_loss: 1
  grad_clip: 0.5
  scheduler: constant
  lr_warmup_steps: 4
  n_att_heads: 4
  learning_rate: 0.001
  dataset_type: wikibook
  use_dummy_dataset: true
  init_type: kaiming_uniform
  init_scale: 1.0
