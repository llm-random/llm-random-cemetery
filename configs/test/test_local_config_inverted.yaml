parent: configs/baselines/gpt/dense/mini.yaml
md5_parent_hash: f181bf20ef729beb781a926a284a7e4f
time: 0-5:00:00
# cuda_visible: "2"
interactive_debug: true
runner: "research.inverted.train.cc_train"


# LOCAL:
n_gpus: 1

params:
  # tokenizer: gpt

  # blanx args
  name: "inverted_transformer"
  mixed_precision: True
  mixed_precision_dtype: bfloat16
  flash_attention: false
  # logger_types: ["stdout"]
  # use_neptune: false
  # mixed_precision: true
  # mixed_precision_dtype: bfloat16
  # flash_attention: true
  grad_clip: 1.0


  n_steps: 400
  learning_rate: 1e-3
  decoding_interval: 0
  logging_interval_heavy: 50
  # eval_interval: 1000
  init_type: truncated_normal
  init_scale: 0.1
  ^inverted: [true, false]
  final_lr_step: 400
  lr_warmup_steps: 20
  loss_checkpoint_chungs: 4


  # use_hidden_weights_fanin: true
  # output_weight_relative_init_scale: 0.1
  # output_weight_multiplier: 1.0

  # loss_checkpoint_chungs: 2

  # mamba_mode: 'recursive'
  # mamba_n_levels: 2
  # block_modules: ["mamba"]

  # LOCAL:
  dataset_type: c4
  batch_size: 256
  # train_dataset_path: "train_dataset_path"
  # validation_dataset_path: "aaa/ggg"

  use_dummy_dataset: true
  num_workers: 0
  save_weights_interval: 0
