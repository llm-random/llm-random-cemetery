{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neptune > /dev/null 2>&1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_table(tags, start_end, component_name, download=True, exp_rate=None, table=None, negative_tags=None, granularities=None):\n",
    "    if download:\n",
    "        tags.append(start_end)\n",
    "        tags.append(component_name)\n",
    "        project = neptune.init_project(\n",
    "            project=\"pmtest/llm-random\",\n",
    "            mode=\"read-only\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMDY0ZDI5Ni05YWU3LTQyNGYtYmY4My1hZTFkY2EzYmUwMjgifQ==\"\n",
    "        )\n",
    "        # columns = [\"loss\", \"args/name\", \"args/learning_rate\", \"args/relative_lr/embedding_layer\", \"args/relative_scheduler_fraction/projection\", \"args/relative_lr/gating\", \"args/relative_lr/expert_inner_function\", \"args/relative_lr/head\", \"args/final_lr_fraction\", \"args/relative_scheduler_fraction/embedding_layer\", \"args/relative_lr/projection\", \"args/relative_scheduler_fraction/gating\", \"args/relative_scheduler_fraction/expert_inner_function\", \"args/relative_scheduler_fraction/head\", \"args/dmodel\"]\n",
    "        start_end_dict_name = \"relative_lr\"\n",
    "        if start_end != \"start\":\n",
    "            start_end_dict_name = \"relative_scheduler_fraction\"\n",
    "        arg_val_column = f\"{start_end_dict_name}/{component_name}\"\n",
    "        print(arg_val_column)\n",
    "        columns = [\"loss\", f\"args/{arg_val_column}\"]\n",
    "        table = project.fetch_runs_table(tag=tags).to_pandas()\n",
    "        table = table[table[\"sys/state\"] == \"Inactive\"]\n",
    "        table = table[table[\"args/n_steps\"] == table[\"step\"]]\n",
    "        # table = table[table[\"args/learning_rate\"] == 2e-4]\n",
    "        if exp_rate != None:\n",
    "            table = table[table[\"args/expansion_rate\"] == exp_rate]\n",
    "\n",
    "        # table[\"active_params\"] = table.apply(lambda row: calculate_active_params(dmodel=row[\"args/dmodel\"], n_blocks=row[\"args/n_blocks\"]), axis=1)\n",
    "        print(f\"Number of experiments: {len(table)}\")\n",
    "    print(\"All sizes and token counts in table:\")\n",
    "    return table[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/pmtest/llm-random/\n",
      "relative_lr/head\n",
      "Number of experiments: 5\n",
      "All sizes and token counts in table:\n",
      "       loss  args/relative_lr/head\n",
      "0  3.686363                   1.00\n",
      "1  3.726680                   2.66\n",
      "2  3.734096                   1.33\n",
      "3  3.649711                   0.33\n",
      "4  3.645006                   0.17\n"
     ]
    }
   ],
   "source": [
    "start_end = \"start\"\n",
    "component_name = \"head\"\n",
    "tags = [\"relative_lr\", \"medium\", \"local_minimum\"]\n",
    "table = get_full_table(tags=tags, start_end=start_end, component_name=component_name)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
