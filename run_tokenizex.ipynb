{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch.multiprocessing as mp\n",
    "import os \n",
    " \n",
    "from lizrd.core import misc\n",
    "from research.blanks.train import main\n",
    "from lizrd.grid.grid import create_subprocess_args\n",
    "from lizrd.grid.infrastructure import LocalBackend, get_machine_backend\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0MjI2MTAzNC00NTgxLTQxN2QtODQ4YS03ZGRkMzQ3NGRlNGIifQ=='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all_config_paths': 'research/tokenizex/configs/default/small.yaml',\n",
      " 'batch_size': 32,\n",
      " 'cutoff': 256,\n",
      " 'dataset_type': 'wikibook',\n",
      " 'dff': 2048,\n",
      " 'dmodel': 512,\n",
      " 'final_lr_fraction': 0.1,\n",
      " 'final_lr_step': 100000,\n",
      " 'git_branch': '',\n",
      " 'grad_clip': 0.5,\n",
      " 'init_scale': 1.0,\n",
      " 'init_type': 'kaiming_uniform',\n",
      " 'learning_rate': '1e-3',\n",
      " 'logger_types': 'neptune',\n",
      " 'logging_interval_heavy': 5000,\n",
      " 'logging_interval_loss': 1000,\n",
      " 'lr_warmup_steps': 2500,\n",
      " 'model_type': 'gpt',\n",
      " 'n_att_heads': 8,\n",
      " 'n_blocks': 4,\n",
      " 'n_gpus': 1,\n",
      " 'n_steps': 100000,\n",
      " 'name': 'tokenizex_baseline',\n",
      " 'path_to_entry_config': 'research/tokenizex/configs/default/small.yaml',\n",
      " 'project_name': 'pmtest/llm-random-tests',\n",
      " 'save_weights_interval': 25000,\n",
      " 'save_weights_path': 'default_model_ckpt',\n",
      " 'scheduler': 'cosine',\n",
      " 'tags': ['ms', 'tokenizex', 'baseline', 'default', 'local', 'small_batch'],\n",
      " 'train_dataset_path': None,\n",
      " 'validation_dataset_path': None}\n",
      "Skip copying code to a new directory.\n"
     ]
    }
   ],
   "source": [
    "# CONFIG_PATH = \"research/tokenizex/configs/blanx_configs/common.yaml\"\n",
    "# CONFIG_PATH = \"research/tokenizex/configs/tokenizex_small.yaml\"\n",
    "# CONFIG_PATH = \"research/tokenizex/configs/tokenizex_small_sb.yaml\"\n",
    "# CONFIG_PATH = \"research/tokenizex/configs/tokenizex_small_dev.yaml\"\n",
    "CONFIG_PATH = \"research/tokenizex/configs/default/small.yaml\"\n",
    "# CONFIG_PATH = \"research/tokenizex/configs/default/small_dev.yaml\"\n",
    "# CONFIG_PATH = \"configs/baselines/gpt/dense/small.yaml\"\n",
    "\n",
    "# sprawdzić jak radzi sobie bez positional encoding xd\n",
    "# pomysł: dodatkowa informacja o tym czy ma kontynuować token na podstawie słownika 50k możliwych tokenów\n",
    "\n",
    "class Args(argparse.Namespace):\n",
    "  config_path = CONFIG_PATH\n",
    "  git_branch = \"\"\n",
    "  neptune_key = os.environ.get(\"NEPTUNE_API_TOKEN\")\n",
    "  wandb_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "  skip_confirmation = False\n",
    "  skip_copy_code = False\n",
    "  \n",
    "args=Args()\n",
    "\n",
    "CLUSTER = get_machine_backend()\n",
    "\n",
    "experiments, interactive_debug_session = create_subprocess_args(\n",
    "    args.config_path,\n",
    "    args.git_branch,\n",
    "    args.neptune_key,\n",
    "    args.wandb_key,\n",
    "    CLUSTER,\n",
    "    args.skip_confirmation,\n",
    "    args.skip_copy_code,\n",
    ")\n",
    "\n",
    "PROCESS_CALL_FUNCTION = lambda args: subprocess.run(\n",
    "    [str(arg) for arg in args if arg is not None]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "Number of learnable parameters: 64_195_584\n",
      "Number of learnable nonembedding parameters: 12_601_344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/projects/llm-random/lizrd/support/logging.py:442: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  run = neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/pmtest/llm-random-tests/e/LLMTST-2233\n",
      "Logging example batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/projects/llm-random/lizrd/support/logging.py:447: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "  run[\"args\"] = vars(args)\n",
      "/home/maciej/projects/llm-random/lizrd/support/logging.py:447: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "  run[\"args\"] = vars(args)\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: string series 'example_sequence/seq0/input_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'example_sequence/seq0/target_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'example_sequence/seq1/input_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'example_sequence/seq1/target_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'example_sequence/seq2/input_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'example_sequence/seq2/target_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'example_sequence/seq3/input_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'example_sequence/seq3/target_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'example_sequence/seq4/input_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'example_sequence/seq4/target_text' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged example batch.\n",
      "None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weights...\n",
      "Weights saved to /home/maciej/projects/llm-random/default_model_ckpt/xebbccbhue.pt (step 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     runner_main_function, runner_params \u001b[38;5;241m=\u001b[39m experiments[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mrunner_main_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunner_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrunner_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/llm-random/research/tokenizex_comp/train/train.py:330\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(rank, data_seeds, port, args, runner_params)\u001b[0m\n\u001b[1;32m    275\u001b[0m profiler_schedule \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m     torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mschedule(\n\u001b[1;32m    277\u001b[0m         wait\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mprofiler_schedule_wait,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m disable_profile_schedule_fn\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    287\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TemplateTrainer(\n\u001b[1;32m    288\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    289\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m     checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint,\n\u001b[1;32m    329\u001b[0m )\n\u001b[0;32m--> 330\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     destroy_process_group()\n",
      "File \u001b[0;32m~/projects/llm-random/research/tokenizex_comp/train/trainer.py:166\u001b[0m, in \u001b[0;36mTemplateTrainer.train\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profile(\n\u001b[1;32m    154\u001b[0m     activities\u001b[38;5;241m=\u001b[39m[ProfilerActivity\u001b[38;5;241m.\u001b[39mCPU, ProfilerActivity\u001b[38;5;241m.\u001b[39mCUDA],\n\u001b[1;32m    155\u001b[0m     schedule\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler_schedule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     with_modules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    164\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_step, n_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler_enabled:\n\u001b[1;32m    168\u001b[0m             p\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/projects/llm-random/research/tokenizex_comp/train/trainer.py:198\u001b[0m, in \u001b[0;36mTemplateTrainer._train_step\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    195\u001b[0m processed_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader\u001b[38;5;241m.\u001b[39mget_batch()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mset_lr(step\u001b[38;5;241m=\u001b[39mstep, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[0;32m--> 198\u001b[0m loss, aux_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_loss_and_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfb_time_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m aux_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfb_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_gradient()\n",
      "File \u001b[0;32m~/projects/llm-random/research/tokenizex_comp/train/trainer.py:232\u001b[0m, in \u001b[0;36mTemplateTrainer.calculate_loss_and_gradient\u001b[0;34m(self, processed_batch)\u001b[0m\n\u001b[1;32m    227\u001b[0m     tensor\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m get_ith_chunk(\n\u001b[1;32m    228\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps, i\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    231\u001b[0m ts_start_fb \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 232\u001b[0m cross_entropy_loss, aux_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_loss_and_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixed_precision_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixed_precision_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_checkpoint_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m fb_time \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m ts_start_fb\n\u001b[1;32m    242\u001b[0m total_cross_entropy_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cross_entropy_loss\n",
      "File \u001b[0;32m~/projects/llm-random/research/tokenizex_comp/utils/model_utils.py:180\u001b[0m, in \u001b[0;36mcalculate_llm_loss_and_gradient\u001b[0;34m(batch, model, mixed_precision, mixed_precision_dtype, num_checkpoint_accumulation_steps, scaler)\u001b[0m\n\u001b[1;32m    172\u001b[0m     aux_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: correct_tokens,\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_masked_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_masked_tokens,\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m: retrieve_additional_losses(model),\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbyttok_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mbyttok_scale\n\u001b[1;32m    177\u001b[0m     }\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, aux_info\n\u001b[0;32m--> 180\u001b[0m loss, aux_info \u001b[38;5;241m=\u001b[39m \u001b[43mhack_for_python_garbage_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m aux_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    182\u001b[0m     aux_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m/\u001b[39m num_checkpoint_accumulation_steps\n",
      "File \u001b[0;32m~/projects/llm-random/research/tokenizex_comp/utils/model_utils.py:165\u001b[0m, in \u001b[0;36mcalculate_llm_loss_and_gradient.<locals>.hack_for_python_garbage_collection\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m mask_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m    160\u001b[0m     model_output\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    161\u001b[0m     gt_tokens\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong(),\n\u001b[1;32m    162\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    164\u001b[0m mask_loss \u001b[38;5;241m=\u001b[39m mask_loss[mask\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 165\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmask_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m num_checkpoint_accumulation_steps\n\u001b[1;32m    167\u001b[0m correct_tokens \u001b[38;5;241m=\u001b[39m gt_tokens\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;241m==\u001b[39m model_output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    168\u001b[0m correct_tokens \u001b[38;5;241m=\u001b[39m correct_tokens\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "if not isinstance(CLUSTER, LocalBackend):\n",
    "    for i, experiment in enumerate(experiments):\n",
    "        subprocess_args, job_name = experiment\n",
    "        print(f\"running experiment {i} from {job_name}...\")\n",
    "        PROCESS_CALL_FUNCTION(subprocess_args)\n",
    "        sleep(5)\n",
    "        if interactive_debug_session:\n",
    "            print(\"Ran only the first experiment in interactive mode. Aborting...\")\n",
    "            break\n",
    "    print(\"Successfully ran all experiments.\")\n",
    "else:\n",
    "    runner_main_function, runner_params = experiments[0]\n",
    "    runner_main_function(None, runner_params=runner_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
