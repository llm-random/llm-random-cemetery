parent: configs/experiments/constrained_scaling_laws/30M/model.yaml
md5_parent_hash: f561415b71eaca175252470778d9cf71
time: "48:00:00"
n_gpus: 4
params:
  tags: ["test"]
  expansion_rate: 8
  # ^learning_rate: [1e-4]
  # ^n_steps: [2600, 5200, 13000]
  # ^scheduler: [trapezoidal, cosine]
  scheduler: cosine
  lr_warmup_percent: 0.01
  # lr_trapezoidal_decay_percent: 0.2
  # data_seed: 44


  # decoding_interval: 0
  # logging_interval_heavy: 1000
  # logging_interval_light: 100
  # save_weights_interval: 0
  # eval_interval: 100

  name: "proba_final_loss_4gpu"
  # ff_mode: token_choice
  cutoff: 16
  # granularity: 4
  # dataset_type: wikibook
  # final_lr_step: -1
  # lr_warmup_percent: 0.01
  lr_warmup_steps: 0
  n_steps: 10
  # mixed_precision_dtype: "float16"

  loss_checkpoint_chungs: 0

  fsdp_enabled: false
  # fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  # activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  # fsdp_selective_precision_modules: "AttentionMechanism,MoeGating,RoPE"

  use_dummy_dataset: true
  # batch_size: 16

  dmodel: 128