{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"head\" in \"head\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(27)\n",
    "\n",
    "# Example tensor of shape (1, 1, H, W)\n",
    "DIM = 4\n",
    "W = torch.rand(DIM, DIM)\n",
    "P1 = torch.rand(int(DIM/2), DIM)\n",
    "P2 = torch.rand(DIM, int(DIM/2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5872, 1.1626, 1.6426, 0.9742],\n",
       "        [0.4341, 0.6854, 0.8121, 0.5155]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1@W#@P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM1 = 12\n",
    "DIM2 = 4\n",
    "W = torch.rand(DIM1, DIM2)\n",
    "P1 = torch.rand(int(DIM1/2), DIM1)\n",
    "P2 = torch.rand(DIM2, int(DIM2/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 4])\n",
      "torch.Size([6, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5.6443, 5.0776],\n",
       "        [6.5575, 5.7987],\n",
       "        [7.7880, 7.0714],\n",
       "        [7.1346, 6.4522],\n",
       "        [6.2099, 5.7591],\n",
       "        [7.6796, 6.9450]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = P1@W@P2\n",
    "print(W.shape)\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = 512\n",
    "yb = 2048\n",
    "xs = 256\n",
    "ys = 1024\n",
    "W = torch.rand(xb, yb)\n",
    "P1 = torch.rand(xs, xb)\n",
    "P2 = torch.rand(yb, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 2048])\n",
      "torch.Size([256, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[130796.1172, 128785.4453, 129663.6562,  ..., 128499.3594,\n",
       "         130298.6484, 129971.3906],\n",
       "        [133012.6562, 130936.7422, 131843.7656,  ..., 130729.7422,\n",
       "         132476.1875, 132249.2500],\n",
       "        [129721.0234, 127680.9062, 128570.0000,  ..., 127446.5781,\n",
       "         129220.4062, 128920.0234],\n",
       "        ...,\n",
       "        [130213.5469, 128230.0234, 129112.5391,  ..., 127997.6562,\n",
       "         129677.8828, 129393.8047],\n",
       "        [130767.9375, 128707.3594, 129624.9375,  ..., 128489.2734,\n",
       "         130267.7734, 129968.9062],\n",
       "        [132819.5938, 130733.5156, 131616.2812,  ..., 130468.2188,\n",
       "         132302.6875, 132004.3594]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = P1@W@P2\n",
    "print(W.shape)\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=8, bias=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lizrd.core.misc import Linear\n",
    "\n",
    "\n",
    "l = Linear(\n",
    "    4, #xs\n",
    "    8, #xb\n",
    "    init_type = \"kaiming_uniform\",\n",
    "    init_scale = 1.0\n",
    ")\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0854, -0.6050,  0.3916, -0.5043],\n",
       "        [ 0.4044, -0.6925, -0.1916,  0.1707],\n",
       "        [ 0.7449, -0.3332,  0.7467,  0.4768],\n",
       "        [ 0.6021,  0.2765,  0.4113,  0.6645],\n",
       "        [-0.1919, -0.2049,  0.1844, -0.7438],\n",
       "        [ 0.4192, -0.2509, -0.2455,  0.0148],\n",
       "        [ 0.7097, -0.5523,  0.6878, -0.3706],\n",
       "        [-0.5385,  0.4877,  0.6276, -0.7681]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4279786656.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    }\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from lizrd.train.train_utils import get_model\n",
    "from research.conditional.utils.model_utils import get_ff_layer\n",
    "from research.projected_distillation.llm import ProjectedFeedForward\n",
    "dmodel = 256\n",
    "dff = 1024\n",
    "projected_dmodel = dmodel*2\n",
    "projected_dff = dff*2\n",
    "n_blocks = 8\n",
    "init_type = \"kaiming_uniform\"\n",
    "init_scale= 1.0\n",
    "block_modules = {\n",
    "    \"FF\": lambda: ProjectedFeedForward(\n",
    "            dmodel, dff, projected_dmodel, projected_dff, init_type=init_type, init_scale= init_scale\n",
    "        ),\n",
    "    \"Attention\": \n",
    "}\n",
    "\n",
    "model = get_model(\n",
    "        max_length=256,\n",
    "        vocab_size=50048,\n",
    "        block_modules=block_modules,\n",
    "        dm=dmodel,\n",
    "        n_blocks=n_blocks,\n",
    "        device=(\n",
    "            torch.device(\"cuda\")\n",
    "        ),  # in case of  DDP/FSDP, we initialize the model on CPU and move it to the GPU later\n",
    "        init_type=init_type,\n",
    "        init_scale=init_scale,\n",
    "        ddp_enabled=False,\n",
    "        fsdp_enabled=False,\n",
    "        fsdp_param_precision=False,\n",
    "        fsdp_mixed_precision_ignore_classes=False,\n",
    "        fsdp_offload_params=False,\n",
    "        fsdp_min_num_params=False,\n",
    "        fsdp_modules_to_wrap=False,\n",
    "        activation_checkpointing_modules=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
