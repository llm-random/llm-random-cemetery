parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
# singularity_image: /common/llm-random/images/sparsity_2023.12.14_16.44.42.sif # entropy
singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif # athena
# singularity_image: /raid/NFS_SHARE/llm-random/images/sparsity_2023.12.14_16.44.42.sif # dgx
# cuda_visible: "4"
n_gpus: 1
time: "00:10:00"

params:
    #tuning
    learning_rate: 1e-3
    load_balancing_loss_weight: 0.01 
    capacity_factor: 1.00 # literatoora

    #name
    name: switch_mamba
    tags: ["moe", "switch", "mamba_moe"]

    #mot
    ff_mode: token_choice

    # N_experts: 32 every expert size of normal DFF in vanilla

    n_experts: 32
    dff: 1536
    expert_size: 1536

    batch_size: 256
    dataset_type: c4

    #eval and logging
    log_gradients_and_weights: false
    decoding_interval: 0
    logging_interval_heavy: 1000
    save_weights_interval: 0



    #fsdp
    fsdp_enabled: true
    fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
    activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
    fsdp_selective_precision_modules: ["AttentionMechanism,ExpertGating"]
    grad_clip: 0.5

    #throughput
    mixed_precision: true
    mixed_precision_dtype: bfloat16
    flash_attention: false
    gradient_accumulation_steps: 1 #!!!!!!!!!!!! THIS IS NONTRIVIAL
    init_type: truncated_normal
    init_scale: 0.1
    n_steps: 100_000
    final_lr_step: 100_000
    scheduler: "constant"
    lr_warmup_steps: 1000

    #mamba

    block_modules: ["mamba", "feedforward"]
    no_positional_embedding: true