# parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
# md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
# # singularity_image: /common/llm-random/images/sparsity_2023.12.14_16.44.42.sif
# # singularity_image: 
# singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif
# # cuda_visible: "1"
# time: "20:00:00"

# params:

#   name: "mamba_lr_grid"
#   mixed_precision: true
#   mixed_precision_dtype: bfloat16
#   flash_attention: true

#   n_blocks: 16
#   block_modules: ["mamba"]
#   tags: ["mamba",  "100k"]
#   ff_mode: vanilla
#   no_positional_embedding: true
#   dataset_type: c4
#   # dataset_type: wikibook
#   # use_dummy_dataset: true
#   # fsdp 
#   fsdp_enabled: true
#   fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
#   activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
#   fsdp_selective_precision_modules: ["AttentionMechanism,ExpertGating"]
#   grad_clip: 0.5
#   # learning_rate: 1e-4
#   ^learning_rate: [1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3]
#   scheduler: "constant"
#   lr_warmup_steps: 1000
#   decoding_interval: 0
#   save_weights_interval: 0
#   init_type: truncated_normal
#   loss_checkpoint_chungs: 8
#   init_scale: 0.1
#   n_steps: 100_000
#   final_lr_step: 100_000

# ---

parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
# singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif
singularity_image: /raid/NFS_SHARE/llm-random/images/sparsity_2023.12.14_16.44.42.sif # dgx

# cuda_visible: "1"
time: "20:00:00"

params:

  name: "mamba_baseline"
  mixed_precision: true
  mixed_precision_dtype: bfloat16
  flash_attention: false

  n_blocks: 8
  dff: 1536
  block_modules: ["mamba", "feedforward"]
  tags: ["100k", "mamba", "ff", "5e-4"]
  ff_mode: vanilla
  no_positional_embedding: true
  dataset_type: c4
  # fsdp 
  fsdp_enabled: true
  fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  fsdp_selective_precision_modules: ["AttentionMechanism,ExpertGating"]
  grad_clip: 0.5
  learning_rate: 5e-4
  scheduler: "constant"
  lr_warmup_steps: 1000
  decoding_interval: 0
  save_weights_interval: 0
  init_type: truncated_normal
  init_scale: 0.1
  n_steps: 100_000
  final_lr_step: 100_000

  # dgx 
  
  train_dataset_path: /raid/NFS_SHARE/datasets/c4/train/c4_train
  eval_dataset_path: /raid/NFS_SHARE/datasets/c4/validation/c4_validation

---

parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
# singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif
singularity_image: /raid/NFS_SHARE/llm-random/images/sparsity_2023.12.14_16.44.42.sif # dgx

# cuda_visible: "1"
time: "20:00:00"

params:

  name: "mamba_baseline"
  mixed_precision: true
  mixed_precision_dtype: bfloat16
  flash_attention: false

  n_blocks: 8
  # block_modules: ["mamba", "feedforward"]
  tags: ["100k", "baseline", "5e-4"]
  ff_mode: vanilla
  # no_positional_embedding: true
  dataset_type: c4
  # fsdp 
  fsdp_enabled: true
  fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  fsdp_selective_precision_modules: ["AttentionMechanism,ExpertGating"]
  grad_clip: 0.5
  learning_rate: 5e-4
  scheduler: "constant"
  lr_warmup_steps: 1000
  decoding_interval: 0
  save_weights_interval: 0
  init_type: truncated_normal
  init_scale: 0.1
  n_steps: 100_000
  final_lr_step: 100_000

  # dgx

  train_dataset_path: /raid/NFS_SHARE/datasets/c4/train/c4_train
  eval_dataset_path: /raid/NFS_SHARE/datasets/c4/validation/c4_validation

