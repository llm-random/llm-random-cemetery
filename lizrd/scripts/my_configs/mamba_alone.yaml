parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
# singularity_image: /common/llm-random/images/sparsity_2023.12.14_16.44.42.sif
# singularity_image: 
singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif
# cuda_visible: "1"
time: "20:00:00"

params:

  name: "mamba_weight_decay_skocznia"
  mixed_precision: true
  mixed_precision_dtype: bfloat16
  flash_attention: true

  n_blocks: 16
  block_modules: ["mamba"]

  tags: ["mamba",  "100k"]
  ff_mode: vanilla
  no_positional_embedding: true
  dataset_type: c4
  # fsdp 
  fsdp_enabled: true
  fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  fsdp_selective_precision_modules: ["AttentionMechanism,ExpertGating"]
  
  grad_clip: 0.5
  weight_decay: 0.1
  
  ^learning_rate: [1e-4, 2e-4, 5e-4, 1e-3]
  scheduler: "cosine"
  lr_warmup_steps: 1000
  decoding_interval: 0
  save_weights_interval: 0
  init_type: truncated_normal
  loss_checkpoint_chungs: 8
  init_scale: 0.1
  n_steps: 100_000
  final_lr_step: 100_000

  batch_size: 256
  cutoff: 256



# ---

# parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
# md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
# # singularity_image: /common/llm-random/images/sparsity_2023.12.14_16.44.42.sif
# # singularity_image: 
# singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif
# # cuda_visible: "1"
# time: "20:00:00"

# params:

#   name: "mamba_lr_grid"
#   mixed_precision: true
#   mixed_precision_dtype: bfloat16
#   flash_attention: true

#   n_blocks: 16
#   block_modules: ["mamba"]
#   tags: ["mamba",  "100k"]
#   ff_mode: vanilla
#   no_positional_embedding: true
#   dataset_type: c4
#   # fsdp 
#   fsdp_enabled: true
#   fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
#   activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
#   fsdp_selective_precision_modules: ["AttentionMechanism,ExpertGating"]
#   grad_clip: 0.5
  
#   learning_rate: 1e-3
#   scheduler: "constant"
#   lr_warmup_steps: 1000
#   decoding_interval: 0
#   save_weights_interval: 0
#   init_type: truncated_normal
#   loss_checkpoint_chungs: 8
#   init_scale: 0.1
#   n_steps: 100_000
#   final_lr_step: 100_000
#   weight_decay: 0.1

#   batch_size: 64
#   cutoff: 1024


# ---

# parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
# md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
# # singularity_image: /common/llm-random/images/sparsity_2023.12.14_16.44.42.sif
# # singularity_image: 
# singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif
# # cuda_visible: "1"
# time: "20:00:00"

# params:

#   name: "mamba_lr_grid"
#   mixed_precision: true
#   mixed_precision_dtype: bfloat16
#   flash_attention: true

#   n_blocks: 16
#   block_modules: ["mamba"]
#   tags: ["mamba",  "100k"]
#   ff_mode: vanilla
#   no_positional_embedding: true
#   dataset_type: c4
#   # fsdp 
#   fsdp_enabled: true
#   fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
#   activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
#   fsdp_selective_precision_modules: ["AttentionMechanism,ExpertGating"]
#   grad_clip: 0.5
  
#   learning_rate: 1e-3
#   scheduler: "constant"
#   lr_warmup_steps: 1000
#   decoding_interval: 0
#   save_weights_interval: 0
#   init_type: truncated_normal
#   loss_checkpoint_chungs: 8
#   init_scale: 0.1
#   n_steps: 100_000
#   final_lr_step: 100_000
#   weight_decay: 0.1

#   batch_size: 32
#   cutoff: 2048

# ---

# parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
# md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
# # singularity_image: /common/llm-random/images/sparsity_2023.12.14_16.44.42.sif
# # singularity_image: 
# singularity_image: /net/pr2/projects/plgrid/plggllmeffi/images/sparsity_2023.12.14_16.44.42.sif
# # cuda_visible: "1"
# time: "20:00:00"

# params:

#   name: "mamba_lr_grid"
#   mixed_precision: true
#   mixed_precision_dtype: bfloat16
#   flash_attention: true

#   n_blocks: 16
#   block_modules: ["mamba"]
#   tags: ["mamba",  "100k"]
#   ff_mode: vanilla
#   no_positional_embedding: true
#   dataset_type: c4
#   # fsdp 
#   fsdp_enabled: true
#   fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
#   activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
#   fsdp_selective_precision_modules: ["AttentionMechanism,ExpertGating"]
#   grad_clip: 0.5
  
#   learning_rate: 1e-3
#   scheduler: "constant"
#   lr_warmup_steps: 1000
#   decoding_interval: 0
#   save_weights_interval: 0
#   init_type: truncated_normal
#   loss_checkpoint_chungs: 8
#   init_scale: 0.1
#   n_steps: 100_000
#   final_lr_step: 100_000
#   weight_decay: 0.1

#   batch_size: 16
#   cutoff: 4096