{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If8NBlvj9huE"
      },
      "source": [
        "## Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhA4I3MzIwNn",
        "outputId": "fba49ff9-259c-49ff-9cf7-089247bf08ea"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# %pip install neptune plotly scikit-learn scipy torch seaborn tqdm multiprocess kaleido\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import neptune\n",
        "import resource\n",
        "from itertools import product\n",
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "from functools import reduce\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycHE6nK591dT"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRylyqY171SO"
      },
      "source": [
        "#### Calculate params count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K72Lhqhd-HSc"
      },
      "outputs": [],
      "source": [
        "def calculate_total_params(dmodel, expansion_rate, n_blocks, **_):\n",
        "    # assume no params in routing and embeddings\n",
        "    return dmodel**2 * (9*expansion_rate + 4) * n_blocks\n",
        "\n",
        "def calculate_active_params(dmodel, n_blocks, **_):\n",
        "    return calculate_total_params(dmodel=dmodel, n_blocks=n_blocks, expansion_rate=1)\n",
        "\n",
        "def calc_active_from_total(total_params, expansion_rate):\n",
        "  return total_params / (9*expansion_rate + 4) * 13\n",
        "\n",
        "def calc_total_from_active(active_params, expansion_rate):\n",
        "  return active_params * (9*expansion_rate + 4) / 13\n",
        "\n",
        "def calc_dmodel_from_active(active_params, depth_to_width_ratio=64):\n",
        "  # active = dmodel**2 * 13 * dmodel / depth_to_width_ratio\n",
        "  return (active_params*depth_to_width_ratio / 13)**(1/3)\n",
        "\n",
        "def calc_embedd_params_from_dmodel(dmodel, vocab_size=50257):\n",
        "  return 2*dmodel * vocab_size\n",
        "\n",
        "def calc_embedd_params_from_active(active_params):\n",
        "  return calc_embedd_params_from_dmodel(calc_dmodel_from_active(active_params))\n",
        "\n",
        "def calc_tokens_rampup(steps, first_transition=7629, second_transition=11443):\n",
        "  return 512*(steps * 512 - 128*np.minimum(steps, first_transition) - np.minimum(steps, second_transition) * 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEVom8OY77gu"
      },
      "source": [
        "#### Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fUgizu-8JpWb"
      },
      "outputs": [],
      "source": [
        "def format_large_numbers(num):\n",
        "    if abs(num) >= 1_000_000_000:\n",
        "        return f'{num / 1_000_000_000:.1f}B'\n",
        "    elif abs(num) >= 1_000_000:\n",
        "        return f'{num / 1_000_000:.1f}M'\n",
        "    else:\n",
        "        return str(num)\n",
        "\n",
        "def present_v(v):\n",
        "    if 1e6 <= v < 1e9:\n",
        "        return f\"{v/1e6:.2f}M\"\n",
        "    elif 1e9 <= v < 1e12:\n",
        "        return f\"{v/1e9:.2f}B\"\n",
        "    elif 1e12 <= v < 1e15:\n",
        "        return f\"{v/1e12:.2f}T\"\n",
        "    elif v >= 1e15:\n",
        "        return f\"{v:.5e}\"\n",
        "    elif isinstance(v, float):\n",
        "        return f\"{v:.6}\"\n",
        "    else:\n",
        "        return f\"{v}\"\n",
        "\n",
        "def rmse(L, L_pred):\n",
        "    return torch.sqrt(torch.mean((L - L_pred) ** 2))\n",
        "\n",
        "def get_train_valid_chunks(N, D, L, total_chunks, chunk_num):\n",
        "    chunk_size = len(N) // total_chunks\n",
        "    valid_start = chunk_num * chunk_size\n",
        "    valid_end = valid_start + chunk_size\n",
        "    N_valid = N[valid_start:valid_end]\n",
        "    D_valid = D[valid_start:valid_end]\n",
        "    L_valid = L[valid_start:valid_end]\n",
        "    N_train = torch.cat([N[:valid_start], N[valid_end:]])\n",
        "    D_train = torch.cat([D[:valid_start], D[valid_end:]])\n",
        "    L_train = torch.cat([L[:valid_start], L[valid_end:]])\n",
        "    return N_train, D_train, L_train, N_valid, D_valid, L_valid\n",
        "\n",
        "def print_scaling_law(A, alpha, B, beta, E):\n",
        "    A = A.item() if A is torch.Tensor else A\n",
        "    alpha = alpha.item() if alpha is torch.Tensor else alpha\n",
        "    B = B.item() if B is torch.Tensor else B\n",
        "    beta = beta.item() if beta is torch.Tensor else beta\n",
        "    e = E.item() if E is torch.Tensor else E\n",
        "    print(f\"Loss = {A:.2f} * -N^{alpha:.2f} + {B:.2f} * -D^{beta:.2f} + {e:.2f}\")\n",
        "\n",
        "def get_scaling_numbers(input_str):\n",
        "    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", input_str)\n",
        "    numbers = [float(i) for i in numbers]\n",
        "    if len(numbers) != 5:\n",
        "        raise ValueError('Input string must contain exactly 5 numbers.')\n",
        "    return tuple(numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mLkoju-VbSxX"
      },
      "outputs": [],
      "source": [
        "resource.setrlimit(resource.RLIMIT_NOFILE, (1000000, 1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v8RysZXDbSxY"
      },
      "outputs": [],
      "source": [
        "def lazy_run(file_path, function, always_run=False):\n",
        "    if os.path.exists(file_path) and not always_run:\n",
        "        print(f\"File '{file_path}' exists. Loading results from file.\")\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        print(f\"File '{file_path}' does not exist. Running the function.\")\n",
        "        result_df = function()\n",
        "        result_df.to_csv(file_path, index=False)  # Save the DataFrame to a CSV file\n",
        "        print(f\"Results saved to '{file_path}'.\")\n",
        "        return result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Ea7SJObSxZ"
      },
      "source": [
        "##### Rank hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8_EjZKC9bSxZ"
      },
      "outputs": [],
      "source": [
        "def clean_dataframe(data):\n",
        "    data = data[[col for col in data.columns if col.endswith('_init')]]\n",
        "    data = data.loc[:, data.nunique() > 1]\n",
        "    data = data.reset_index(drop=True)\n",
        "    return data\n",
        "\n",
        "def calculate_rank(data, column):\n",
        "    ranks = {}\n",
        "    unique_values = data[column].unique()\n",
        "\n",
        "    for value in unique_values:\n",
        "        positions = data[data[column] == value].index.to_numpy()\n",
        "        average_position = positions.mean() / len(data)\n",
        "        ranks[value] = average_position\n",
        "\n",
        "    return ranks\n",
        "\n",
        "def calculate_ranks_for_all_columns(data):\n",
        "    all_ranks = {}\n",
        "\n",
        "    for column in data.columns:\n",
        "        ranks = calculate_rank(data, column)\n",
        "        all_ranks[column] = ranks\n",
        "\n",
        "    return all_ranks\n",
        "\n",
        "def format_ranks_for_display(ranks):\n",
        "  print(\"Scores of grid-parameters (average position, so the lower the better):\\n\")\n",
        "  for column, values in ranks.items():\n",
        "      print(f\"Column: {column}\")\n",
        "      sorted_values = sorted(values.items(), key=lambda item: item[1])\n",
        "      for value, rank in sorted_values:\n",
        "          print(f\"  Value: {value}, Rank: {rank:.4f}\")\n",
        "      print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRSllIAXOlyb"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tuwDTI0NOijH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from matplotlib.lines import Line2D  # For custom legend handles\n",
        "\n",
        "def plot_pred_vs_actual(\n",
        "    L,\n",
        "    L_pred,\n",
        "    E_train,\n",
        "    L_valid=None,\n",
        "    L_valid_pred=None,\n",
        "    E_valid=None,\n",
        "    L_test=None,\n",
        "    L_test_pred=None,\n",
        "    E_test=None,\n",
        "    in_log=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots predicted vs actual values for training, validation, and test datasets with color encoding based on distinct \"E\" values.\n",
        "\n",
        "    Parameters:\n",
        "    - L (array-like or torch.Tensor): Actual values for training.\n",
        "    - L_pred (array-like or torch.Tensor): Predicted values for training.\n",
        "    - E_train (array-like or torch.Tensor): \"E\" values for training data, used for coloring.\n",
        "    - L_valid (array-like or torch.Tensor, optional): Actual values for validation.\n",
        "    - L_valid_pred (array-like or torch.Tensor, optional): Predicted values for validation.\n",
        "    - E_valid (array-like or torch.Tensor, optional): \"E\" values for validation data, used for coloring.\n",
        "    - L_test (array-like or torch.Tensor, optional): Actual values for testing.\n",
        "    - L_test_pred (array-like or torch.Tensor, optional): Predicted values for testing.\n",
        "    - E_test (array-like or torch.Tensor, optional): \"E\" values for test data, used for coloring.\n",
        "    - in_log (bool, default=True): If True, plots the logarithm of the values. Otherwise, plots the raw values.\n",
        "    \"\"\"\n",
        "    # Create a figure and axis using the OO interface\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Helper function to convert data to numpy arrays\n",
        "    def to_numpy(data):\n",
        "        if isinstance(data, torch.Tensor):\n",
        "            return data.detach().cpu().numpy()\n",
        "        elif isinstance(data, list):\n",
        "            return np.array(data)\n",
        "        else:\n",
        "            return np.array(data)\n",
        "\n",
        "    # Helper function to convert E values to scalars\n",
        "    def to_scalar(e):\n",
        "        if isinstance(e, torch.Tensor):\n",
        "            return e.item()\n",
        "        else:\n",
        "            return e\n",
        "\n",
        "    # Convert E_train, E_valid, E_test to numpy arrays of scalars\n",
        "    E_train_np = to_numpy(E_train) if E_train is not None else None\n",
        "    E_valid_np = to_numpy(E_valid) if E_valid is not None else None\n",
        "    E_test_np = to_numpy(E_test) if E_test is not None else None\n",
        "\n",
        "    # Initialize lists to collect all E values for consistent color mapping\n",
        "    all_E = []\n",
        "    if E_train_np is not None:\n",
        "        all_E.extend(E_train_np)\n",
        "    if E_valid_np is not None:\n",
        "        all_E.extend(E_valid_np)\n",
        "    if E_test_np is not None:\n",
        "        all_E.extend(E_test_np)\n",
        "\n",
        "    # Identify unique E values\n",
        "    unique_E = sorted(list(set(all_E)))\n",
        "    num_unique_E = len(unique_E)\n",
        "\n",
        "    # Assign a distinct color to each unique E value using a discrete colormap\n",
        "    # Choose 'tab10' for up to 10 unique values, 'tab20' for up to 20\n",
        "    if num_unique_E <= 10:\n",
        "        cmap = plt.get_cmap('tab10', num_unique_E)\n",
        "    elif num_unique_E <= 20:\n",
        "        cmap = plt.get_cmap('tab20', num_unique_E)\n",
        "    else:\n",
        "        # For more than 20 unique E values, use a larger colormap or custom colors\n",
        "        cmap = plt.get_cmap('tab20', num_unique_E)\n",
        "        print(\"Warning: More than 20 unique E values may result in color repetition.\")\n",
        "\n",
        "    # Create color_map dictionary\n",
        "    color_map = {e: cmap(i) for i, e in enumerate(unique_E)}\n",
        "\n",
        "    # Transformation function to handle torch tensors and apply log if needed\n",
        "    def transform(data):\n",
        "        data = to_numpy(data)\n",
        "        if in_log:\n",
        "            # Ensure all data is positive before applying log\n",
        "            if np.any(data <= 0):\n",
        "                raise ValueError(\"All values must be positive to apply logarithm.\")\n",
        "            data = np.log(data)\n",
        "        return data\n",
        "\n",
        "    # Apply transformation to all datasets\n",
        "    try:\n",
        "        actual_train = transform(L)\n",
        "        pred_train = transform(L_pred)\n",
        "\n",
        "        if L_valid is not None and L_valid_pred is not None and E_valid_np is not None:\n",
        "            actual_valid = transform(L_valid)\n",
        "            pred_valid = transform(L_valid_pred)\n",
        "        if L_test is not None and L_test_pred is not None and E_test_np is not None:\n",
        "            actual_test = transform(L_test)\n",
        "            pred_test = transform(L_test_pred)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error in data transformation: {e}\")\n",
        "        return\n",
        "\n",
        "    # Plot Training Data with circles ('o')\n",
        "    scatter_train = ax.scatter(\n",
        "        actual_train,\n",
        "        pred_train,\n",
        "        c=[color_map[e] for e in E_train_np],\n",
        "        marker='o',\n",
        "        label=\"Training\",\n",
        "        edgecolor='k',  # Edge color for better visibility\n",
        "        alpha=0.7\n",
        "    )\n",
        "\n",
        "    # Plot Validation Data with squares ('s') if provided\n",
        "    if L_valid is not None and L_valid_pred is not None and E_valid_np is not None:\n",
        "        scatter_valid = ax.scatter(\n",
        "            actual_valid,\n",
        "            pred_valid,\n",
        "            c=[color_map[e] for e in E_valid_np],\n",
        "            marker='s',  # Square marker for validation\n",
        "            label=\"Validation\",\n",
        "            edgecolor='k',\n",
        "            alpha=0.7\n",
        "        )\n",
        "\n",
        "    # Plot Test Data with 'X' markers if provided\n",
        "    if L_test is not None and L_test_pred is not None and E_test_np is not None:\n",
        "        scatter_test = ax.scatter(\n",
        "            actual_test,\n",
        "            pred_test,\n",
        "            c=[color_map[e] for e in E_test_np],\n",
        "            marker='x',\n",
        "            label=\"Test\",\n",
        "            alpha=0.7\n",
        "            # Removed edgecolor to avoid UserWarning\n",
        "        )\n",
        "\n",
        "    # Determine the range for the reference line\n",
        "    min_vals = []\n",
        "    max_vals = []\n",
        "    if L is not None:\n",
        "        min_vals.append(actual_train.min())\n",
        "        max_vals.append(actual_train.max())\n",
        "    if L_valid is not None:\n",
        "        min_vals.append(actual_valid.min())\n",
        "        max_vals.append(actual_valid.max())\n",
        "    if L_test is not None:\n",
        "        min_vals.append(actual_test.min())\n",
        "        max_vals.append(actual_test.max())\n",
        "    min_val = min(min_vals)\n",
        "    max_val = max(max_vals)\n",
        "\n",
        "    if in_log:\n",
        "        # Logarithmic ticks\n",
        "        major_ticks = np.logspace(np.floor(np.log10(min_val)), np.ceil(np.log10(max_val)), num=10)\n",
        "        minor_ticks = []\n",
        "    else:\n",
        "        # Linear ticks with major intervals of 0.1\n",
        "        major_ticks = np.round(np.arange(np.floor(min_val), np.ceil(max_val) + 0.1, 0.1), 1)\n",
        "        minor_ticks = np.round(np.arange(np.floor(min_val), np.ceil(max_val) + 0.05, 0.05), 1)\n",
        "\n",
        "    # Apply major and minor ticks to the axes\n",
        "    ax.set_xticks(major_ticks)\n",
        "    if not in_log:\n",
        "        ax.set_xticks(minor_ticks, minor=True)\n",
        "    ax.set_yticks(major_ticks)\n",
        "    if not in_log:\n",
        "        ax.set_yticks(minor_ticks, minor=True)\n",
        "\n",
        "    # Add grid lines for both major and minor ticks\n",
        "    ax.grid(True, which='both', linestyle='--', alpha=0.5)\n",
        "    ax.grid(True, which='major', linestyle='-', linewidth=0.8)\n",
        "\n",
        "    # Plot the ideal reference line (y = x)\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], color=\"red\", linestyle=\"--\", label=\"Ideal\")\n",
        "\n",
        "    # Set axis labels\n",
        "    ax.set_xlabel(\"log(Actual)\" if in_log else \"Actual\", fontsize=14)\n",
        "    ax.set_ylabel(\"log(Predicted)\" if in_log else \"Predicted\", fontsize=14)\n",
        "\n",
        "    # Create custom legend for datasets\n",
        "    dataset_handles = [\n",
        "        Line2D([0], [0], marker='o', color='w', label='Training',\n",
        "               markerfacecolor='grey', markersize=10, markeredgecolor='k'),\n",
        "        Line2D([0], [0], marker='s', color='w', label='Validation',\n",
        "               markerfacecolor='grey', markersize=10, markeredgecolor='k'),\n",
        "        Line2D([0], [0], marker='x', color='grey', label='Test',\n",
        "               markerfacecolor='grey', markersize=10)\n",
        "    ]\n",
        "\n",
        "    # Create custom legend for E values\n",
        "    E_handles = [Line2D([0], [0], marker='o', color='w', label=str(int(e) if e.is_integer() else e),\n",
        "                        markerfacecolor=color_map[e], markersize=10) for e in unique_E]\n",
        "\n",
        "    # Add legends to the plot\n",
        "    legend1 = ax.legend(handles=dataset_handles, title=\"Dataset\", loc='upper left')\n",
        "    ax.add_artist(legend1)  # Add the first legend manually\n",
        "    ax.legend(handles=E_handles, title=\"E Values\", loc='lower right')\n",
        "\n",
        "    # Add grid for better readability\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Adjust layout for better spacing\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP4ghZpYIY92"
      },
      "source": [
        "# Downloading Data from Neptune\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPOAkh4pCh4"
      },
      "source": [
        "#### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oD_usITvIXvH"
      },
      "outputs": [],
      "source": [
        "def download_from_neptune(tag):\n",
        "  project = neptune.init_project(\n",
        "        project=\"pmtest/llm-random\",\n",
        "        mode=\"read-only\",\n",
        "        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNDYxZjgzZC0xYTljLTQwZGQtOTVjNC02MTI5ZTc4ZjBiNGIifQ==\"\n",
        "  )\n",
        "  df = project.fetch_runs_table(tag=f\"{tag}\").to_pandas()\n",
        "  df.drop(list(df.filter(regex = 'monitoring')), axis = 1, inplace = True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def is_token_exponential(token_count):\n",
        "  log_tokens = np.log2(token_count/1e9)\n",
        "  print(f\"{np.unique(np.round(log_tokens, 1))=}\")\n",
        "  return np.round(log_tokens, 1) == np.round(log_tokens, 0)\n",
        "\n",
        "\n",
        "def get_full_table(raw_table, name, exp_rate=None, tokens_counted_per_active=False, tokens_counted_per_total=False, table=None, negative_tags=None, granularities=None,\n",
        "                   use_active=False, loss_column=\"loss_interval/10\", with_embeddings=False, thresh_active=(-np.inf, np.inf), thresh_length=(-np.inf, np.inf), filter_non_exp_d=False,\n",
        "                   thresh_token_to_param_ratio=(-np.inf, np.inf), loss_thresh=(-np.inf, np.inf), attn_heads_to_remove=()):\n",
        "\n",
        "  table = raw_table[raw_table[\"args/name\"] == name]\n",
        "  table = table[table[\"sys/state\"] == \"Inactive\"]\n",
        "  table = table[table[\"lr\"]==0]  # finished trapezoid schedule\n",
        "  table[\"loss\"] = table[loss_column]\n",
        "  table = table[np.isfinite(table[\"loss\"])]\n",
        "\n",
        "\n",
        "  table[\"total_params\"] = table.apply(lambda row: calculate_total_params(row[\"args/dmodel\"], row[\"args/expansion_rate\"], row[\"args/n_blocks\"]), axis=1)\n",
        "  table[\"active_params\"] = table.apply(lambda row: calculate_active_params(dmodel=row[\"args/dmodel\"], n_blocks=row[\"args/n_blocks\"]), axis=1)\n",
        "  table[\"params\"] = table[\"active_params\"] if use_active else table[\"total_params\"]\n",
        "  table[\"tokens\"] = calc_tokens_rampup(table[\"step\"])\n",
        "\n",
        "  table[\"embed_params\"] = calc_embedd_params_from_dmodel(table[\"args/dmodel\"])\n",
        "\n",
        "  table[\"active_with_embed\"] = table[\"active_params\"] + table[\"embed_params\"]\n",
        "  table[\"total_with_embed\"] = table[\"total_params\"] + table[\"embed_params\"]\n",
        "\n",
        "  if with_embeddings:\n",
        "    table[\"params\"] += table[\"embed_params\"]\n",
        "\n",
        "  table[\"str_active_non_embed\"] = table[\"active_params\"].apply(format_large_numbers)\n",
        "  table[\"str_params\"] = table[\"params\"].apply(format_large_numbers)\n",
        "  table[\"str_tokens\"] = table[\"tokens\"].apply(format_large_numbers)\n",
        "\n",
        "\n",
        "  # filters\n",
        "  if filter_non_exp_d:\n",
        "    table = table[is_token_exponential(table[\"tokens\"])]\n",
        "\n",
        "  for attn_heads in attn_heads_to_remove:\n",
        "    table = table[table[\"args/n_att_heads\"] != attn_heads]\n",
        "\n",
        "  table = table[table.apply(lambda x:not \"remove_constrained_scaling_laws\" in x[\"sys/tags\"],axis=1)]\n",
        "  if exp_rate != None:\n",
        "      if not isinstance(exp_rate, list):\n",
        "          exp_rate = [exp_rate]\n",
        "      table = table[table[\"args/expansion_rate\"].isin(exp_rate)]\n",
        "\n",
        "  table = table[table[\"active_params\"] >= thresh_active[0]]\n",
        "  table = table[table[\"active_params\"] <= thresh_active[1]]\n",
        "  table = table[table[\"tokens\"] >= thresh_length[0]]\n",
        "  table = table[table[\"tokens\"] <= thresh_length[1]]\n",
        "  table = table[table[\"tokens\"] / table[\"active_params\"] >= thresh_token_to_param_ratio[0]]\n",
        "  table = table[table[\"tokens\"] / table[\"active_params\"] <= thresh_token_to_param_ratio[1]]\n",
        "  table = table[table[\"loss\"] >= loss_thresh[0]]\n",
        "  table = table[table[\"loss\"] <= loss_thresh[1]]\n",
        "\n",
        "  if tokens_counted_per_active:\n",
        "    table[\"tokens\"] = table[\"tokens\"] / table[\"active_params\"]\n",
        "  elif tokens_counted_per_total:\n",
        "    table[\"tokens\"] = table[\"tokens\"] / table[\"total_params\"]\n",
        "\n",
        "  print(f\"Number of experiments: {len(table)}\")\n",
        "  print(\"All sizes and token counts in table:\")\n",
        "  print(table[\"str_params\"].unique())\n",
        "  print(table[\"str_tokens\"].unique())\n",
        "  return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUMewLHWqBAr"
      },
      "outputs": [],
      "source": [
        "def extract_params(table, name=\"Train\"):\n",
        "  print(f\"{name} size: {len(table)}\")\n",
        "  print(f\"{name} params: {table['str_params'].unique()}\")\n",
        "  min_loss, max_loss = table[\"loss\"].min(), table[\"loss\"].max()\n",
        "  print(f\"{name} losses interval: ({min_loss:.4f}, {max_loss:.4f}\")\n",
        "  return (torch.Tensor(table[\"params\"].tolist()).requires_grad_(False),\n",
        "          torch.Tensor(table[\"tokens\"].tolist()).requires_grad_(False),\n",
        "          torch.Tensor(table[\"loss\"].tolist()).requires_grad_(False),\n",
        "          torch.Tensor(table[\"args/expansion_rate\"].tolist()).requires_grad_(False))\n",
        "\n",
        "def get_train_test_values(table, topn_losses_for_test):\n",
        "  N_test = D_test = L_test = E_test = None\n",
        "\n",
        "  if topn_losses_for_test is not None:\n",
        "    unique_losses = table[\"loss\"].unique()\n",
        "    test_sizes = unique_losses[np.argsort(unique_losses)[:topn_losses_for_test]].tolist()\n",
        "\n",
        "    split_condition = table[\"loss\"].isin(test_sizes)\n",
        "    test_table = table[split_condition]\n",
        "    train_table = table[~split_condition]\n",
        "    assert len(train_table) + len(test_table) == len(table)\n",
        "\n",
        "    N_test, D_test, L_test, E_test = extract_params(test_table, \"Test\")\n",
        "    table = train_table\n",
        "\n",
        "  N, D, L, E = extract_params(table)\n",
        "  return N, D, L, E, N_test, D_test, L_test, E_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hGX00WfoZXC"
      },
      "source": [
        "### Fetch dataset (slow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "925ef98985ac444f924fc106df54c484",
            "8e5d34f526604ad399f4421b25b0046e",
            "1224d4ca62df4f25a371f72d532b2335",
            "72cdcfc03440460582d36a6e9061757a",
            "d31935f7d621472296919548f100deaf",
            "bad72c67668b4b4fb98d82c2414016e7",
            "3651325ade9048208a671b3c4a39684d",
            "66245d45d99648b386ade7370d3cf28b",
            "f794b4dbdf3f40caacf0010c7ea80fa3",
            "2724ebddf2d04c6da3544ce7141b9b95",
            "a477bb3e2ba44ffeb04630de6ee8ebaf"
          ]
        },
        "id": "yB2MNQc_giEN",
        "outputId": "f586b438-88a9-4cd0-99d0-0edaa178c8d8"
      },
      "outputs": [],
      "source": [
        "raw_table = lazy_run(\"neptune_data_const.csv\", lambda: download_from_neptune(\"constrained_scaling_grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2_4Rxp_IbZO"
      },
      "source": [
        "# Scaling Formula\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37kM-mWrpE-c"
      },
      "source": [
        "### Predict scaling law\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFjz6kiMUBk0"
      },
      "source": [
        "#### Predict functions in log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NWHlCAGjUAV1"
      },
      "outputs": [],
      "source": [
        "def predict_loss_in_log(N, a, alpha, D, b, beta, c, Nb=None, Nb_a=torch.nan, Nb_alpha=0, **_):\n",
        "    if Nb is None:\n",
        "      Nb = calc_embedd_params_from_active(N)\n",
        "    if c.shape != D.shape:\n",
        "        c = c.repeat(D.shape[0])\n",
        "    parts = [a-alpha*torch.log(N), b-beta*torch.log(D), c]\n",
        "    if isinstance(Nb_a, torch.Tensor) and not torch.isnan(Nb_a):\n",
        "      parts.append(Nb_a-Nb_alpha*torch.log(Nb))\n",
        "    return torch.logsumexp(torch.stack(parts), 0)\n",
        "\n",
        "def map_separate_params(chinchilla_params, E, Nb_a=torch.nan, Nb_alpha=0, **other_params):\n",
        "  E_rounded = (2**torch.round(torch.log2(E))).to(torch.int)\n",
        "\n",
        "  for key in chinchilla_params.keys():\n",
        "    if len(E.shape) > 0:\n",
        "      for i, e in enumerate(E_rounded):\n",
        "        value = other_params.get(f\"{key}_{e}\", math.nan)\n",
        "        if not math.isnan(value):\n",
        "          chinchilla_params[key][i] = value\n",
        "    else: #  2**int(round(torch.log2(E).item()))\n",
        "        value = other_params.get(f\"{key}_{E_rounded}\", math.nan)\n",
        "        if not math.isnan(value):\n",
        "          chinchilla_params[key] = value\n",
        "\n",
        "  if isinstance(Nb_a, torch.Tensor) and not torch.isnan(Nb_a):\n",
        "    chinchilla_params[\"Nb_a\"] = Nb_a\n",
        "    chinchilla_params[\"Nb_alpha\"] = Nb_alpha\n",
        "\n",
        "def project_joined_to_chinchilla_in_log(a, b, c, alpha, beta, gamma, delta, omega, zeta, E, e_start, e_end, **other_params):\n",
        "  e_start = torch.exp(-e_start)\n",
        "  e_end = torch.exp(-e_end)\n",
        "  e_dash = torch.log((E - 0.99 + (e_start - e_end)**-1)**-1 + e_end)\n",
        "\n",
        "  a_e = a + delta*e_dash\n",
        "  b_e = b + omega*e_dash\n",
        "  alpha_e = (alpha - e_dash*gamma)\n",
        "  beta_e = (beta - e_dash*zeta)\n",
        "\n",
        "  chinchilla_params = {\"a\":a_e, \"b\":b_e, \"c\":c, \"alpha\": alpha_e, \"beta\": beta_e}\n",
        "  map_separate_params(chinchilla_params, E, **other_params)\n",
        "  return chinchilla_params\n",
        "\n",
        "def predict_loss_joined_in_log(N, D, E, Nb=None, **params):\n",
        "    return predict_loss_in_log(N=N, D=D, Nb=Nb, **project_joined_to_chinchilla_in_log(E=E, **params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0TlX_WjUNGG"
      },
      "source": [
        "#### Wrappers for evaluating normal functions in log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MUe-f6DTURuX"
      },
      "outputs": [],
      "source": [
        "def predict_loss_torch(N, A, alpha, D, B, beta, C, Nb=None, Nb_a=torch.nan, Nb_alpha=0, **_):\n",
        "    if N is None or D is None or E is None:\n",
        "        return None\n",
        "    return torch.exp(predict_loss_in_log(N=N, a=torch.log(A), alpha=alpha, D=D, b=torch.log(B), beta=beta, c=torch.log(C), Nb_a=Nb_a, Nb_alpha=Nb_alpha, Nb=Nb))\n",
        "\n",
        "def predict_loss_joined_torch(N, D, E, A, B, C, **params):\n",
        "    if N is None or D is None or E is None:\n",
        "        return None\n",
        "    return torch.exp(predict_loss_joined_in_log(N=N, D=D, E=E, a=torch.log(A), b=torch.log(B), c=torch.log(C), **params))\n",
        "\n",
        "def project_joined_to_chinchilla_torch(A, B, C, **params):\n",
        "    params = project_joined_to_chinchilla_in_log(a=torch.log(A), b=torch.log(B), c=torch.log(C), **params)\n",
        "    add_params = {}\n",
        "    if \"Nb_a\" in params:\n",
        "      add_params = {\"Nb_a\": params[\"Nb_a\"], \"Nb_alpha\": params[\"Nb_alpha\"]}\n",
        "    return {\"A\":torch.exp(params[\"a\"]), \"B\":torch.exp(params[\"b\"]), \"C\":torch.exp(params[\"c\"]), \"alpha\": params[\"alpha\"], \"beta\": params[\"beta\"], **add_params}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGM-U4nTlye6"
      },
      "source": [
        "#### Wrappers for numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cg4QPsMclxnG"
      },
      "outputs": [],
      "source": [
        "def change_all_to_torch(**args):\n",
        "    return {k: torch.tensor(v) if isinstance(v, np.generic) or isinstance(v, np.ndarray) or isinstance(v, float) or isinstance(v, int)  else v for k, v in args.items()}\n",
        "\n",
        "def change_all_to_numpy(**args):\n",
        "    return {k: v.numpy() for k, v in args.items()}\n",
        "\n",
        "def project_joined_to_chinchilla(**params):\n",
        "    return change_all_to_numpy(**project_joined_to_chinchilla_torch(**change_all_to_torch(**params)))\n",
        "\n",
        "def predict_loss(**params):\n",
        "    w = predict_loss_torch(**change_all_to_torch(**params))\n",
        "    return w.numpy() if isinstance(params[\"N\"], np.ndarray) else w\n",
        "\n",
        "def predict_loss_joined(**params):\n",
        "    w = predict_loss_joined_torch(**change_all_to_torch(**params))\n",
        "    return w.numpy() if isinstance(params[\"N\"], np.ndarray) else w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP8rnDeSIysg"
      },
      "source": [
        "### Joint Scaling Law Fitting\n",
        "\n",
        "$ L_{Ch}(N, D) = aN^{-\\alpha} + bD^{-\\beta} + c $\n",
        "\n",
        "\n",
        "$ L(N, D, E') = aN^{\\alpha}E'^{\\delta + \\gamma log(N) } + bD^{\\beta}E'^{\\omega + \\zeta log(D)} + c $\n",
        "\n",
        "equivalent form:\n",
        "\n",
        "$L(N, D, E') = {a{E'}^{\\delta}}N^{\\alpha + {\\gamma}{\\texttt{log}}(E')} + {b{E'}^{\\omega}}D^{\\beta + {\\zeta}{\\texttt{log}}(E')} + c$\n",
        "\n",
        "\n",
        "$ L(N, D, E') = a(E) N^{\\alpha(E)} + b(E) D^{\\beta(E) } + c. $\n",
        "\n",
        "for\n",
        "\n",
        "$a(E')=a{E'}^{\\delta}$, $\\alpha(E')={\\alpha + {\\gamma}{\\texttt{log}}(E')}$, $b(E')=b{E'}^{\\omega}$, $\\beta(E')={\\beta + {\\zeta}{\\texttt{log}}(E')}$\n",
        "\n",
        "\n",
        "$\\frac{1}{E'} = \\frac{1}{E +\\mathbf{1}_{E=1}\\phi - 1 + \\left(\\frac{1}{E_{start}} - \\frac{1}{E_{max}}\\right)^{-1}} + \\frac{1}{E_{max}},$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6hTrci5soTVL"
      },
      "outputs": [],
      "source": [
        "def get_example_weight(log_L, N, D, loss_weight, pareto_distance_weigth):\n",
        "    example_weight = (log_L ** loss_weight)\n",
        "    example_weight *= (torch.clamp(40*N/D, max=1))**pareto_distance_weigth\n",
        "    example_weight = example_weight/example_weight.sum() * len(example_weight)\n",
        "    return example_weight\n",
        "\n",
        "def compute_joined_scaling_law(N, a, alpha, D, b, beta, c, E, delta, gamma, omega, zeta, L, huber_delta, Nb_a=np.nan, Nb_alpha=0, e_start=1.0, e_end=1.0, e_a_conv=1.0, e_b_conv=1.0, e_1_param=1.0, e_2_param=0.0, e_1_term=0, e_1_special=False,\n",
        "                               weight_decay=0, optimize_c=False, lr=0.1, alternative_form=True, with_estart=False, with_eend=False, with_e_conv=True, loss_weight=0, pareto_distance_weigth=0.,\n",
        "                               history_size=10000, max_iter=100000, line_search_fn=\"strong_wolfe\", steps=1, separate_chinchillas_for=(), tolerance_change=1e-9, tolerance_grad=1e-7, **add_params):\n",
        "    \"\"\"\n",
        "    params:\n",
        "    N, a, alpha, D, b, beta, e: initial values for the parameters of the scaling law\n",
        "    huber_delta: delta parameter for the Huber loss\n",
        "    weight_decay: weight decay parameter for the L1 regularization\n",
        "    separate_chinchillas_for: tuple of expansion rates for a separate chinchillas\n",
        "    \"\"\"\n",
        "    create_tensor = lambda val, log=False, grad=True: torch.tensor([math.log(val) if log else val], dtype=torch.float64).requires_grad_(grad)\n",
        "\n",
        "    N, D, L, E = [x.to(torch.float64) for x in [N, D, L, E]]\n",
        "\n",
        "    params_dict = {\n",
        "        \"a\": create_tensor(a, log=True),\n",
        "        \"b\": create_tensor(b, log=True),\n",
        "        \"c\": create_tensor(c, log=True, grad=optimize_c),\n",
        "        \"alpha\": create_tensor(alpha), \"beta\": create_tensor(beta),  \"delta\": create_tensor(delta),\n",
        "        \"gamma\": create_tensor(gamma), \"omega\": create_tensor(omega), \"zeta\": create_tensor(zeta),\n",
        "        \"e_start\": create_tensor(e_start, log=True, grad=with_estart),\n",
        "        \"e_end\": create_tensor(e_end, log=True, grad=with_eend),\n",
        "        **{k: create_tensor(v) for k, v in add_params.items()},\n",
        "    }\n",
        "\n",
        "    if not np.isnan(Nb_a):\n",
        "      params_dict[\"Nb_a\"] = create_tensor(Nb_a, log=True)\n",
        "      params_dict[\"Nb_alpha\"] = create_tensor(Nb_alpha)\n",
        "\n",
        "    for separate_E in separate_chinchillas_for:\n",
        "      for base_param in ['a', 'b', 'alpha', 'beta']:\n",
        "        params_dict[f\"{base_param}_{separate_E}\"] =  nn.Parameter(params_dict[base_param].clone())\n",
        "\n",
        "    params = params_dict.values()\n",
        "\n",
        "    def objective():\n",
        "        inp = predict_loss_joined_in_log(N=N, D=D, E=E, **params_dict)\n",
        "        target = torch.log(L)\n",
        "        example_weight = get_example_weight(target, N, D, loss_weight, pareto_distance_weigth)\n",
        "        loss = F.huber_loss(inp, target, delta=huber_delta, reduction='none') * example_weight\n",
        "        loss = loss.sum()\n",
        "        l2_reg = sum([param.sum()**2 for param in params])\n",
        "        loss += weight_decay * l2_reg\n",
        "        return loss\n",
        "\n",
        "    def closure():\n",
        "        lbfgs.zero_grad()\n",
        "        loss = objective()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    lbfgs = optim.LBFGS(params,\n",
        "                        history_size=history_size,\n",
        "                        lr=lr,\n",
        "                        max_iter=max_iter,\n",
        "                        tolerance_change=tolerance_change,\n",
        "                        tolerance_grad=tolerance_grad,\n",
        "                        line_search_fn=line_search_fn)\n",
        "    for i in range(steps):\n",
        "      lbfgs.step(closure)\n",
        "\n",
        "    return_dict = dict(\n",
        "        **params_dict,\n",
        "        A = torch.exp(params_dict['a']),\n",
        "        B = torch.exp(params_dict['b']),\n",
        "        C = torch.exp(params_dict['c']),\n",
        "    )\n",
        "    del return_dict['a']\n",
        "    del return_dict['b']\n",
        "    del return_dict['c']\n",
        "\n",
        "    return_dict =  {\n",
        "        key: (value.detach() if isinstance(value, torch.Tensor) else value)\n",
        "        for key, value in return_dict.items()\n",
        "    }\n",
        "\n",
        "    return return_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYnd-g4lLCHF"
      },
      "source": [
        "### Compute optimal curve calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NULQ7abOnwIM"
      },
      "outputs": [],
      "source": [
        "def get_optimal_config_for_flops(A, alpha, B, beta, flops, **_):\n",
        "    a = beta / (alpha + beta)\n",
        "    b = alpha / (alpha + beta)\n",
        "    G = (alpha*A / (beta*B)) ** (1 / (alpha + beta))\n",
        "    d_opt = (flops / 6) ** b / G\n",
        "    n_opt = (flops / 6) ** a * G\n",
        "    return n_opt, d_opt, a, b\n",
        "\n",
        "def binsearch(eval, start, end, tolerance=1e-5):\n",
        "    mid = (start + end) / 2\n",
        "    if abs(end - start) <= tolerance:\n",
        "        return mid\n",
        "    elif eval(mid) > 0:\n",
        "        return binsearch(eval, start, mid, tolerance)\n",
        "    else:\n",
        "        return binsearch(eval, mid, end, tolerance)\n",
        "\n",
        "def calc_flops(N, D):\n",
        "    Nb = calc_embedd_params_from_active(N)\n",
        "    return 6 * (N + Nb) * D\n",
        "\n",
        "def calc_D_from_active(N, flops):\n",
        "    Nb = calc_embedd_params_from_active(N)\n",
        "    D = flops / (6 * (N + Nb))\n",
        "    return D\n",
        "\n",
        "def calc_compute_optimal(flops, single_chinchilla_params=None, **params):\n",
        "    if single_chinchilla_params is not None:\n",
        "      n_opt, d_opt, a, b = get_optimal_config_for_flops(**single_chinchilla_params, flops=flops)\n",
        "      return n_opt, 0, d_opt  # this does not calculate separate embeddings but we dont need them\n",
        "\n",
        "    predict = lambda N,D: predict_loss_joined(N=N, D=D, **params)\n",
        "    predict_loss_for_N = lambda N: predict(N, calc_D_from_active(N, flops))\n",
        "    #print(f\"{flops=} {predict_loss_for_N(1e8)=} {predict_loss_for_N(1e9)=}\")\n",
        "    N = (10**(binsearch(lambda logN: - predict_loss_for_N(10**logN) + predict_loss_for_N((10**logN)*1.01), 3, 40, 1e-3)))*1.005\n",
        "    D = calc_D_from_active(N, flops)\n",
        "    Nb = calc_embedd_params_from_active(N)\n",
        "    return N, Nb, D\n",
        "\n",
        "\n",
        "def get_optimal_D_for_N_with_embeddings(N_with_emb, **params):\n",
        "    flops = 10**binsearch(lambda log_flops: sum(calc_compute_optimal(10**log_flops, **params)[:2]) - N_with_emb, 3, 60, 1e-3)\n",
        "    return calc_compute_optimal(flops, **params)[-1]\n",
        "\n",
        "\n",
        "def get_optimal_D_for_N_no_emb(N, **params):\n",
        "    flops = 10**binsearch(lambda log_flops: calc_compute_optimal(10**log_flops, **params)[0] - N, 3, 60, 1e-3)\n",
        "    return calc_compute_optimal(flops, **params)\n",
        "\n",
        "\n",
        "def print_compute_optimal_configs_for_N_no_emb(Ns, **params):\n",
        "  for N in Ns:\n",
        "      N_, Nb, D = get_optimal_D_for_N_no_emb(N, **params)\n",
        "      assert abs(N_ - N)/N < 0.2\n",
        "      print(f\"Optimal for N (active no embeddings) {present_v(N):7} : D: {present_v(D):7}, embeddings Nb: {present_v(Nb):7}  D/N: {present_v(D/(N)):7} \")\n",
        "\n",
        "def print_compute_optimal_configs_for_N_with_emb(N_with_embs, single_chinchilla_params, **params):\n",
        "  for N_with_emb in N_with_embs:\n",
        "      D = get_optimal_D_for_N_with_embeddings(N_with_emb, single_chinchilla_params=single_chinchilla_params, **params)\n",
        "      print(f\"Optimal for N (with embeddings) {present_v(N_with_emb):7} : D: {present_v(D):7} D/N: {present_v(D/(N_with_emb)):7} \")\n",
        "\n",
        "def print_compute_optimal_configs_for_N(Ns, with_embeddings, single_chinchilla_params, **params):\n",
        "  if with_embeddings:\n",
        "    print_compute_optimal_configs_for_N_with_emb(Ns, single_chinchilla_params=single_chinchilla_params, **params)\n",
        "  else:\n",
        "    print_compute_optimal_configs_for_N_no_emb(Ns, **params)\n",
        "\n",
        "def print_compute_optimal_configs_for_flops_binsearch(flop_budgets, with_embeddings, **params):\n",
        "  for flops in flop_budgets:\n",
        "    N, Nb, D = calc_compute_optimal(flops, **params)\n",
        "    print(f\"Optimal for {flops:10} FLOPs: N: {present_v(N):10}, Nb: {present_v(Nb):10}, D: {present_v(D):10} D/N: {present_v(D/N)}\")\n",
        "\n",
        "def print_chinchilla_optimal_configs_from_formula(A, alpha, B, beta, flop_budgets, **_):\n",
        "    for flop_budget in flop_budgets:\n",
        "        n_opt, d_opt, a, b = get_optimal_config_for_flops(A, alpha, B, beta, flop_budget)\n",
        "        print(f\"Optimal for {flop_budget} FLOPs: N: {present_v(n_opt)}, D: {present_v(d_opt)} D/N: {present_v(d_opt/n_opt)}\")\n",
        "    return a, b\n",
        "\n",
        "def print_compute_optimal_configs_for_flops(flop_budgets, with_embeddings, single_chinchilla_params, **params):\n",
        "  if with_embeddings:\n",
        "    print_chinchilla_optimal_configs_from_formula(**single_chinchilla_params, flop_budgets=flop_budgets)\n",
        "  else:\n",
        "    print_compute_optimal_configs_for_flops_binsearch(flop_budgets, with_embeddings, **params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnlTDlysIEMb"
      },
      "source": [
        "## Fit the curve with inits grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b8QrfIoeSFjS"
      },
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "import threading\n",
        "import time\n",
        "from typing import Callable, Any\n",
        "from tqdm.cli import tqdm\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Ensure spawn is the default context\n",
        "mp.set_start_method('fork', force=True)\n",
        "\n",
        "\n",
        "def calculate_params_for_init_dicts(grid_init_dicts, **other_params):\n",
        "  for init_grid in grid_init_dicts:\n",
        "      yield init_grid, compute_joined_scaling_law(**init_grid, **other_params)\n",
        "\n",
        "def compute_no_kwargs(args):\n",
        "  i, init_grid, other_params = args\n",
        "  w = None\n",
        "  while w is None:\n",
        "      try:\n",
        "        #print(f\"worker {i} started\")\n",
        "        w = init_grid, compute_joined_scaling_law(**init_grid, **other_params)\n",
        "        #print(f\"worker {i} finished\")\n",
        "      except Exception as ex:\n",
        "        print(f\" Exception {ex}\", flush=True)\n",
        "      except:\n",
        "        print(f\" Exception unknown\", flush=True)\n",
        "  return w\n",
        "\n",
        "def calculate_params_for_init_dicts_multithreaded(grid_init_dicts, cores, **other_params):\n",
        "  print(f\"\\nRunning concurrently {len(grid_init_dicts)} jobs with {cores=}\")\n",
        "\n",
        "  args = [(i, init_grid, other_params) for i, init_grid in enumerate(grid_init_dicts)]\n",
        "\n",
        "  with mp.Pool(cores) as pool:\n",
        "      for result in pool.imap_unordered(compute_no_kwargs, args):\n",
        "         if result is not None:\n",
        "            yield result\n",
        "\n",
        "\n",
        "def timeout_wrapper(args) -> Any:\n",
        "    result_container = [None]  # To hold the result of the function call\n",
        "\n",
        "    def target():\n",
        "        try:\n",
        "            result_container[0] = compute_no_kwargs(*args)\n",
        "        except Exception as e:\n",
        "            print(f\" Exception thread {ex}\")\n",
        "\n",
        "    thread = threading.Thread(target=target)\n",
        "    thread.start()\n",
        "    thread.join(60)\n",
        "\n",
        "    if thread.is_alive():\n",
        "        return None  # Indicate timeout occurred\n",
        "\n",
        "    return result_container[0]  # Return the function's result or exception\n",
        "\n",
        "def calculate_joined_scaling_law(N, D, L, E, N_test, D_test, L_test, E_test, inits, cores=None, optimize_c=True):\n",
        "  grid_init_dicts = [dict(zip(inits.keys(), values)) for values in product(*inits.values())]\n",
        "  total_iterations = len(grid_init_dicts)\n",
        "\n",
        "  if cores is None:\n",
        "      cores = min(mp.cpu_count(), total_iterations, 80)\n",
        "\n",
        "  calculate_function = calculate_params_for_init_dicts if cores == 1 else calculate_params_for_init_dicts_multithreaded\n",
        "  opt_values_for_inits = calculate_function(grid_init_dicts, cores=cores, N=N, D=D, L=L, E=E, optimize_c=optimize_c)\n",
        "\n",
        "  df = pd.DataFrame(columns=[])\n",
        "  pbar = tqdm(total=total_iterations, dynamic_ncols=True, smoothing=0)\n",
        "  for i, (init_dict, opt_values) in enumerate(opt_values_for_inits):\n",
        "      predicted_loss = predict_loss_joined(N=N, D=D, E=E, **opt_values)\n",
        "      #print(f\"{i}/{total_iterations} done\")\n",
        "      temp_df = pd.DataFrame([{\n",
        "          \"train_score\": rmse(torch.log(L), torch.log(predicted_loss)).item(),\n",
        "          \"test_score\": rmse(torch.log(L_test), torch.log(predict_loss_joined(N=N_test, D=D_test, E=E_test, **opt_values))).item() if E_test is not None else None,\n",
        "          **{k: v.item() for k, v in opt_values.items()},\n",
        "          **{f\"{k}_init\": v for k, v in init_dict.items()}\n",
        "      }])\n",
        "      df = pd.concat([df, temp_df], ignore_index=True)\n",
        "      pbar.update(1)\n",
        "  pbar.close()\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTKCiwFzIrMd"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02BhVVpcllfC"
      },
      "source": [
        "## Joint fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SjVU3f5irr_"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LOCI58m-C967"
      },
      "outputs": [],
      "source": [
        "data_filters = dict(\n",
        "    name=\"constrained_scaling_grid_21_11\",\n",
        "    thresh_active=(1e7, np.inf),\n",
        "    thresh_length=(-np.inf, np.inf),\n",
        "    thresh_token_to_param_ratio=(-np.inf, 100),\n",
        "    loss_thresh=(-np.inf,  np.inf),\n",
        "    exp_rate=[1, 2, 4, 8, 16, 32],\n",
        "    attn_heads_to_remove=[],\n",
        "    use_active=True,\n",
        "    with_embeddings=True,\n",
        "    filter_non_exp_d=False,\n",
        "    tokens_counted_per_active=False,\n",
        "    tokens_counted_per_total=False,\n",
        "    #loss_column=\"loss_interval/100\",\n",
        "    loss_column=\"final_eval_capacity_factor_1.5\",\n",
        "    #loss_column=\"final_eval\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "K3uQygrVe6Kg"
      },
      "outputs": [],
      "source": [
        "inits_experimental = dict(\n",
        "    lr = [0.0001], #, 0.00001],\n",
        "    huber_delta = [0.01],\n",
        "    weight_decay = [1e-5],\n",
        "    loss_weight = [-30], #, -20, -30, -50, -10],\n",
        "    pareto_distance_weigth = [0], #2, 1, 0.1], #, 3, 5],\n",
        "\n",
        "    alpha = [0.25, 0.5, 0.05], # ], #\n",
        "    beta = [0.25, 0.5, 0.05],\n",
        "    a = [100, 30, 300], #, 1], #], #\n",
        "    b = [100, 30, 300], #, 1], #],\n",
        "    c = [2, 1, 0.5], #, 0.2],\n",
        "    delta = [0.0, 0.5, -0.5], #\n",
        "    gamma = [0.0, 0.5, -0.5],\n",
        "    omega = [0.0, -0.5, 0.5],\n",
        "    zeta = [0.0, -0.5, 0.5],\n",
        "    e_start=[1], #, 1], #, 2, 5],\n",
        "    e_end = [300], #, 500], #, 10, 5],\n",
        "\n",
        "    Nb_a = [np.nan], #[5, 1, 30],  # np.nan to turn separate params for embeddings off\n",
        "    #Nb_alpha = #[0.25, 0.5, 0.05],\n",
        "    history_size=[20000],\n",
        "    max_iter=[10000],\n",
        "    line_search_fn=[\"strong_wolfe\"], # [\"strong_wolfe\"]\n",
        "    with_estart=[True],\n",
        "    with_eend=[True],\n",
        "    tolerance_change=[1e-10],\n",
        "    tolerance_grad=[1e-10],\n",
        "    #separate_chinchillas_for=[(1, 2, 4, 8, 16, 32),],\n",
        ")\n",
        "\n",
        "trim_to = 2  # we assume that first value from the list was some previous SOTA\n",
        "inits_best = {k: v[:trim_to] for k, v in inits_experimental.items()}\n",
        "\n",
        "inits = inits_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCg5bp07fA7I",
        "outputId": "7d8c16d0-6a12-4d02-e365-04005d2e1c60"
      },
      "outputs": [],
      "source": [
        "table = get_full_table(raw_table, **data_filters)\n",
        "table = table[~((table[\"str_active_non_embed\"]==\"340.8M\") & (table[\"args/expansion_rate\"] == 16))]\n",
        "table = table[~((table[\"str_params\"]==\"2.7B\") & (table[\"args/expansion_rate\"] == 2))]\n",
        "N, D, L, E, N_test, D_test, L_test, E_test = get_train_test_values(table=table, topn_losses_for_test=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EaDDDRBFLXg"
      },
      "source": [
        "### Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvnbIqMXfA7T",
        "outputId": "13a586be-6709-45a7-e46a-d2ceb43b45e9"
      },
      "outputs": [],
      "source": [
        "scores_df = lazy_run(\"scores_df.csv\",\n",
        "         lambda: calculate_joined_scaling_law(N=N, D=D, L=L, E=E, N_test=N_test, D_test=D_test, L_test=L_test, E_test=E_test, inits=inits, optimize_c=True),\n",
        "        #  always_run=True)  # set to false to reuse previous data\n",
        "         always_run=False)  # set to false to reuse previous data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GkCzM5ckfA7T",
        "outputId": "b01efa97-2829-4fa1-aaec-2b10dfd606fd"
      },
      "outputs": [],
      "source": [
        "scores_df[\"score\"] = scores_df[\"test_score\"] * 5/10 + scores_df[\"train_score\"] * 5/10\n",
        "# scores_df[\"score\"] = scores_df[\"train_score\"]\n",
        "scores_df = scores_df.sort_values(by=\"score\")\n",
        "scores_df = scores_df.reset_index(drop=True)\n",
        "best_result = scores_df.index[0]\n",
        "res = scores_df.loc[best_result]\n",
        "print(res)\n",
        "df = scores_df\n",
        "scores_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mOUSfzFgTVPS",
        "outputId": "50f9c241-6446-4cc7-e116-bbbfae40f350"
      },
      "outputs": [],
      "source": [
        "clean_scores = clean_dataframe(scores_df)\n",
        "ranks = calculate_ranks_for_all_columns(clean_scores)\n",
        "format_ranks_for_display(ranks)\n",
        "print(\"Best parameters from left to right:\")\n",
        "clean_scores[\"[score]\"] = scores_df[\"score\"]\n",
        "clean_scores[\"[train_score]\"] = scores_df[\"train_score\"]\n",
        "clean_scores[\"[test_score]\"] = scores_df[\"test_score\"]\n",
        "clean_scores.transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVy2CMnwbrXn"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "jCwBMd2ifA7U",
        "outputId": "725b2dc8-aff5-4414-eb8e-caaf0944f4da"
      },
      "outputs": [],
      "source": [
        "predicted_loss = predict_loss_joined(N=N, D=D, E=E, **res)\n",
        "test_predicted_loss = predict_loss_joined(N=N_test, D=D_test, E=E_test, **res)\n",
        "plot_pred_vs_actual(L, predicted_loss, L_test=L_test, L_test_pred=test_predicted_loss, E_train=E, E_test=E_test, in_log=False)\n",
        "print(f\"{res['train_score']=}\")\n",
        "print(f\"{res['test_score']=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K5Xfj_9orBG",
        "outputId": "8b6ffe07-0623-49d0-81b5-a9de7efcdb41"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def list_top_errors_train_test(\n",
        "    N_train, D_train, E_train, L_train, L_pred_train,\n",
        "    N_test, D_test, E_test, L_test, L_pred_test,\n",
        "    top_n=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Lists the top N biggest errors for both training and test datasets.\n",
        "\n",
        "    Parameters:\n",
        "    - N_train (array-like or torch.Tensor): Identifier for training samples.\n",
        "    - D_train (array-like or torch.Tensor): Feature D for training samples.\n",
        "    - E_train (array-like or torch.Tensor): Feature E for training samples.\n",
        "    - L_train (array-like or torch.Tensor): Actual loss values for training samples.\n",
        "    - L_pred_train (array-like or torch.Tensor): Predicted loss values for training samples.\n",
        "    - N_test (array-like or torch.Tensor): Identifier for test samples.\n",
        "    - D_test (array-like or torch.Tensor): Feature D for test samples.\n",
        "    - E_test (array-like or torch.Tensor): Feature E for test samples.\n",
        "    - L_test (array-like or torch.Tensor): Actual loss values for test samples.\n",
        "    - L_pred_test (array-like or torch.Tensor): Predicted loss values for test samples.\n",
        "    - top_n (int, default=10): Number of top errors to display.\n",
        "\n",
        "    Outputs:\n",
        "    - Prints two tables showing the top N errors for training and test sets respectively.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to convert data to numpy arrays\n",
        "    def to_numpy(data):\n",
        "        if isinstance(data, torch.Tensor):\n",
        "            return data.detach().cpu().numpy()\n",
        "        elif isinstance(data, list):\n",
        "            return np.array(data)\n",
        "        else:\n",
        "            return np.array(data)\n",
        "\n",
        "    # Convert all inputs to numpy arrays\n",
        "    N_train = to_numpy(N_train)\n",
        "    D_train = to_numpy(D_train)\n",
        "    E_train = to_numpy(E_train)\n",
        "    L_train = to_numpy(L_train)\n",
        "    L_pred_train = to_numpy(L_pred_train)\n",
        "\n",
        "    N_test = to_numpy(N_test)\n",
        "    D_test = to_numpy(D_test)\n",
        "    E_test = to_numpy(E_test)\n",
        "    L_test = to_numpy(L_test)\n",
        "    L_pred_test = to_numpy(L_pred_test)\n",
        "\n",
        "    # Compute absolute errors\n",
        "    errors_train = np.abs(L_train - L_pred_train)\n",
        "    errors_test = np.abs(L_test - L_pred_test)\n",
        "\n",
        "    # Create DataFrames for train and test\n",
        "    df_train = pd.DataFrame({\n",
        "        'N': N_train,\n",
        "        'D': D_train,\n",
        "        'E': E_train,\n",
        "        'D/N': D_train/N_train,\n",
        "        'Actual L': L_train,\n",
        "        'Predicted L': L_pred_train,\n",
        "        'Error': errors_train\n",
        "    })\n",
        "\n",
        "    df_test = pd.DataFrame({\n",
        "        'N': N_test,\n",
        "        'D': D_test,\n",
        "        'E': E_test,\n",
        "        'D/N': D_test/N_test,\n",
        "        'Actual L': L_test,\n",
        "        'Predicted L': L_pred_test,\n",
        "        'Error': errors_test\n",
        "    })\n",
        "\n",
        "    # Sort DataFrames by error descendingly and take top_n\n",
        "    top_errors_train = df_train.sort_values(by='Error', ascending=False).head(top_n)\n",
        "    top_errors_test = df_test.sort_values(by='Error', ascending=False).head(top_n)\n",
        "\n",
        "    # Display the results\n",
        "    print(\"\\n=== Top {} Biggest Errors in Training Set ===\".format(top_n))\n",
        "    print(top_errors_train.to_string(index=False))\n",
        "\n",
        "    print(\"\\n=== Top {} Biggest Errors in Test Set ===\".format(top_n))\n",
        "    print(top_errors_test.to_string(index=False))\n",
        "list_top_errors_train_test(\n",
        "    N_train=N,\n",
        "    D_train=D,\n",
        "    E_train=E,\n",
        "    L_train=L,\n",
        "    L_pred_train=predicted_loss,\n",
        "    N_test=N_test,\n",
        "    D_test=D_test,\n",
        "    E_test=E_test,\n",
        "    L_test=L_test,\n",
        "    L_pred_test=test_predicted_loss,\n",
        "    top_n=10  # Specify the number of top errors to display\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft5Rj57PdLrp"
      },
      "source": [
        "#### Bias in terms of E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgZjVvzCbSxi"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_p5P6hnNbSxi"
      },
      "outputs": [],
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "def analyse_e_bias():\n",
        "    df = pd.DataFrame({'N': N, 'D': D, 'E': E, 'L': L, 'L_pred': predicted_loss})\n",
        "    # remove smallest N < 60M\n",
        "    #df = df[df['N'] > 6e7]\n",
        "\n",
        "    df.loc[:, 'residual'] = df['L'] - df['L_pred']\n",
        "    df.loc[:, 'residual_log'] = np.log(df['L']) - np.log(df['L_pred'])\n",
        "    gdf = df.groupby('E')\n",
        "    gdf = gdf.mean('residual')\n",
        "    print(gdf)\n",
        "    # Plotting a bar plot for residual_log values for each E\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    gdf['residual_log'].plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "\n",
        "    # Adding labels and title\n",
        "    plt.title(\"Residual Log Bias for Each E\")\n",
        "    plt.xlabel(\"E\")\n",
        "    plt.ylabel(\"Residual Log\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Show plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Function to format large numbers (N, D) into human-readable format\n",
        "def human_format(x, pos):\n",
        "    if x >= 1e9:\n",
        "        return f'{x*1e-9:.1f}B'\n",
        "    elif x >= 1e6:\n",
        "        return f'{x*1e-6:.1f}M'\n",
        "    elif x >= 1e3:\n",
        "        return f'{x*1e-3:.1f}K'\n",
        "    else:\n",
        "        return str(x)\n",
        "\n",
        "def analyse_bias_heatmaps():\n",
        "    # Create a DataFrame with N, D, E, L, L_pred\n",
        "    df = pd.DataFrame({'N': N, 'D': D, 'E': E, 'L': L, 'L_pred': predicted_loss})\n",
        "    df.loc[:, 'residual'] = df['L'] - df['L_pred']\n",
        "    df.loc[:, 'residual_log'] = np.log(df['L']) - np.log(df['L_pred'])\n",
        "\n",
        "    # Group by N and E, by D and E, and by N and D, calculating the mean for residual_log\n",
        "    gdf_n_e = df.groupby(['N', 'E']).mean('residual_log').reset_index()\n",
        "    gdf_d_e = df.groupby(['D', 'E']).mean('residual_log').reset_index()\n",
        "    gdf_n_d = df.groupby(['N', 'D']).mean('residual_log').reset_index()\n",
        "\n",
        "    # Pivot the tables for heatmaps\n",
        "    pivot_table_n_e = gdf_n_e.pivot(index='N', columns='E', values='residual_log')\n",
        "    pivot_table_d_e = gdf_d_e.pivot(index='D', columns='E', values='residual_log')\n",
        "    pivot_table_n_d = gdf_n_d.pivot(index='N', columns='D', values='residual_log')\n",
        "\n",
        "    # Set up the figure and axes for side-by-side plots\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\n",
        "\n",
        "\n",
        "    # Determine maximum absolute value in the DataFrame to scale symmetrically\n",
        "    pivot_tables = [\n",
        "        pivot_table_n_e.to_numpy(),\n",
        "        pivot_table_d_e.to_numpy(),\n",
        "        pivot_table_n_d.to_numpy()\n",
        "    ]\n",
        "\n",
        "    max_abs_value = max(np.nanmax(np.abs(arr[np.isfinite(arr)])) for arr in pivot_tables)\n",
        "\n",
        "    # Plot the N vs E heatmap\n",
        "    sns.heatmap(pivot_table_n_e, annot=True, fmt=\".4f\", cmap='bwr', ax=ax1, cbar_kws={'label': 'Residual Log Bias'}, center=0, vmin=-max_abs_value, vmax=max_abs_value)\n",
        "    ax1.set_title(\"Residual Log Bias Heatmap: N (Parameters) vs E (Expansion Rate)\")\n",
        "    ax1.set_xlabel(\"E (Expansion Rate)\")\n",
        "    ax1.set_ylabel(\"N (Parameters)\")\n",
        "\n",
        "    # Format N axis with human-readable format\n",
        "    n_values = pivot_table_n_e.index\n",
        "    ax1.set_yticks(np.arange(len(n_values)) + 0.5)\n",
        "    ax1.set_yticklabels([human_format(n, None) for n in n_values])\n",
        "\n",
        "    # Plot the D vs E heatmap\n",
        "    sns.heatmap(pivot_table_d_e, annot=True, fmt=\".4f\", cmap='bwr', ax=ax2, cbar_kws={'label': 'Residual Log Bias'}, center=0, vmin=-max_abs_value, vmax=max_abs_value)\n",
        "    ax2.set_title(\"Residual Log Bias Heatmap: D (Training Tokens) vs E (Expansion Rate)\")\n",
        "    ax2.set_xlabel(\"E (Expansion Rate)\")\n",
        "    ax2.set_ylabel(\"D (Training Tokens)\")\n",
        "\n",
        "    # Format D axis with human-readable format\n",
        "    d_values = pivot_table_d_e.index\n",
        "    ax2.set_yticks(np.arange(len(d_values)) + 0.5)\n",
        "    ax2.set_yticklabels([human_format(d, None) for d in d_values])\n",
        "\n",
        "    # Plot the N vs D heatmap\n",
        "    sns.heatmap(pivot_table_n_d, annot=True, fmt=\".4f\", cmap='bwr', ax=ax3, cbar_kws={'label': 'Residual Log Bias'}, center=0, vmin=-max_abs_value, vmax=max_abs_value)\n",
        "    ax3.set_title(\"Residual Log Bias Heatmap: N (Parameters) vs D (Training Tokens)\")\n",
        "    ax3.set_ylabel(\"N (Parameters)\")\n",
        "    ax3.set_xlabel(\"D (Training Tokens)\")\n",
        "\n",
        "    # Format N axis with human-readable format for the third plot\n",
        "    n_values_nd = pivot_table_n_d.index\n",
        "    ax3.set_yticks(np.arange(len(n_values_nd)) + 0.5)\n",
        "    ax3.set_yticklabels([human_format(n, None) for n in n_values_nd])\n",
        "\n",
        "    # Format D axis with human-readable format for the third plot\n",
        "    d_values_nd = pivot_table_n_d.columns\n",
        "    ax3.set_xticks(np.arange(len(d_values_nd)) + 0.5)\n",
        "    ax3.set_xticklabels([human_format(d, None) for d in d_values_nd], rotation=45)\n",
        "\n",
        "    # Adjust layout and show plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keiaTp31bSxj"
      },
      "source": [
        "##### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e9S-eeDCbSxj",
        "outputId": "29618c42-4d40-4ef6-b59b-16d2de0240bd"
      },
      "outputs": [],
      "source": [
        "analyse_e_bias()\n",
        "analyse_bias_heatmaps()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl51YRvwdTkp"
      },
      "source": [
        "#### Coefficients in terms of E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "mafG_-iyC1Iv",
        "outputId": "04dd3280-facd-42d2-be0c-c677d820127f"
      },
      "outputs": [],
      "source": [
        "def generate_separate_chinchillas_grid(E_range=(1,2,4,8,16,32), **params):\n",
        "  return {e: project_joined_to_chinchilla(E=torch.tensor(e), **params) for e in E_range}\n",
        "\n",
        "chinchillas_from_joined = generate_separate_chinchillas_grid(**res)\n",
        "df_w = pd.DataFrame.from_dict(chinchillas_from_joined, orient='index', columns=chinchillas_from_joined[list(chinchillas_from_joined.keys())[0]].keys())\n",
        "\n",
        "similarity = predict_loss(**chinchillas_from_joined[4], N=1e8, D=1e10), predict_loss_joined(E=4, **res, N=1e8, D=1e10)\n",
        "assert np.allclose(similarity[0], similarity[1])\n",
        "\n",
        "df_w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6J7zYDEdVvd"
      },
      "source": [
        "#### Optimal D/N ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "solDGtJ60jBu",
        "outputId": "e383d26e-1136-4c31-9ccf-cb13ec36425e"
      },
      "outputs": [],
      "source": [
        "N_values = np.array([100e6, 1e9, 3e9, 7e9, 70e9, 400e9])\n",
        "\n",
        "for e, row in chinchillas_from_joined.items():\n",
        "  print(f\"For {e=}\")\n",
        "  print_compute_optimal_configs_for_N(N_values, E=e, **res, single_chinchilla_params=chinchillas_from_joined[e], with_embeddings=data_filters[\"with_embeddings\"])\n",
        "\n",
        "\n",
        "#flops_budgets = np.array([1.192473e+18, 1.809944e+18, 2.758660e+18, 4.222465e+18, 6.490638e+18, 1.002030e+19])\n",
        "flops_budgets = np.array([1e18, 1e19, 1e20, 1e21, 1e22, 1e23])\n",
        "\n",
        "for e, row in chinchillas_from_joined.items():\n",
        "  print(f\"For {e=}\")\n",
        "  print_compute_optimal_configs_for_flops(flops_budgets, E=e, single_chinchilla_params=chinchillas_from_joined[e], **res, with_embeddings=data_filters[\"with_embeddings\"])\n",
        "\n",
        "\n",
        "def compute_opt_D_for_N_with_embeddings(N_with_emb, e):\n",
        "  return get_optimal_D_for_N_with_embeddings(N_with_emb, single_chinchilla_params=chinchillas_from_joined[e], **res, with_embeddings=True)\n",
        "\n",
        "print(f\"{compute_opt_D_for_N_with_embeddings(1e9, 4)=}\")\n",
        "print(f\"{compute_opt_D_for_N_with_embeddings(1e9, 1)=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvernIn-_LqG"
      },
      "source": [
        "## Plots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZMWftEidden"
      },
      "source": [
        "#### Validation plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zSPpnQpEsY3N",
        "outputId": "f957f21d-99f2-4e3f-97fc-51c4b8e52bde"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import torch\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "\n",
        "# Get unique D values\n",
        "unique_D_values = torch.unique(D).flip(dims=(0,))\n",
        "num_plots = len(unique_D_values)\n",
        "\n",
        "# Golden ratio aspect (1:1.618)\n",
        "golden_ratio = 1.2\n",
        "\n",
        "# Set number of columns to 2\n",
        "num_cols = 4\n",
        "num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate rows to accommodate all subplots\n",
        "\n",
        "# Create subplots with 2 columns\n",
        "fig = make_subplots(\n",
        "    rows=num_rows,\n",
        "    cols=num_cols,\n",
        "    shared_xaxes=True,\n",
        "    subplot_titles=[f'D = {present_v(d.item())}' for d in unique_D_values],\n",
        "    vertical_spacing=0.05,  # Reduce vertical spacing\n",
        "    horizontal_spacing=0.05  # Reduce horizontal spacing\n",
        ")\n",
        "\n",
        "# Loop through each unique D value to create individual subplots\n",
        "for i, d_value in enumerate(unique_D_values):\n",
        "    # Select points where D is equal to the current d_value\n",
        "    mask = (D == d_value)\n",
        "    E_filtered = E[mask]\n",
        "    L_filtered = L[mask]\n",
        "    N_filtered = N[mask]\n",
        "\n",
        "    # Create scatter plot for the filtered values (actual L values)\n",
        "    scatter_actual = go.Scatter(x=E_filtered.numpy(), y=L_filtered.numpy(), mode='markers', name=f'Actual L (D = {present_v(d_value.item())})',marker=dict(size=7))\n",
        "    # version with bigger dots:\n",
        "\n",
        "\n",
        "    # Determine row and col positions dynamically\n",
        "    row = (i // num_cols) + 1\n",
        "    col = (i % num_cols) + 1\n",
        "\n",
        "    # Add actual data trace to the subplot\n",
        "    fig.add_trace(scatter_actual, row=row, col=col)\n",
        "\n",
        "    # Get unique N values for the current D\n",
        "    unique_N_values = torch.unique(N_filtered)\n",
        "\n",
        "    # Generate predicted loss lines for each unique N\n",
        "    for n_value in unique_N_values:\n",
        "        # Generate a range of E values for prediction\n",
        "        E_range = np.logspace(np.log10(E_filtered.min()), np.log10(E_filtered.max()), num=100)\n",
        "\n",
        "        # Calculate predicted L values\n",
        "        L_predicted = [predict_loss_joined(E=e, **res, N=n_value.item(), D=d_value.item()) for e in E_range]\n",
        "\n",
        "        # Create scatter plot for predicted values (without adding to legend)\n",
        "        scatter_predicted = go.Scatter(x=E_range, y=L_predicted, mode='lines', line=dict(dash='dot'),\n",
        "                                       showlegend=False)\n",
        "\n",
        "        # Add predicted data trace to the subplot\n",
        "        fig.add_trace(scatter_predicted, row=row, col=col)\n",
        "\n",
        "# Update layout for golden ratio\n",
        "fig.update_layout(\n",
        "    height=400 * num_rows,  # Adjust height for each row\n",
        "    width=400 * num_cols * golden_ratio,  # Adjust width based on golden ratio\n",
        "    title_text=\"Loss L vs E with Predicted Loss for different D values\",\n",
        "    showlegend=True\n",
        ")\n",
        "# hardcoded\n",
        "#min = np.log10(2.7)\n",
        "#max = np.log10(6.3)\n",
        "# auto\n",
        "min_v = np.log10(L.min() - 0.1)\n",
        "max_v = np.log10(L.max() + 0.1)\n",
        "\n",
        "# Update axes for log scale and set y-limits\n",
        "fig.update_xaxes(title_text=\"E\", type=\"log\")\n",
        "fig.update_yaxes(title_text=\"L\", type=\"log\", range=[min_v, max_v])  # Set y-limits here\n",
        "\n",
        "\n",
        "# set proper ticks for E : 1,2,4,8,16,32\n",
        "fig.update_xaxes(tickvals=[1, 2, 4, 8, 16, 32], ticktext=['1', '2', '4', '8', '16', '32'])\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "il5MYmNtIiFm",
        "outputId": "d50854f0-1126-4c59-9578-a6f69e237fa0"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import torch\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "from plotly.colors import qualitative\n",
        "\n",
        "def present_ratio(d, n):\n",
        "    r = d/n if not data_filters[\"tokens_counted_per_active\"] else d\n",
        "    return f\"{r:.3g}\"  # 3 significant digits\n",
        "\n",
        "def is_pareto_point(x, y, points):\n",
        "    \"\"\"\n",
        "    Determines if a point (x, y) is on the Pareto curve.\n",
        "    A point is not Pareto-optimal if there exist two other points (x1, y1) and (x2, y2)\n",
        "    such that the line between (x1, y1) and (x2, y2) lies below (x, y).\n",
        "    \"\"\"\n",
        "    for i, (x1, y1) in enumerate(points):\n",
        "        for j, (x2, y2) in enumerate(points):\n",
        "            if i >= j:  # Avoid duplicate or reverse comparisons\n",
        "                continue\n",
        "            # Skip if the test line cannot bracket the current x value\n",
        "            if not (min(x1, x2) < x < max(x1, x2)):\n",
        "                continue\n",
        "            # Linear interpolation to check if the line lies below the point\n",
        "            y_line = y1 + (y2 - y1) * (x - x1) / (x2 - x1)\n",
        "            if y_line < y:\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "# Define unique E values\n",
        "unique_E_values = torch.unique(E)\n",
        "num_plots = len(unique_E_values)\n",
        "\n",
        "# Set the layout of subplots (e.g., 2 columns)\n",
        "num_cols = 2\n",
        "num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate rows to fit all subplots\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(\n",
        "    rows=num_rows,\n",
        "    cols=num_cols,\n",
        "    subplot_titles=[f'E = {e.item()}' for e in unique_E_values],\n",
        "    shared_xaxes=True,\n",
        "    vertical_spacing=0.05,\n",
        "    horizontal_spacing=0.05\n",
        ")\n",
        "\n",
        "# Define a distinct color palette for N values\n",
        "color_palette = qualitative.Set2  # Use Plotly's Set2 palette for clear distinctions\n",
        "\n",
        "# Loop over each unique E value\n",
        "for i, e_value in enumerate(unique_E_values):\n",
        "    # Filter data for the current E value\n",
        "    mask_E = (E == e_value)\n",
        "    N_filtered = N[mask_E]\n",
        "    L_filtered = L[mask_E]\n",
        "    D_filtered = D[mask_E]\n",
        "\n",
        "    # Determine subplot row and column\n",
        "    row = (i // num_cols) + 1\n",
        "    col = (i % num_cols) + 1\n",
        "\n",
        "    # Combine points for Pareto check\n",
        "    points = 6 * N_filtered * D_filtered if not data_filters[\"tokens_counted_per_active\"] else 6 * N_filtered * D_filtered * N_filtered\n",
        "    points = list(zip((points).numpy(), L_filtered.numpy()))\n",
        "\n",
        "    # Loop over unique N values for points and predicted curves\n",
        "    unique_N_values = torch.unique(N_filtered)\n",
        "    for j, n_value in enumerate(unique_N_values):\n",
        "        # Use a color from the palette, cycling if needed\n",
        "        color = color_palette[j % len(color_palette)]\n",
        "\n",
        "        # Mask data for the current N value\n",
        "        mask_N = (N_filtered == n_value)\n",
        "        D_current = D_filtered[mask_N]\n",
        "        L_current = L_filtered[mask_N]\n",
        "\n",
        "        if data_filters[\"tokens_counted_per_active\"]:\n",
        "          FLOPS_current = (6 * n_value.item() * D_current * n_value.item()).numpy()\n",
        "        else:\n",
        "          FLOPS_current = (6 * n_value.item() * D_current).numpy()\n",
        "\n",
        "\n",
        "        # Identify Pareto points\n",
        "        labels = [\n",
        "            present_ratio(D, n_value.item()) if is_pareto_point(f, l, points) else \"\"\n",
        "            for f, l, D in zip(FLOPS_current, L_current.numpy(), D_current)\n",
        "        ]\n",
        "\n",
        "        # Add actual data points\n",
        "        scatter_actual = go.Scatter(\n",
        "            x=FLOPS_current,\n",
        "            y=L_current.numpy(),\n",
        "            text=labels,\n",
        "            textfont=dict(size=10, color=f'rgba({color[4:-1]}, 0.7)'),  # Match color with alpha\n",
        "            mode='markers+text',\n",
        "            textposition=\"bottom center\",\n",
        "            name=f'Actual {present_v(n_value.item())} (E={e_value.item()})',\n",
        "            marker=dict(size=8, color=color),\n",
        "            legendgroup=f'{n_value.item()}',\n",
        "            showlegend=(row == 1 and col == 1)  # Only show legend once\n",
        "        )\n",
        "        fig.add_trace(scatter_actual, row=row, col=col)\n",
        "\n",
        "        # Generate predicted loss for interpolated D\n",
        "        D_range = np.logspace(np.log10(D.min()), np.log10(D.max()), num=100)\n",
        "        L_predicted = [\n",
        "            predict_loss_joined(E=e_value.item(), **res, N=n_value.item(), D=d)\n",
        "            for d in D_range\n",
        "        ]\n",
        "\n",
        "        if data_filters[\"tokens_counted_per_active\"]:\n",
        "          #FLOPS_current = (6 * n_value.item() * D_current * n_value.item()).numpy()\n",
        "          FLOPS_predicted = (6 * n_value.item() * D_range* n_value.item())\n",
        "        else:\n",
        "          FLOPS_predicted = (6 * n_value.item() * D_range)\n",
        "\n",
        "        # Add predicted loss curve\n",
        "        scatter_predicted = go.Scatter(\n",
        "            x=FLOPS_predicted,\n",
        "            y=L_predicted,\n",
        "            mode='lines',\n",
        "            line=dict(color=color, dash='dot'),\n",
        "            name=f'Predicted {present_v(n_value.item())} (E={e_value.item()})',\n",
        "            legendgroup=f'{n_value.item()}',\n",
        "            showlegend=False\n",
        "        )\n",
        "        fig.add_trace(scatter_predicted, row=row, col=col)\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Loss (L) vs FLOPS for Different E Values (Pareto-Filtered Labels)\",\n",
        "    xaxis_title=\"FLOPS (log scale)\",\n",
        "    yaxis_title=\"Loss (L)\",\n",
        "    showlegend=True,\n",
        "    height=400 * num_rows,\n",
        "    width=800 * num_cols\n",
        ")\n",
        "\n",
        "# Update axes for log scale\n",
        "fig.update_xaxes(title_text=\"FLOPS\", type=\"log\")\n",
        "fig.update_yaxes(title_text=\"Loss (L)\", type=\"log\")\n",
        "\n",
        "# Show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SyfA49OMmfrY",
        "outputId": "453ce717-2fec-4562-adc9-fb61c416d30c"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import torch\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "\n",
        "# Get unique N values\n",
        "unique_N_values = torch.unique(N).flip(dims=(0,))\n",
        "num_plots = len(unique_N_values)\n",
        "\n",
        "# Golden ratio aspect (1:1.618)\n",
        "golden_ratio = 1.2\n",
        "\n",
        "# Set number of columns to 4\n",
        "num_cols = 4\n",
        "num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate rows to accommodate all subplots\n",
        "\n",
        "# Create subplots with increased vertical spacing\n",
        "fig = make_subplots(\n",
        "    rows=num_rows,\n",
        "    cols=num_cols,\n",
        "    shared_xaxes=True,\n",
        "    subplot_titles=[f'N = {present_v(n.item())}' for n in unique_N_values],\n",
        "    vertical_spacing=0.15,  # Increase vertical spacing\n",
        "    horizontal_spacing=0.05  # Keep horizontal spacing as before\n",
        ")\n",
        "\n",
        "D_names = set()\n",
        "\n",
        "# Loop through each unique N value to create individual subplots\n",
        "for i, n_value in enumerate(unique_N_values):\n",
        "    # Select points where N is equal to the current n_value\n",
        "    mask = (N == n_value)\n",
        "    E_filtered = E[mask]\n",
        "    L_filtered = L[mask]\n",
        "    D_filtered = D[mask]\n",
        "\n",
        "    # Create scatter plot for the filtered values (actual L values)\n",
        "    scatter_actual = go.Scatter(\n",
        "        x=E_filtered.numpy(),\n",
        "        y=L_filtered.numpy(),\n",
        "        mode='markers',\n",
        "        name=f'Actual L (N = {present_v(n_value.item())})',\n",
        "        marker=dict(size=7)\n",
        "    )\n",
        "\n",
        "    # Determine row and col positions dynamically\n",
        "    row = (i // num_cols) + 1\n",
        "    col = (i % num_cols) + 1\n",
        "\n",
        "    # Add actual data trace to the subplot\n",
        "    fig.add_trace(scatter_actual, row=row, col=col)\n",
        "\n",
        "    # Get unique D values for the current N\n",
        "    unique_D_values = torch.unique(D_filtered)\n",
        "\n",
        "    # Generate predicted loss lines for each unique D\n",
        "    for d_value in unique_D_values:\n",
        "        # Generate a range of E values for prediction\n",
        "        E_range = np.logspace(np.log10(E_filtered.min()), np.log10(E_filtered.max()), num=100)\n",
        "\n",
        "        # Calculate predicted L values\n",
        "        L_predicted = [predict_loss_joined(E=e, **res, N=n_value.item(), D=d_value.item()) for e in E_range]\n",
        "\n",
        "        name = f'D = {present_v(d_value.item())}'\n",
        "\n",
        "        # Create scatter plot for predicted values (without adding to legend)\n",
        "        scatter_predicted = go.Scatter(\n",
        "            x=E_range,\n",
        "            y=L_predicted,\n",
        "            mode='lines',\n",
        "            name=name,\n",
        "            line=dict(dash='dot'),\n",
        "            showlegend=name not in D_names\n",
        "        )\n",
        "\n",
        "        D_names.add(name)\n",
        "\n",
        "        # Add predicted data trace to the subplot\n",
        "        fig.add_trace(scatter_predicted, row=row, col=col)\n",
        "\n",
        "# Update layout for golden ratio\n",
        "fig.update_layout(\n",
        "    height=450 * num_rows,  # Adjust height for each row\n",
        "    width=400 * num_cols * golden_ratio,  # Adjust width based on golden ratio\n",
        "    title_text=\"Loss L vs E with Predicted Loss for different N values\",\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Auto calculate min and max values for the y-axis\n",
        "min_v = np.log10(L.min() - 0.1)\n",
        "max_v = np.log10(L.max() + 0.1)\n",
        "\n",
        "# Update axes for log scale and set y-limits\n",
        "fig.update_xaxes(title_text=\"E\", type=\"log\")\n",
        "fig.update_yaxes(title_text=\"L\", type=\"log\", range=[min_v, max_v])  # Set y-limits here\n",
        "\n",
        "# Set proper ticks for E : 1,2,4,8,16,32\n",
        "fig.update_xaxes(tickvals=[1, 2, 4, 8, 16, 32], ticktext=['1', '2', '4', '8', '16', '32'])\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeMCARLYppsC"
      },
      "source": [
        "#### Memory constrained plots jkrajewski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMUtWSLNpv4l"
      },
      "source": [
        "###### Code for plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jYIWM-GYpsqT"
      },
      "outputs": [],
      "source": [
        "def plot_for_memory_constraint(\n",
        "    predict_loss: callable,\n",
        "    memory_constraint: int,\n",
        "    get_total_from_active: callable,\n",
        "    get_active_from_total: callable,\n",
        "    min_train_flop_budget=1e18,\n",
        "    max_train_flop_budget=5e21,\n",
        "    N_active_from_exps=None,\n",
        "    D_from_exps=None,\n",
        "    E_from_exps=None,\n",
        "    L_from_exps=None,\n",
        "    inference_tokens=None\n",
        "):\n",
        "    import numpy as np\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.colors import qualitative\n",
        "\n",
        "    if N_active_from_exps is not None:\n",
        "      N_total_from_exps = get_total_from_active(N_active_from_exps, E_from_exps)\n",
        "\n",
        "      points_to_plot_mask = (np.abs((N_total_from_exps - memory_constraint)) / memory_constraint) < 0.1\n",
        "      # points_to_plot_mask = (np.abs(np.log(N_total_from_exps) - np.log(memory_constraint)) / np.log(memory_constraint)) < 0.005\n",
        "      N_active_from_exps = N_active_from_exps[points_to_plot_mask]\n",
        "      D_from_exps = D_from_exps[points_to_plot_mask]\n",
        "      E_from_exps = E_from_exps[points_to_plot_mask]\n",
        "      L_from_exps = L_from_exps[points_to_plot_mask]\n",
        "\n",
        "    palette = qualitative.Dark24_r\n",
        "\n",
        "    flops_range = np.logspace(\n",
        "        np.log10(min_train_flop_budget),\n",
        "        np.log10(max_train_flop_budget),\n",
        "        30\n",
        "    )\n",
        "\n",
        "    expansion_rates = [1, 2, 4, 8, 16, 32]\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for i, E in enumerate(expansion_rates):\n",
        "        color = color_palette[i % len(color_palette)] if E != 32 else 'darkorange'\n",
        "\n",
        "        plot_mask = E_from_exps == E\n",
        "        budgets = 6 * N_active_from_exps[plot_mask] * D_from_exps[plot_mask]\n",
        "        if inference_tokens is not None:\n",
        "          budgets += 6 * N_active_from_exps[plot_mask] * inference_tokens\n",
        "        losses = L_from_exps[plot_mask]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=budgets,\n",
        "                y=losses,\n",
        "                mode='markers',\n",
        "                name=f'E={E}',\n",
        "                marker=dict(color=color)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        budgets=np.logspace(np.log10(min_train_flop_budget), np.log10(max_train_flop_budget), num=1000)\n",
        "        active_params=np.repeat(get_active_from_total(memory_constraint, E), len(budgets))\n",
        "        D = budgets / (6 * active_params)\n",
        "        losses = predict_loss(active_params, D, np.repeat(E, len(active_params)))\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=budgets,\n",
        "                y=losses,\n",
        "                mode='lines',\n",
        "                name=f'E={E}',\n",
        "                line=dict(color=color)\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "    compute_opt_for_dense=6*5*memory_constraint**2\n",
        "    fig.add_vline(\n",
        "        x=compute_opt_for_dense, # compute optimal for dense\n",
        "        line_width=2,\n",
        "        line_dash=\"dash\",\n",
        "        line_color=color_palette[0],\n",
        "    )\n",
        "\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis=dict(type='log', title='Training FLOPs'),\n",
        "        yaxis=dict(type='log', title='Predicted Loss'),\n",
        "        height=900,\n",
        "        width=1200,\n",
        "        title=f\"Scaling Curves for Models with {format_large_numbers(memory_constraint)} Total Params\"\n",
        "    )\n",
        "    fig.add_annotation(\n",
        "        x=0.01,\n",
        "        y=0.01,                # you can adjust this\n",
        "        xref=\"paper\",\n",
        "        yref=\"paper\",\n",
        "        text=\"Dashed:\\nCompute optimal for dense\",\n",
        "        showarrow=False,\n",
        "        font=dict(\n",
        "        size=20,          # Increased font size from 12 to 16\n",
        "        color=color_palette[0],\n",
        "        family=\"Arial\"    # Optional: Specify a font family\n",
        "    ),\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwxAsNSOpz-F"
      },
      "source": [
        "##### Actual plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "ze4TpTFip2M4",
        "outputId": "a5a1d74b-b61b-42fd-c7cd-c279b17073aa"
      },
      "outputs": [],
      "source": [
        "plot_for_memory_constraint(\n",
        "    predict_loss=lambda N, D, E: predict_loss_joined(N=N, D=D, E=E, **res),\n",
        "    # memory_constraint=0.6e09,\n",
        "    memory_constraint=1e09,\n",
        "    get_total_from_active=lambda N, E : N * (9*E+4)/13,\n",
        "    get_active_from_total=lambda N, E : N * 13/(9*E+4),\n",
        "    # max_train_flop_budget=5e25,\n",
        "    min_train_flop_budget=5e16,\n",
        "    N_active_from_exps=N,\n",
        "    D_from_exps=D,\n",
        "    E_from_exps=E,\n",
        "    L_from_exps=L,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA7PtZbo7tu2"
      },
      "source": [
        "#### Estimating optimal token to param ratio using Chinchilla Approach 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vELLa8287zKE"
      },
      "outputs": [],
      "source": [
        "def get_total_params(\n",
        "    n_layers: int,\n",
        "    dmodel: int,\n",
        "    expert_hidden_size: int,\n",
        "    n_experts: int,\n",
        "    vocab_size: int = 50257,\n",
        "    with_embeddings: bool = False\n",
        "):\n",
        "    # -- 1) Att per layer (simplified: 4*dmodel^2).\n",
        "    attention_params_per_layer = 4 * (dmodel ** 2)\n",
        "\n",
        "    # -- 2) MoE feed-forward for all experts in one layer:\n",
        "    ff_moe_params_per_layer = n_experts * (3 * dmodel * expert_hidden_size)\n",
        "\n",
        "    # -- 3) Gating network per layer: (dmodel -> n_experts)\n",
        "    gating_params_per_layer = (dmodel * n_experts)\n",
        "\n",
        "    # Sum for one layer\n",
        "    layer_params = attention_params_per_layer + ff_moe_params_per_layer + gating_params_per_layer\n",
        "\n",
        "    # Multiply by number of layers\n",
        "    transformer_params = n_layers * layer_params\n",
        "\n",
        "    # -- 4) Embeddings (input + output)\n",
        "    embedding_params = 2 * vocab_size * dmodel\n",
        "\n",
        "    # Total\n",
        "    total_params = transformer_params\n",
        "\n",
        "    if with_embeddings:\n",
        "      total_params += embedding_params\n",
        "\n",
        "    return total_params\n",
        "\n",
        "\n",
        "def get_active_params(\n",
        "    n_layers: int,\n",
        "    dmodel: int,\n",
        "    expert_hidden_size: int,\n",
        "    n_experts: int,\n",
        "    vocab_size: int = 50257,\n",
        "    with_embeddings: bool = False\n",
        "):\n",
        "    # -- 1) MHA per layer (same as total, because all attention heads/layers are used)\n",
        "    attention_params_per_layer = 4 * (dmodel ** 2)\n",
        "\n",
        "    # -- 2) Gating network per layer: we must compute gating for all experts\n",
        "    gating_params_per_layer = (dmodel * n_experts)\n",
        "\n",
        "    # -- 3) Only 1 expert is active per layer (top-1 gating):\n",
        "    ff_moe_params_per_layer = 3 * dmodel * expert_hidden_size\n",
        "\n",
        "    layer_params = attention_params_per_layer + gating_params_per_layer + ff_moe_params_per_layer\n",
        "    transformer_params = n_layers * layer_params\n",
        "\n",
        "    # -- 4) Embeddings (input + output).\n",
        "    embedding_params = 2 * vocab_size * dmodel\n",
        "\n",
        "    active_params = transformer_params\n",
        "\n",
        "    if with_embeddings:\n",
        "      active_params += embedding_params\n",
        "\n",
        "    return active_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7KOzoAf7zyD",
        "outputId": "ba91ca05-6d8a-46f6-bc4d-b3603d4fac39"
      },
      "outputs": [],
      "source": [
        "table = get_full_table(raw_table, \"constrained_scaling_grid_21_11\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB5MqEasGqBF",
        "outputId": "9ace7a80-f69e-4f02-84c7-774f1f7bae61"
      },
      "outputs": [],
      "source": [
        "p_table = pd.DataFrame()\n",
        "p_table[\"n_layers\"] = table[\"args/n_blocks\"]\n",
        "p_table[\"dmodel\"] = table[\"args/dmodel\"]\n",
        "p_table[\"n_experts\"] = table[\"args/n_experts\"]\n",
        "p_table[\"expert_hidden_size\"] = 3 * table[\"args/dmodel\"]\n",
        "p_table[\"train_tokens\"] = table[\"tokens\"]\n",
        "p_table[\"loss\"] = table[\"loss_interval/100\"]\n",
        "p_table.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PtprzA-TGqtS"
      },
      "outputs": [],
      "source": [
        "p_table[\"total_params\"] = p_table.apply(\n",
        "    lambda row: get_total_params(\n",
        "        n_layers=row[\"n_layers\"],\n",
        "        dmodel=row[\"dmodel\"],\n",
        "        expert_hidden_size=row[\"expert_hidden_size\"],\n",
        "        n_experts=row[\"n_experts\"],\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "p_table[\"active_params\"] = p_table.apply(\n",
        "    lambda row: get_active_params(\n",
        "        n_layers=row[\"n_layers\"],\n",
        "        dmodel=row[\"dmodel\"],\n",
        "        expert_hidden_size=row[\"expert_hidden_size\"],\n",
        "        n_experts=row[\"n_experts\"]\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "p_table[\"flops\"] = 6 * p_table[\"active_params\"] * p_table[\"train_tokens\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "buZibzxQT_h9"
      },
      "outputs": [],
      "source": [
        "def calc_num_exps(budget):\n",
        "  e_rate = 1\n",
        "  flop_budget = budget\n",
        "  tol = 0.005\n",
        "\n",
        "  table1 = p_table[p_table[\"n_experts\"] == e_rate]\n",
        "  table1 = table1.sort_values(by=\"flops\")\n",
        "  table1[\"tpr\"] = table1[\"train_tokens\"] / table1[\"total_params\"]\n",
        "\n",
        "  flop_min = np.exp((1-tol)*np.log(flop_budget))\n",
        "  flop_max = np.exp((1+tol)*np.log(flop_budget))\n",
        "  # print(f\"{flop_min=:.2e},{flop_max=:.2e}\")\n",
        "  filtered_table = table1[table1[\"flops\"] > flop_min]\n",
        "  filtered_table = filtered_table[filtered_table[\"flops\"] < flop_max]\n",
        "  return len(filtered_table), filtered_table[\"flops\"].min(), filtered_table[\"flops\"].max(), flop_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1tCspDDYGt79"
      },
      "outputs": [],
      "source": [
        "start_budget = 8.741692e+16\n",
        "end_budget = 3.177170e+20\n",
        "\n",
        "budgets_to_anaylyze = []\n",
        "current_budget = start_budget\n",
        "prev_max_found = current_budget\n",
        "\n",
        "while current_budget < end_budget:\n",
        "  num_exps, min_found, max_found, flop_max = calc_num_exps(current_budget)\n",
        "  if num_exps >= 5 and min_found > prev_max_found:\n",
        "    budgets_to_anaylyze.append(current_budget)\n",
        "    prev_max_found = max_found\n",
        "  current_budget = flop_max\n",
        "\n",
        "budgets_to_analyze = budgets_to_anaylyze[1:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mCX_Aj6iHDwW"
      },
      "outputs": [],
      "source": [
        "def calculate_closest_budget(budget):\n",
        "    closest_budget = min(budgets_to_analyze, key=lambda x: abs(x - budget))\n",
        "    return closest_budget\n",
        "\n",
        "def fit_parabola_and_find_min(group):\n",
        "    # Log-transform the data\n",
        "    x = np.log(group['train_tokens'])\n",
        "    y = np.log(group['loss'])\n",
        "\n",
        "    # Fit a quadratic function: y = a*x^2 + b*x + c\n",
        "    coefficients = np.polyfit(x, y, deg=2)\n",
        "    a, b, c = coefficients\n",
        "\n",
        "    # Calculate the x-coordinate of the parabola's vertex (minimum point)\n",
        "    x_min = -b / (2 * a)\n",
        "\n",
        "    # Convert back to the original scale\n",
        "    optimal_train_tokens = np.exp(x_min)\n",
        "\n",
        "    # Calculate the predicted minimum loss\n",
        "    min_loss_log = a * x_min**2 + b * x_min + c\n",
        "    predicted_min_loss = np.exp(min_loss_log)\n",
        "\n",
        "    # Return the results\n",
        "    return pd.DataFrame({\n",
        "        'budget_group': [group.name],\n",
        "        'optimal_train_tokens': [optimal_train_tokens],\n",
        "        'predicted_min_loss': [predicted_min_loss]\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fbkr8G1HHRt1",
        "outputId": "a0705142-eeb9-4abe-d8f4-71e8796e62a1"
      },
      "outputs": [],
      "source": [
        "e_rate = 1\n",
        "\n",
        "p_table[\"budget_group\"] = p_table[\"flops\"].map(calculate_closest_budget)\n",
        "budget_group_diff = np.abs(np.log(p_table[\"flops\"]) - np.log(p_table[\"budget_group\"]))\n",
        "filtered_table = p_table[budget_group_diff < 0.2]\n",
        "df = filtered_table[filtered_table[\"n_experts\"] == e_rate]\n",
        "flop_budgets = budgets_to_analyze[1:-1]\n",
        "df[\"budget_group\"] = df[\"flops\"].map(calculate_closest_budget)\n",
        "\n",
        "plot_width = 1200\n",
        "plot_height = 800\n",
        "fig = go.Figure()\n",
        "colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray']\n",
        "color_map = {bg: colors[i % len(colors)] for i, bg in enumerate(flop_budgets)}\n",
        "\n",
        "for bg in flop_budgets:\n",
        "    df_bg = df[df['budget_group'] == bg].copy()\n",
        "    x = df_bg['train_tokens']\n",
        "    y = df_bg['loss']\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        mode='markers',\n",
        "        name=f'Budget Group {bg}',\n",
        "        marker=dict(color=color_map[bg])\n",
        "    ))\n",
        "\n",
        "    # Fit a parabola\n",
        "    log_x = np.log(x)\n",
        "    log_y = np.log(y)\n",
        "    coeffs = np.polyfit(log_x, log_y, 2)\n",
        "\n",
        "    # Generate x values for the fit line in the actual value space\n",
        "    x_fit = np.logspace(np.log10(x.min()), np.log10(x.max()), 100)\n",
        "    log_x_fit = np.log(x_fit)\n",
        "    log_y_fit = coeffs[0]*log_x_fit**2 + coeffs[1]*log_x_fit + coeffs[2]\n",
        "    y_fit = np.exp(log_y_fit)\n",
        "\n",
        "    # Plot the fitted parabola as a dashed line\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_fit,\n",
        "        y=y_fit,\n",
        "        mode='lines',\n",
        "        showlegend=False,\n",
        "        line=dict(dash='dash', color=color_map[bg])\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='IsoFLOP profiles',\n",
        "    xaxis_title='Training Tokens',\n",
        "    yaxis_title='Loss',\n",
        "    legend_title='Budget Groups',\n",
        "    template='plotly_white',\n",
        "    width=plot_width,\n",
        "    height=plot_height,\n",
        "    xaxis_type='log',\n",
        "    yaxis_type='log',\n",
        "    xaxis=dict(\n",
        "        tickformat='.1e',\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickformat='.1e',\n",
        "        range=[np.log10(y.min()-0.1), np.log10(3.3)],\n",
        "        tickmode='linear',\n",
        "        dtick=0.5\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qSaxEBoSHY1V",
        "outputId": "5481cf4a-8cf8-4e7e-c833-302decd2e1cf"
      },
      "outputs": [],
      "source": [
        "def optimal_tpr(e_rate):\n",
        "  p_table[\"budget_group\"] = p_table[\"flops\"].map(calculate_closest_budget)\n",
        "  budget_group_diff = np.abs(np.log(p_table[\"flops\"]) - np.log(p_table[\"budget_group\"]))\n",
        "  filtered_table = p_table[budget_group_diff < 0.2]\n",
        "\n",
        "  # Apply the function to each budget_group\n",
        "  results = filtered_table[filtered_table[\"n_experts\"] == e_rate].groupby('budget_group').apply(fit_parabola_and_find_min)\n",
        "\n",
        "  # Reset index to flatten the DataFrame\n",
        "  results = results.reset_index(drop=True)\n",
        "\n",
        "  results[\"token_param_ratio\"] = results[\"optimal_train_tokens\"] / (results[\"budget_group\"] / (6*results[\"optimal_train_tokens\"]))\n",
        "  return results[\"token_param_ratio\"][:6].mean()\n",
        "\n",
        "def list_optimal_tpr(e_rate):\n",
        "  p_table[\"budget_group\"] = p_table[\"flops\"].map(calculate_closest_budget)\n",
        "  budget_group_diff = np.abs(np.log(p_table[\"flops\"]) - np.log(p_table[\"budget_group\"]))\n",
        "  filtered_table = p_table[budget_group_diff < 0.2]\n",
        "\n",
        "  # Apply the function to each budget_group\n",
        "  results = filtered_table[filtered_table[\"n_experts\"] == e_rate].groupby('budget_group').apply(fit_parabola_and_find_min)\n",
        "\n",
        "  # Reset index to flatten the DataFrame\n",
        "  results = results.reset_index(drop=True)\n",
        "\n",
        "  results[\"token_param_ratio\"] = results[\"optimal_train_tokens\"] / (results[\"budget_group\"] / (6*results[\"optimal_train_tokens\"]))\n",
        "  return results[\"token_param_ratio\"][:6]\n",
        "\n",
        "list_optimal_tpr(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EJyJroclHiVc",
        "outputId": "f71724f4-9b61-468d-fb74-22d8daeacb22"
      },
      "outputs": [],
      "source": [
        "tprs = pd.DataFrame({\"Expansion Rate\": [1, 2, 4, 8, 16]})\n",
        "tprs[\"Optimal Token to Active Param Ratio\"] = tprs[\"Expansion Rate\"].map(optimal_tpr)\n",
        "\n",
        "tprs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Cl2N-f4kHqUu"
      },
      "outputs": [],
      "source": [
        "def get_compute_optimal_tpr(e_rate):\n",
        "  return tprs[tprs[\"Expansion Rate\"] == e_rate][\"Optimal Token to Active Param Ratio\"].iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3lqfBNzIC1q"
      },
      "source": [
        "#### Memory constrained plots based on data (plot with buttons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "HOHbYsROHl_b"
      },
      "outputs": [],
      "source": [
        "def render_with_buttons(options: list):\n",
        "    frames = []\n",
        "    params = []\n",
        "    nums_experts = []\n",
        "\n",
        "    # Build one frame per option\n",
        "    for i, (dmodel1, n_experts1, dmodel2, n_experts2) in enumerate(options):\n",
        "        # Filter data\n",
        "        filtered_table_1 = p_table[(p_table[\"dmodel\"] == dmodel1) & (p_table[\"n_experts\"] == n_experts1)]\n",
        "        filtered_table_2 = p_table[(p_table[\"dmodel\"] == dmodel2) & (p_table[\"n_experts\"] == n_experts2)]\n",
        "        filtered_table = pd.concat([filtered_table_1, filtered_table_2])\n",
        "\n",
        "        filtered_table[\"tokens_r\"] = filtered_table[\"train_tokens\"].map(format_large_numbers)\n",
        "        filtered_table[\"active_r\"] = filtered_table[\"active_params\"].map(format_large_numbers)\n",
        "        filtered_table = filtered_table.sort_values(by=\"flops\")\n",
        "\n",
        "        data_traces = []\n",
        "        # Build traces for each unique n_experts in filtered data\n",
        "        for n_exp in filtered_table[\"n_experts\"].unique():\n",
        "            sub_df = filtered_table[filtered_table[\"n_experts\"] == n_exp]\n",
        "            data_traces.append(\n",
        "                go.Scatter(\n",
        "                    x=sub_df[\"flops\"],\n",
        "                    y=sub_df[\"loss\"],\n",
        "                    mode=\"lines+markers\",\n",
        "                    name=f\"n_experts={n_exp}\",\n",
        "                    text=[\n",
        "                        f\"tokens_r={x}; active_r={y}\"\n",
        "                        for x, y in zip(sub_df[\"tokens_r\"], sub_df[\"active_r\"])\n",
        "                    ],\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # We'll create the vertical lines and place annotations near those lines\n",
        "        annotations = []\n",
        "        y_min, y_max = filtered_table[\"loss\"].min(), filtered_table[\"loss\"].max()\n",
        "\n",
        "        # Vline + annotation for model 1\n",
        "        if not filtered_table_1.empty:\n",
        "            compute_opt_tpr_1 = get_compute_optimal_tpr(n_experts1)\n",
        "            vline1_x = (\n",
        "                filtered_table_1[\"active_params\"].iloc[0]\n",
        "                * compute_opt_tpr_1\n",
        "                * filtered_table_1[\"active_params\"].iloc[0]\n",
        "                * 6\n",
        "            )\n",
        "            data_traces.append(\n",
        "                go.Scatter(\n",
        "                    x=[vline1_x, vline1_x],\n",
        "                    y=[y_min, y_max],\n",
        "                    mode=\"lines\",\n",
        "                    line=dict(dash=\"dash\", color=\"blue\"),\n",
        "                    showlegend=False,\n",
        "                )\n",
        "            )\n",
        "            # Position the annotation near the top of that line\n",
        "            annotations.append(\n",
        "                go.layout.Annotation(\n",
        "                    x=vline1_x,\n",
        "                    y=y_max,\n",
        "                    xref=\"x\",\n",
        "                    yref=\"y\",\n",
        "                    text=\"Compute optimal for model 1\",\n",
        "                    showarrow=False,\n",
        "                    font=dict(color=\"blue\"),\n",
        "                    xanchor=\"left\",\n",
        "                    yanchor=\"top\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Vline + annotation for model 2\n",
        "        if not filtered_table_2.empty:\n",
        "            compute_opt_tpr_2 = get_compute_optimal_tpr(n_experts2)\n",
        "            vline2_x = (\n",
        "                filtered_table_2[\"active_params\"].iloc[0]\n",
        "                * compute_opt_tpr_2\n",
        "                * filtered_table_2[\"active_params\"].iloc[0]\n",
        "                * 6\n",
        "            )\n",
        "            data_traces.append(\n",
        "                go.Scatter(\n",
        "                    x=[vline2_x, vline2_x],\n",
        "                    y=[y_min, y_max],\n",
        "                    mode=\"lines\",\n",
        "                    line=dict(dash=\"dash\", color=\"red\"),\n",
        "                    showlegend=False,\n",
        "                )\n",
        "            )\n",
        "            # Position the annotation near the bottom of that line\n",
        "            annotations.append(\n",
        "                go.layout.Annotation(\n",
        "                    x=vline2_x,\n",
        "                    y=y_min,\n",
        "                    xref=\"x\",\n",
        "                    yref=\"y\",\n",
        "                    text=\"Compute optimal for model 2\",\n",
        "                    showarrow=False,\n",
        "                    font=dict(color=\"red\"),\n",
        "                    xanchor=\"left\",\n",
        "                    yanchor=\"bottom\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Create a frame with the data and layout (annotations)\n",
        "        frames.append(\n",
        "            go.Frame(\n",
        "                data=data_traces,\n",
        "                name=f\"frame_{i}\",\n",
        "                layout=go.Layout(annotations=annotations),\n",
        "            )\n",
        "        )\n",
        "        params.append(format_large_numbers(filtered_table_2[\"total_params\"].iloc[0]))\n",
        "        nums_experts.append((n_experts2, n_experts1))\n",
        "\n",
        "    # Build the figure using the first frame as initial data if it exists\n",
        "    fig = go.Figure(data=frames[0].data if frames else [])\n",
        "\n",
        "    # Add all frames\n",
        "    fig.update(frames=frames)\n",
        "\n",
        "    # Add buttons for each option\n",
        "    fig.update_layout(\n",
        "        updatemenus=[\n",
        "            {\n",
        "                \"type\": \"buttons\",\n",
        "                \"buttons\": [\n",
        "                    {\n",
        "                        \"label\": f\"Total {params[i]}, E={nums_experts[i][0]} vs E={nums_experts[i][1]}\",\n",
        "                        \"method\": \"animate\",\n",
        "                        \"args\": [\n",
        "                            [f\"frame_{i}\"],\n",
        "                            {\n",
        "                                \"frame\": {\"duration\": 0, \"redraw\": True},\n",
        "                                \"transition\": {\"duration\": 0},\n",
        "                                \"mode\": \"immediate\",\n",
        "                            },\n",
        "                        ],\n",
        "                    }\n",
        "                    for i in range(len(options))\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        width=1500,\n",
        "        height=600,\n",
        "    )\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6ETpBDISHycW",
        "outputId": "8dc008c4-7736-41b0-d666-729d11d906cd"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "options = [\n",
        "    [768, 16, 1664.0, 1.0],\n",
        "    [1408, 2, 1664.0, 1.0],\n",
        "    [1280, 4, 1664.0, 1.0],\n",
        "    [768, 8, 1408, 1],\n",
        "    [640.0, 16.0, 1408.0, 1.0],\n",
        "    [1024.0, 2.0, 1280.0, 1.0],\n",
        "    [640, 2, 768, 1],\n",
        "    [384, 8, 512, 1],\n",
        "    [256, 16, 512, 1]\n",
        "]\n",
        "render_with_buttons(options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivq2a_FOXwAO"
      },
      "source": [
        "# MP Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Dyau2u1fXy2R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "DV = 50257\n",
        "\n",
        "\n",
        "PLOTS_DIR = './analysis_plots'\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "def save_to_zip():\n",
        "    !rm ./analysis_plots.zip\n",
        "    !zip -r /content/analysis_plots.zip ./analysis_plots\n",
        "    pass\n",
        "\n",
        "\n",
        "# def get_dm_from_N(N, dv, E):\n",
        "#     dms = np.roots([(4+9*E)/64, 0, 2 * dv, -N])\n",
        "#     real_positive_solutions = [dm for dm in dms if np.isreal(dm) and np.real(dm) > 0]\n",
        "#     assert len(real_positive_solutions) == 1\n",
        "#     return np.real(real_positive_solutions[0])\n",
        "\n",
        "# def get_Nact_from_N(N, dv, E):\n",
        "#     # dm = get_dm_from_N(N, dv, E)\n",
        "#     # return dm**3 / 4 + 2 * dm * dv\n",
        "#     raise NotImplementedError()\n",
        "\n",
        "# def get_dm_from_B_with_kv(B, dv, E, T):\n",
        "#     dms = np.roots([(4+9*E)/64, T/32, 2 * dv, -B])\n",
        "#     real_positive_solutions = [dm for dm in dms if np.isreal(dm) and np.real(dm) > 0]\n",
        "#     assert len(real_positive_solutions) == 1\n",
        "#     return np.real(real_positive_solutions[0])\n",
        "\n",
        "# def get_Nkv_per_token_from_N(N, dv, E):\n",
        "#     dm = get_dm_from_N(N, dv, E)\n",
        "#     kv = (dm**2) / 32\n",
        "#     return kv\n",
        "\n",
        "# def get_dm_from_N(N, dv, E):\n",
        "#     # dms = np.roots([(4+9*E)/64, 0, 2 * dv, -N])\n",
        "#     dm = get_dm_from_N_total_with_embeddings_with_kv(B=N, dv=dv, E=)\n",
        "#     real_positive_solutions = [dm for dm in dms if np.isreal(dm) and np.real(dm) > 0]\n",
        "#     assert len(real_positive_solutions) == 1\n",
        "#     return np.real(real_positive_solutions[0])\n",
        "\n",
        "def get_Nact_from_N(N, dv, E):\n",
        "    # dm = get_dm_from_N(N, dv, E)\n",
        "    # return dm**3 / 4 + 2 * dm * dv\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def get_dm_from_N_total_with_embeddings_with_kv(B, dv, E, T):\n",
        "    dms = np.roots([(4+9*E)/64, T/32, 2 * dv, -B])\n",
        "    real_positive_solutions = [dm for dm in dms if np.isreal(dm) and np.real(dm) > 0]\n",
        "    assert len(real_positive_solutions) == 1, f\"Found multiple or no solutions: {real_positive_solutions}\"\n",
        "    return np.real(real_positive_solutions[0])\n",
        "\n",
        "def get_dm_from_N_active_nonembedding(N):\n",
        "    return (64 * N / 13) ** (1/3)\n",
        "\n",
        "def get_dm_from_N_active_w_embedding(N, dv):\n",
        "    dms = np.roots([13/64, 0, 2 * dv, -N])\n",
        "    real_positive_solutions = [dm for dm in dms if np.isreal(dm) and np.real(dm) > 0]\n",
        "    assert len(real_positive_solutions) == 1, f\"Found multiple or no solutions: {real_positive_solutions}\"\n",
        "    return np.real(real_positive_solutions[0])\n",
        "\n",
        "def get_Nkv_per_token_from_N_total_with_embeddings(N, dv, E):\n",
        "    dm = get_dm_from_N_total_with_embeddings_with_kv(B=N, dv=dv, E=E, T=0)\n",
        "    kv = (dm**2) / 32\n",
        "    return kv\n",
        "\n",
        "# print(get_dm_from_N(1e9, DV, 1))\n",
        "# print(get_dm_from_B_with_kv(1e9, DV, 1, 16384))\n",
        "# print(get_Nkv_per_token_from_N(8e9, DV, 1))\n",
        "# print(get_Nact_from_N(1e9, DV, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k8_axHVGXzkn",
        "outputId": "08731199-1e6b-4f70-f9a5-8452fef6e035"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "predicted_loss = predict_loss_joined(N=N, D=D, E=E, **res)\n",
        "colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'black']\n",
        "\n",
        "\n",
        "predicted_loss = predict_loss_joined(N=N, D=D, E=E, **res)\n",
        "\n",
        "def plot_L_N_kvcache_inference_optimal(res, budget,\n",
        "                                       n_kv_tokens, n_inference_tokens, is_log_y,\n",
        "                                       subtract_dataset_entropy, ending_mode='all',\n",
        "                                       points_to_plot = None, Es = [1, 2, 4, 8, 16, 32],\n",
        "                                       path_to_save = None):\n",
        "\n",
        "\n",
        "\n",
        "    budget_exponent = np.log10(budget)\n",
        "    total_N = np.logspace(budget_exponent / 2 - 2, budget_exponent / 2, num=1000, dtype='float64')\n",
        "    Ls = [f'L_{e}' for e in Es]\n",
        "    rows = []\n",
        "    x_column = 'total_N'\n",
        "    for i in range(len(total_N)):\n",
        "        row = {x_column: total_N[i]}\n",
        "        if n_kv_tokens > 0:\n",
        "            dm = get_dm_from_N_total_with_embeddings_with_kv(total_N[i], DV, 1, 0)\n",
        "            kv_size = n_kv_tokens * dm ** 2 / 32\n",
        "            memory_constraint = total_N[i] + kv_size\n",
        "        else:\n",
        "            memory_constraint = total_N[i]\n",
        "        for exp_rate in Es:\n",
        "            dm = get_dm_from_N_total_with_embeddings_with_kv(memory_constraint, DV, exp_rate, n_kv_tokens)\n",
        "            # dm = get_dm_from_N_total_with_embeddings_with_kv(total_N[i], 0, exp_rate, n_kv_tokens)\n",
        "            active_N_nonembed = (13 / 64) * dm**3\n",
        "            embed_size = 2 * dm * DV\n",
        "            active_N_with_embed = active_N_nonembed + embed_size\n",
        "            inference_cost = 2 * n_inference_tokens * active_N_with_embed\n",
        "            train_budget = budget - inference_cost\n",
        "            D = train_budget / (6 * active_N_with_embed)\n",
        "            row[f'D_{exp_rate}'] = D\n",
        "            # row[f'active_N_{exp_rate}'] = active_N_nonembed\n",
        "            row[f'active_N_{exp_rate}'] = active_N_with_embed\n",
        "            row[f'dm_{exp_rate}'] = dm\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    for e in Es:\n",
        "        L = predict_loss_joined(N=torch.tensor(df[f'active_N_{e}']), D=torch.tensor(df[f'D_{e}']), E=torch.tensor([e] * len(df)), **res)\n",
        "        if subtract_dataset_entropy:\n",
        "            L -= res['C']\n",
        "\n",
        "        df[f\"L_{e}\"] = L.numpy()\n",
        "\n",
        "\n",
        "\n",
        "    df['optimal'] = df[Ls].idxmin(axis=1)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # Determine the min and max values for the x and y axes\n",
        "    min_x = df[x_column].min()\n",
        "    max_x = df[x_column].max()\n",
        "    min_y = df[Ls].min().min()\n",
        "    max_y = df[Ls].max().max()\n",
        "\n",
        "    for e, color in zip(Es, colors):\n",
        "        optimal_interval = (df['optimal'] == f'L_{e}')\n",
        "        if not optimal_interval.any():\n",
        "            continue\n",
        "        last_true_index = df[optimal_interval].index[-1]\n",
        "        if last_true_index < df.shape[0] - 1:\n",
        "            optimal_interval[last_true_index + 1] = True\n",
        "\n",
        "        plt.fill_between(df['total_N'], -1, 1000,\n",
        "                    where=optimal_interval,\n",
        "                        color=color, alpha=0.1, label=str(e))\n",
        "\n",
        "    # Determine the min and max values for the x and y axes\n",
        "    min_x = df[x_column].min()\n",
        "    max_x = df[x_column].max()\n",
        "    if ending_mode == 'all':\n",
        "        min_y = df[Ls].min().min()\n",
        "        max_y = df[Ls].max().max()\n",
        "    elif ending_mode == 'constant' or ending_mode == 'cut':\n",
        "        min_y = float('inf')\n",
        "        max_y = -float('inf')\n",
        "    else:\n",
        "        raise ValueError(f'Unknwon ending_mode: {ending_mode}')\n",
        "\n",
        "    for e, color in zip(Es, colors):\n",
        "        if ending_mode == 'all':\n",
        "            sns.lineplot(data=df, x=x_column, y=f'L_{e}', label=f'{e}', color=color)\n",
        "            # sns.lineplot(data=df, x=f\"active_N_{e}\", y=f'L_{e}', label=f'{e}', color=color)\n",
        "        elif ending_mode == 'constant':\n",
        "            dfcp = df.copy()\n",
        "            optimal_N_idx = dfcp[f'L_{e}'].idxmin()\n",
        "            optimal_N = dfcp.iloc[optimal_N_idx]['total_N']\n",
        "            optimal_L = dfcp.iloc[optimal_N_idx][f'L_{e}']\n",
        "            dfcp.loc[df['total_N'] > optimal_N, f'L_{e}'] = optimal_L\n",
        "\n",
        "            min_y = min(min_y, dfcp[f'L_{e}'].min())\n",
        "            max_y = max(max_y, dfcp[f'L_{e}'].max())\n",
        "\n",
        "            sns.lineplot(data=dfcp, x=x_column, y=f'L_{e}', label=f'{e}', color=color)\n",
        "        elif ending_mode == 'cut':\n",
        "            optimal_N_idx = df[f'L_{e}'].idxmin()\n",
        "            optimal_N = df.iloc[optimal_N_idx]['total_N']\n",
        "            subdf = df[df['total_N'] <= optimal_N]\n",
        "\n",
        "            min_y = min(min_y, subdf[f'L_{e}'].min())\n",
        "            max_y = max(max_y, subdf[f'L_{e}'].max())\n",
        "\n",
        "            sns.lineplot(data=subdf, x=x_column, y=f'L_{e}', label=f'{e}', color=color)\n",
        "        else:\n",
        "            raise ValueError(f'Unknwon ending_mode: {ending_mode}')\n",
        "\n",
        "    if points_to_plot is not None:\n",
        "        for e, color in zip(Es, colors):\n",
        "            same_e_subset = points_to_plot[points_to_plot[f'E'] == e]\n",
        "            diff = 0 if not subtract_dataset_entropy else res['C']\n",
        "            plt.scatter(same_e_subset['N_total'], same_e_subset['L'] - diff, marker='o', s=50, color=color)\n",
        "\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel(x_column)\n",
        "    if is_log_y:\n",
        "        plt.yscale('log')\n",
        "    plt.ylabel('Loss (L)')\n",
        "    plt.title('Model Comparison: Dense vs MoE')\n",
        "\n",
        "    # Example: Set custom limits for the x and y axes\n",
        "    plt.xlim(min_x, max_x)\n",
        "    print(max_y, min_y)\n",
        "    # y_margins = (max_y - min_y) / 10\n",
        "    # plt.ylim(min_y - min_y / 1, max_y + max_y / 100)\n",
        "    plt.ylim(min_y - min_y / 100, max_y + max_y / 100)\n",
        "\n",
        "    # plt.axhline(y=2.6491099085, color='r', linestyle='-')\n",
        "    if path_to_save:\n",
        "        plt.savefig(path_to_save, bbox_inches='tight')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# predicted_loss_test = predict_loss_joined(N=N_test, D=D_test, E=E_test, **res)\n",
        "\n",
        "# ad_hoc_table = pd.DataFrame(columns=['N', 'D', 'E', 'L'])\n",
        "# ad_hoc_table = pd.DataFrame({\n",
        "#     \"N_active_w_embedding\": N_test.numpy(),\n",
        "#     \"D\": D_test.numpy(),\n",
        "#     \"E\": E_test.numpy(),\n",
        "#     \"L\": L_test.numpy(),\n",
        "#     \"D/N\": (D_test/N_test).numpy(),\n",
        "#     \"L_pred\": predicted_loss_test.numpy(),\n",
        "#     \"error\": (predicted_loss_test - L_test).numpy(),\n",
        "#     }\n",
        "# )\n",
        "\n",
        "predicted_loss_train = predict_loss_joined(N=N, D=D, E=E, **res)\n",
        "ad_hoc_table = pd.DataFrame({\n",
        "    \"N_active_w_embedding\": N.numpy(),\n",
        "    \"D\": D.numpy(),\n",
        "    \"E\": E.numpy(),\n",
        "    \"L\": L.numpy(),\n",
        "    \"D/N\": (D/N).numpy(),\n",
        "    \"L_pred\": predicted_loss_train.numpy(),\n",
        "    \"error\": (predicted_loss_train - L).numpy(),\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# N, D, L, E, N_test, D_test, L_test, E_test\n",
        "# fill with N, D, E, L and {N, D, E, L}_test\n",
        "# dm = get_dm_from_N_active_nonembedding(ad_hoc_table['N_active_nonembedding'])\n",
        "dm = ad_hoc_table['N_active_w_embedding'].apply(get_dm_from_N_active_w_embedding, args=(DV,))\n",
        "# print(dm)\n",
        "ad_hoc_table['N_total'] = 2*DV*dm + (4 + 9*ad_hoc_table['E']) * dm**3 / 64\n",
        "# ad_hoc_table['N_active_w_embedding'] = 2*DV*dm + (13 / 64) * dm**3\n",
        "ad_hoc_table['FLOPS'] = 6 * ad_hoc_table['N_active_w_embedding'] * ad_hoc_table['D']\n",
        "ad_hoc_table['log_FLOPS'] = np.log10(ad_hoc_table['FLOPS'])\n",
        "\n",
        "px.histogram(ad_hoc_table['log_FLOPS']).show()\n",
        "\n",
        "# subset = ad_hoc_table[(1.5e19 <= ad_hoc_table['FLOPS']) & (ad_hoc_table['FLOPS'] <=2e19)]\n",
        "# lo_f, hi_f = 18.8, 19.0\n",
        "# lo_f, hi_f = 20.0, 20.2\n",
        "# lo_f, hi_f = 20.8, 21.0\n",
        "# lo_f, hi_f = 20.2, 20.4\n",
        "# lo_f, hi_f = 25, 25\n",
        "lo_f, hi_f = 22, 22\n",
        "# lo_f, hi_f = 19.9, 20.0\n",
        "# lo_f, hi_f = np.log10(0.52e20), np.log10(0.54e20)\n",
        "\n",
        "\n",
        "ad_hoc_table[\"flops_overhead\"] = ad_hoc_table['FLOPS']/10**((lo_f + hi_f) / 2)\n",
        "subset = ad_hoc_table[(lo_f <= ad_hoc_table['log_FLOPS']) & (ad_hoc_table['log_FLOPS'] <= hi_f)]\n",
        "\n",
        "plot_L_N_kvcache_inference_optimal(\n",
        "    budget=10**((lo_f + hi_f) / 2),\n",
        "    res=res,\n",
        "    n_kv_tokens=0,\n",
        "    n_inference_tokens=0,\n",
        "    is_log_y=False,\n",
        "    subtract_dataset_entropy=False,\n",
        "    ending_mode='cut',\n",
        "    points_to_plot = subset,\n",
        "    path_to_save=f'{PLOTS_DIR}/best_for_N.pdf'\n",
        ")\n",
        "plot_L_N_kvcache_inference_optimal(\n",
        "    budget=10**((lo_f + hi_f) / 2),\n",
        "    res=res,\n",
        "    n_kv_tokens=8192,\n",
        "    n_inference_tokens=0,\n",
        "    is_log_y=False,\n",
        "    subtract_dataset_entropy=False,\n",
        "    ending_mode='cut',\n",
        "    points_to_plot = subset,\n",
        "    path_to_save=f'{PLOTS_DIR}/best_for_N_kvcache.pdf'\n",
        ")\n",
        "\n",
        "save_to_zip()\n",
        "\n",
        "# inference_budget = 1e22\n",
        "# inference_tokens = 1e12\n",
        "# plot_L_N_kvcache_inference_optimal(\n",
        "#     budget=10**((lo_f + hi_f) / 2) + inference_budget,\n",
        "#     res=res,\n",
        "#     n_kv_tokens=0,\n",
        "#     n_inference_tokens=inference_tokens,\n",
        "#     is_log_y=False,\n",
        "#     subtract_dataset_entropy=False,\n",
        "#     ending_mode='cut',\n",
        "#     points_to_plot = subset\n",
        "# )\n",
        "\n",
        "subset.sort_values(by='error', key=abs, ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aGrQFDwbYYBa",
        "outputId": "007ffc0d-63bd-44ba-ce41-cc82bd7a2b08"
      },
      "outputs": [],
      "source": [
        "llama405budget = 3.8e25\n",
        "llama70budget = llama405budget * (70/405)\n",
        "llama8budget = llama405budget * (8/405)\n",
        "import random\n",
        "\n",
        "# Define the different N values to use\n",
        "# def plot_L_F(fits, Ns, sizes_labels, Fs, n_kv_tokens, n_inference_tokens, path_to_save = None):\n",
        "def plot_L_F(res, Ns, sizes_labels, Fs, n_kv_tokens, n_inference_tokens = None,\n",
        "             inference_cost_times = None, landmark_budgets = [llama8budget, llama70budget, llama405budget],\n",
        "             landmark_labels = ['Llama 3 8B', 'Llama 3 70B', 'Llama 3 405B'],\n",
        "             path_to_save = None, Es=[1, 2, 4, 8, 16, 32]):\n",
        "    plt.figure(figsize=(20, 12))\n",
        "\n",
        "    if not isinstance(n_kv_tokens, list):\n",
        "        n_kv_tokens = [n_kv_tokens] * len(Ns)\n",
        "\n",
        "    Ls = [f'L_{e}' for e in Es]\n",
        "    # Loop over the different N values\n",
        "    for idx, (N, n_kv_tokens_of_N, size_label) in enumerate(zip(Ns, n_kv_tokens, sizes_labels)):\n",
        "        rows = []\n",
        "        # actual_N = N + get_Nkv_per_token_from_N(N, DV, 1) * n_kv_tokens_of_N\n",
        "        # actual_N = N + get_Nkv_per_token_from_N(N, DV, 1) * n_kv_tokens_of_N\n",
        "        actual_N = N + get_Nkv_per_token_from_N_total_with_embeddings(N, DV, 1) * n_kv_tokens_of_N\n",
        "        for i in range(len(Fs)):\n",
        "            row = {'F': Fs[i]}\n",
        "            for exp_rate in Es:\n",
        "                    best_L = float('inf')\n",
        "                    best_N = None\n",
        "                    candidate_Ns = []\n",
        "                    candidate_Ds = []\n",
        "                    candidate_Es = []\n",
        "                    for candidate_N in np.logspace(np.log10(actual_N) / 4, np.log10(actual_N), 100):\n",
        "                        dm = get_dm_from_N_total_with_embeddings_with_kv(candidate_N, DV, exp_rate, n_kv_tokens_of_N)\n",
        "                        active_N_nonembed = (13 / 64) * dm**3\n",
        "                        embed_size = 2 * dm * DV\n",
        "                        active_N_with_embed = active_N_nonembed + embed_size\n",
        "                        assert inference_cost_times is None or n_inference_tokens is None\n",
        "                        if inference_cost_times is not None:\n",
        "                            train_budget = Fs[i] / (1 + inference_cost_times)\n",
        "                        elif n_inference_tokens is not None:\n",
        "                            inference_cost = 2 * n_inference_tokens * active_N_with_embed\n",
        "                            train_budget = Fs[i] - inference_cost\n",
        "                        else:\n",
        "                            train_budget = Fs[i]\n",
        "                        D = train_budget / (6 * active_N_with_embed)\n",
        "                        # candidate_Ns.append(active_N_nonembed)\n",
        "                        candidate_Ns.append(active_N_with_embed)\n",
        "                        candidate_Ds.append(D)\n",
        "                        candidate_Es.append(exp_rate)\n",
        "                        # candidate_L = predict_loss_joined(N=torch.tensor([active_N_nonembed]), D=torch.tensor([D]), E=torch.tensor([exp_rate]), **res).item()\n",
        "                        # candidate_L = rand_float = random.uniform(2, 4)\n",
        "                        # candidate_L -= res['C']\n",
        "                        # if candidate_L < best_L:\n",
        "                        #     best_L = candidate_L\n",
        "                        #     row[f'L_{exp_rate}'] = candidate_L - res['C']\n",
        "                        #     row[f'D_{exp_rate}'] = D\n",
        "                        #     row[f'active_N_{exp_rate}'] = active_N_nonembed\n",
        "                        # else:\n",
        "                        #     break\n",
        "                    # Losses = predict_loss_joined(N=torch.tensor([active_N_nonembed]), D=torch.tensor([D]), E=torch.tensor([exp_rate]), **res).item()\n",
        "                    candidate_Ls = predict_loss_joined(N=torch.tensor(candidate_Ns), D=torch.tensor(candidate_Ds), E=torch.tensor(candidate_Es), **res)\n",
        "                    # candidate_Ls -= res['C']\n",
        "                    min_idx = np.argmin(candidate_Ls)\n",
        "                    row[f'L_{exp_rate}'] = candidate_Ls[min_idx].item()\n",
        "                    row[f'D_{exp_rate}'] = candidate_Ds[min_idx]\n",
        "                    row[f'active_N_{exp_rate}'] = candidate_Ns[min_idx]\n",
        "            rows.append(row)\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "\n",
        "        df['optimal'] = df[Ls].idxmin(axis=1)\n",
        "        df['min_loss'] = df[Ls].min(axis=1)\n",
        "\n",
        "        # Plot only the optimal points with the corresponding color and line style for each N value\n",
        "        for e, color in zip(Es, colors):\n",
        "            optimal_indices = df['optimal'] == f'L_{e}'\n",
        "            label = f'Optimal {e} (N = {N:.0e})'\n",
        "            start_y = df[optimal_indices]['min_loss'].max()\n",
        "            start_x = df[optimal_indices]['F'].min()\n",
        "            sns.lineplot(data=df[optimal_indices], x='F', y='min_loss', label='_nolegend_', color=color) #, linestyle=['-', '-', '-'][idx])\n",
        "            plt.text(start_x * 1.1, start_y * 1.0, f\"{e}\", color=color, fontsize=24)\n",
        "\n",
        "        plt.text(Fs.max() * 0.6, 1.1 * df['min_loss'].min(), size_label, color='black', fontsize=20, ha='left', va='bottom')\n",
        "\n",
        "\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('F', fontsize=16)\n",
        "    plt.ylabel('L - c', fontsize=16)\n",
        "    plt.title('Model Comparison: Optimal Loss by Expansion Rate for Different N Values', fontsize=16)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=20)  # Adjust 'labelsize' as needed\n",
        "\n",
        "    for landmark_budget, landmark_label, linestyle in zip(landmark_budgets, landmark_labels, ['-', '--', '-.']):\n",
        "        plt.axvline(x=landmark_budget, color='black', linestyle=linestyle, linewidth=2, label=landmark_label)\n",
        "\n",
        "    if landmark_budgets and landmark_labels:\n",
        "        plt.legend(loc='lower left', fontsize=24)\n",
        "\n",
        "    if path_to_save:\n",
        "        plt.savefig(path_to_save, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "a100_hour_flops = (1/2) * 312 * 10**12 * 3600 # 50% mfu * bfloat16 perf. * seconds in an hour\n",
        "\n",
        "plot_L_F(res, [2e9, 5e9, 10e9],\n",
        "         sizes_labels = [\"2B\", \"5B\", \"10B\"],\n",
        "         Fs = np.logspace(19, 23, 100),\n",
        "         n_kv_tokens=0, n_inference_tokens=0,\n",
        "         landmark_budgets = [],\n",
        "         landmark_labels = [],\n",
        "         path_to_save=f'{PLOTS_DIR}/optimal_L_F.pdf')\n",
        "\n",
        "plot_L_F(res, [2e9, 5e9, 10e9],\n",
        "         sizes_labels = [\"2B\", \"5B\", \"10B\"],\n",
        "         Fs = np.logspace(19, 23, 100),\n",
        "         n_kv_tokens=32768, n_inference_tokens=0,\n",
        "         landmark_budgets = [],\n",
        "         landmark_labels = [],\n",
        "         path_to_save=f'{PLOTS_DIR}/optimal_L_F_32k_kvcache.pdf')\n",
        "\n",
        "# plot_L_F(res, [8e9, 70e9, 405e9],\n",
        "#          sizes_labels = [\"2B\", \"5B\", \"10B\"],\n",
        "#          Fs = np.logspace(21, 26, 100),\n",
        "#          n_kv_tokens=0, n_inference_tokens=0,\n",
        "#          landmark_budgets = [llama8budget, llama70budget, llama405budget],\n",
        "#          landmark_labels = ['Llama 8B', \"Llama 70B\", \"Llama 405B\"],\n",
        "#          path_to_save=f'{PLOTS_DIR}/optimal_L_F_llamas.pdf')\n",
        "\n",
        "\n",
        "save_to_zip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P07XyVq0zk-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "\n",
        "def run(table, ids, line1_ids, line2_ids,\n",
        "        compute_optimal, path_to_save=None, n_inference_tokens=0,\n",
        "        yaxis_range=None, xaxis_range=None\n",
        "):\n",
        "    # Filter & compute flops\n",
        "    table = table[table['sys/id'].isin(ids)].copy()\n",
        "    table['flops'] = 6 * table['tokens'] * table['active_with_embed'] + 2 * n_inference_tokens * table['active_with_embed']\n",
        "\n",
        "    # Check columns\n",
        "    req_cols = [\"args/expansion_rate\", \"flops\", \"final_eval_capacity_factor_1.5\"]\n",
        "    if not all(c in table.columns for c in req_cols):\n",
        "        raise ValueError(f\"DataFrame must contain columns: {req_cols}\")\n",
        "\n",
        "    # Lines to intersect\n",
        "    line1 = table[table['sys/id'].isin(line1_ids)].sort_values('flops')\n",
        "    line2 = table[table['sys/id'].isin(line2_ids)].sort_values('flops')\n",
        "    if len(line1) < 2 or len(line2) < 2:\n",
        "        raise ValueError(\"Each line must have at least two points.\")\n",
        "\n",
        "    m1, b1 = np.polyfit(line1['flops'], line1['final_eval_capacity_factor_1.5'], 1)\n",
        "    m2, b2 = np.polyfit(line2['flops'], line2['final_eval_capacity_factor_1.5'], 1)\n",
        "    intersection_flops = (b2 - b1) / (m1 - m2) if m1 != m2 else None\n",
        "\n",
        "    # Plotly Express line plot\n",
        "    df_sorted = table.sort_values(['args/expansion_rate','flops'])\n",
        "    df_sorted['Expansion Rate'] = df_sorted['args/expansion_rate']\n",
        "    fig = px.line(\n",
        "        df_sorted,\n",
        "        x='flops',\n",
        "        y='final_eval_capacity_factor_1.5',\n",
        "        color='Expansion Rate',\n",
        "        markers=True,\n",
        "        title=\"FLOPs vs Final Eval Capacity Factor (Colored by Expansion Rate)\",\n",
        "        template='plotly_white',\n",
        "    )\n",
        "    fig.update_traces(\n",
        "       marker={\n",
        "           'size': 20\n",
        "       }\n",
        "    )\n",
        "\n",
        "    # Vertical line at compute_optimal\n",
        "    fig.add_vline(\n",
        "        x=compute_optimal,\n",
        "        line_dash='dash',\n",
        "        line_color='red',\n",
        "        # annotation_text='E=1 Compute Optimal'\n",
        "    )\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=[compute_optimal, compute_optimal],\n",
        "        y=yaxis_range,  # Use your y-range\n",
        "        mode='lines',\n",
        "        line=dict(dash='dash', color='red'),\n",
        "        name='E=1 Compute Optimal',  # Legend label\n",
        "        showlegend=True,\n",
        "    ))\n",
        "    # Intersection line + annotation\n",
        "    if intersection_flops and df_sorted['flops'].min() <= intersection_flops <= df_sorted['flops'].max():\n",
        "        fig.add_vline(x=intersection_flops, line_dash='dash', line_color='green')\n",
        "        # Shading if intersection > compute_optimal\n",
        "        if intersection_flops > compute_optimal:\n",
        "\n",
        "            x0, x1 = compute_optimal, intersection_flops\n",
        "            y0, y1 = -1, 10\n",
        "\n",
        "\n",
        "            fig.add_trace(go.Bar(\n",
        "                x=[(x0 + x1) / 2],          # midpoint\n",
        "                y=[y1 - y0],\n",
        "                width=(x1 - x0),           # span from x0 to x1\n",
        "                name=\"E=2 Memory Optimal\",\n",
        "                marker=dict(\n",
        "                    color=\"rgba(0,0,0,0)\",             # Fully transparent bar\n",
        "                    pattern_shape=\"/\",  # e.g. '/', '\\\\', 'x', '.', '|-', 'brick', etc.\n",
        "                    pattern_fgcolor=\"rgba(0,0,0,0.4)\",\n",
        "                    pattern_solidity=0.2,\n",
        "                    pattern_size=20\n",
        "                ),\n",
        "                showlegend=False\n",
        "            ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis_title='FLOPs',\n",
        "        yaxis_title='Eval Loss',\n",
        "        autosize=False,\n",
        "        width=1500,\n",
        "        height=1000,\n",
        "        yaxis_range=yaxis_range,\n",
        "        xaxis_range=xaxis_range,\n",
        "        font={\n",
        "            'size': 30\n",
        "        }\n",
        "    )\n",
        "    fig.update_layout(legend=dict(\n",
        "        yanchor=\"top\",\n",
        "        y=0.99,\n",
        "        xanchor=\"left\",\n",
        "        x=0.75\n",
        "    ))\n",
        "    if path_to_save:\n",
        "        fig.write_image(path_to_save)\n",
        "    fig.show()\n",
        "\n",
        "ids = [\n",
        "        \"LLMRANDOM-25496\", \"LLMRANDOM-25497\", \"LLMRANDOM-29053\", \"LLMRANDOM-29055\", \"LLMRANDOM-25498\", \"LLMRANDOM-25499\", #E=1\n",
        "        \"LLMRANDOM-28389\", \"LLMRANDOM-28390\", \"LLMRANDOM-29056\", \"LLMRANDOM-29058\", \"LLMRANDOM-28391\", \"LLMRANDOM-28392\", #E=2\n",
        "        # \"LLMRANDOM-28428\", \"LLMRANDOM-28429\", \"LLMRANDOM-28429\", \"LLMRANDOM-28430\", #E=4\n",
        "    ]\n",
        "line1_ids = [\"LLMRANDOM-29055\", \"LLMRANDOM-25498\"]\n",
        "line2_ids = [\"LLMRANDOM-29058\", \"LLMRANDOM-28391\"]\n",
        "\n",
        "run(\n",
        "    table.copy(), ids, line1_ids, line2_ids, 0.53e20,\n",
        "    n_inference_tokens=0, path_to_save=f'{PLOTS_DIR}/real_life_optimal_1.3B.pdf',\n",
        "    yaxis_range=(2.5, 2.75)\n",
        ")\n",
        "\n",
        "ids = [\n",
        "        \"LLMRANDOM-23623\", \"LLMRANDOM-23624\", \"LLMRANDOM-23625\", \"LLMRANDOM-23626\", #E=1\n",
        "        \"LLMRANDOM-28468\", \"LLMRANDOM-28469\", \"LLMRANDOM-28470\", \"LLMRANDOM-28471\", #E=2\n",
        "        # \"LLMRANDOM-28428\", \"LLMRANDOM-28429\", \"LLMRANDOM-28429\", \"LLMRANDOM-28430\", #E=4\n",
        "    ]\n",
        "line1_ids = [\"LLMRANDOM-23624\", \"LLMRANDOM-23625\"]\n",
        "line2_ids = [\"LLMRANDOM-28469\", \"LLMRANDOM-28470\"]\n",
        "\n",
        "compute_optimal_guess = 6 * (310e6)**2 * 5 # ratio - 7.5\n",
        "run(table.copy(), ids, line1_ids, line2_ids, compute_optimal_guess,\n",
        "    path_to_save=f'{PLOTS_DIR}/370M_comparison.pdf',\n",
        "    yaxis_range=(2.9, 3.3)\n",
        ")\n",
        "\n",
        "save_to_zip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExY7eCYgq71j"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def plot_L_F_plotly(res, Ns, sizes_labels, Fs, n_kv_tokens, n_inference_tokens=None,\n",
        "                    inference_cost_times=None, landmark_budgets=None, landmark_labels=None,\n",
        "                    path_to_save=None, Es=[1,2,4,8,16,32]):\n",
        "    fig = go.Figure()\n",
        "\n",
        "    if not isinstance(n_kv_tokens, list):\n",
        "        n_kv_tokens = [n_kv_tokens]*len(Ns)\n",
        "\n",
        "    Ls = [f'L_{e}' for e in Es]\n",
        "    colors = [\"blue\",\"red\",\"green\",\"orange\",\"purple\",\"brown\"]\n",
        "    for idx, (N, kv, size_label) in enumerate(zip(Ns, n_kv_tokens, sizes_labels)):\n",
        "        rows = []\n",
        "        actual_N = N + get_Nkv_per_token_from_N_total_with_embeddings(N, DV, 1)*kv\n",
        "        for fval in Fs:\n",
        "            row = {'F': fval}\n",
        "            for e in Es:\n",
        "                candN, candD = [], []\n",
        "                for cN in np.logspace(np.log10(actual_N)/4, np.log10(actual_N), 100):\n",
        "                    dm = get_dm_from_N_total_with_embeddings_with_kv(cN, DV, e, kv)\n",
        "                    active = (13/64)*(dm**3) + 2*dm*DV\n",
        "                    if inference_cost_times:\n",
        "                        train_budget = fval/(1+inference_cost_times)\n",
        "                    elif n_inference_tokens:\n",
        "                        train_budget = fval - 2*n_inference_tokens*active\n",
        "                    else:\n",
        "                        train_budget = fval\n",
        "                    D = train_budget/(6*active)\n",
        "                    candN.append(active)\n",
        "                    candD.append(D)\n",
        "                cL = predict_loss_joined(N=torch.tensor(candN),\n",
        "                                         D=torch.tensor(candD),\n",
        "                                         E=torch.tensor([e]*len(candN)), **res)\n",
        "                i_min = np.argmin(cL)\n",
        "                row[f'L_{e}'] = cL[i_min].item()\n",
        "                row[f'D_{e}'] = candD[i_min]\n",
        "                row[f'active_{e}'] = candN[i_min]\n",
        "            rows.append(row)\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "        df['optimal'] = df[Ls].idxmin(axis=1)\n",
        "        df['min_loss'] = df[Ls].min(axis=1)\n",
        "\n",
        "        for c, e in zip(colors, Es):\n",
        "            mask = df['optimal'] == f'L_{e}'\n",
        "            fig.add_trace(go.Scatter(x=df[mask]['F'],\n",
        "                                     y=df[mask]['min_loss'],\n",
        "                                     mode='lines',\n",
        "                                     line=dict(color=c),\n",
        "                                     name=f\"N={N:.0e}, e={e}\"))\n",
        "            if mask.any():\n",
        "                lastF = df[mask]['F'].iloc[-1]\n",
        "                lastL = df[mask]['min_loss'].iloc[-1]\n",
        "                fig.add_annotation(x=lastF, y=lastL, text=str(e), showarrow=False, font=dict(color=c))\n",
        "\n",
        "        fig.add_annotation(x=Fs.max()*0.6, y=1.1*df['min_loss'].min(),\n",
        "                           text=size_label, showarrow=False, font=dict(size=16))\n",
        "\n",
        "    fig.update_layout(xaxis_type=\"log\", yaxis_type=\"log\",\n",
        "                      xaxis_title=\"F\", yaxis_title=\"L - c\",\n",
        "                      title=\"Optimal Loss by Expansion Rate\")\n",
        "\n",
        "    if landmark_budgets and landmark_labels:\n",
        "        for lb, lbl in zip(landmark_budgets, landmark_labels):\n",
        "            fig.add_vline(x=lb, line_width=2, line_dash=\"dash\", annotation_text=lbl, annotation_position=\"bottom\")\n",
        "\n",
        "\n",
        "    if path_to_save:\n",
        "        fig.write_image(path_to_save)\n",
        "    fig.show()\n",
        "\n",
        "# Example usage\n",
        "plot_L_F_plotly(res, [2e9,5e9,1e10],\n",
        "                sizes_labels=[\"2B\",\"5B\",\"10B\"],\n",
        "                Fs=np.logspace(19, 23, 100),\n",
        "                n_kv_tokens=0, n_inference_tokens=0,\n",
        "                landmark_budgets=[], landmark_labels=[],\n",
        "                path_to_save=f'{PLOTS_DIR}/optimal_L_F_plotly.pdf')\n",
        "\n",
        "save_to_zip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ4iO_PatUGK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.ticker as mticker\n",
        "\n",
        "def custom_formatter(x, _):\n",
        "    if x >= 1e12:\n",
        "        return '{:.0f}T'.format(x * 1e-12)\n",
        "    elif x >= 1e9:\n",
        "        return '{:.0f}B'.format(x * 1e-9)\n",
        "    elif x >= 1e6:\n",
        "        return '{:.0f}M'.format(x * 1e-6)\n",
        "    else:\n",
        "        return '{:.0f}'.format(x)\n",
        "\n",
        "ml_formatter = mticker.FuncFormatter(custom_formatter)\n",
        "TITLE_SIZE = 16\n",
        "LABEL_SIZE = 14\n",
        "# Constants stored in a dictionary\n",
        "# params = {\n",
        "#     \"c\": 2.4,\n",
        "#     \"a\": 803.0,\n",
        "#     \"alpha\": -0.34,\n",
        "#     \"delta\": 4.80,\n",
        "#     \"gamma\": -0.29,\n",
        "#     \"b\": 519.0,\n",
        "#     \"beta\": -0.30,\n",
        "#     \"omega\": 4.31,\n",
        "#     \"zeta\": -0.19,\n",
        "#     \"E_start\": 2.6527,\n",
        "#     \"E_max\": 4.9748,\n",
        "#     \"phi\": 0.68\n",
        "# }\n",
        "\n",
        "#params = {k: np.float64(v) for k, v in params.items()}\n",
        "\n",
        "# Function for E_prime\n",
        "# def get_E_prime(E, params):\n",
        "#     indicator = 1.0 * (E == 1)\n",
        "#     term2 = indicator * params[\"phi\"]\n",
        "#     term3 = 1 / (1/params[\"E_start\"] - 1/params[\"E_max\"])\n",
        "#     term4 = params[\"E_max\"]\n",
        "#     return 1/(1 / (E + term2 - 1 + term3) + 1 / term4)\n",
        "\n",
        "# Function for L\n",
        "# def get_L(N_act, D, E_prime, params):\n",
        "#     term1 = params[\"a\"] * (E_prime ** params[\"delta\"]) * (N_act ** (params[\"alpha\"] + params[\"gamma\"] * np.log(E_prime)))\n",
        "#     term2 = params[\"b\"] * (E_prime ** params[\"omega\"]) * (D ** (params[\"beta\"] + params[\"zeta\"] * np.log(E_prime)))\n",
        "#     return term1 + term2 + params[\"c\"]\n",
        "\n",
        "\n",
        "def plot_L_limit_vs_D(params, D_range, e_values, path_to_save = None):\n",
        "    data = []\n",
        "    ax = plt.figure(figsize=(10, 6)).gca()\n",
        "\n",
        "\n",
        "    for e, color in zip(e_values, colors):\n",
        "        E_prime = get_E_prime(e, params)\n",
        "        L_limit = params[\"b\"] * (E_prime ** params[\"omega\"]) * (D_range ** (params[\"beta\"] + params[\"zeta\"] * np.log(E_prime)))\n",
        "\n",
        "        sns.lineplot(data=pd.DataFrame({'D':D_range, 'L': L_limit }), x='D', y='L', label=str(e), color=color)\n",
        "\n",
        "    # Plot\n",
        "    # sns.lineplot(data=df, x=\"D\", y=\"L_limit\", hue=\"e\")\n",
        "    plt.xscale(\"log\")  # Using log scale for D if range is large\n",
        "    plt.yscale(\"log\")  # Optionally, if L values also span large range\n",
        "    plt.title(r\"$L(\\infty, D, E)$\", fontsize=TITLE_SIZE)\n",
        "    plt.xlabel(\"$D$\", fontsize=LABEL_SIZE)\n",
        "    plt.ylabel(\"$L - c$\", fontsize=LABEL_SIZE)\n",
        "    plt.legend(fontsize=LABEL_SIZE)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=LABEL_SIZE)  # Adjust 'labelsize' as needed\n",
        "    # Set the x-axis formatter\n",
        "    ax.xaxis.set_major_formatter(ml_formatter)\n",
        "\n",
        "\n",
        "    if path_to_save:\n",
        "        plt.savefig(path_to_save, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_L_limit_vs_N(params, N_range, e_values, path_to_save = None):\n",
        "    data = []\n",
        "    ax = plt.figure(figsize=(10, 6)).gca()\n",
        "\n",
        "\n",
        "    for e, color in zip(e_values, colors):\n",
        "        N_act = np.array([get_Nact_from_N(n, DV, e) for n in N_range])\n",
        "        E_prime = get_E_prime(e, params)\n",
        "        L_limit = params[\"a\"] * (E_prime ** params[\"delta\"]) * (N_act ** (params[\"alpha\"] + params[\"gamma\"] * np.log(E_prime)))\n",
        "\n",
        "        sns.lineplot(data=pd.DataFrame({'N':N_range, 'L': L_limit }), x='N', y='L', label=str(e), color=color)\n",
        "\n",
        "\n",
        "    # Plot\n",
        "    # sns.lineplot(data=df, x=\"D\", y=\"L_limit\", hue=\"e\")\n",
        "    plt.xscale(\"log\")  # Using log scale for D if range is large\n",
        "    plt.yscale(\"log\")  # Optionally, if L values also span large range\n",
        "    plt.title(r\"$L(N_{total}, \\infty, E)$\", fontsize=TITLE_SIZE)\n",
        "    plt.xlabel(\"$N$\", fontsize=LABEL_SIZE)\n",
        "    plt.ylabel(\"$L - c$\", fontsize=LABEL_SIZE)\n",
        "    plt.legend(fontsize=LABEL_SIZE)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=LABEL_SIZE)  # Adjust 'labelsize' as needed\n",
        "\n",
        "    # Set the x-axis formatter\n",
        "    ax.xaxis.set_major_formatter(ml_formatter)\n",
        "\n",
        "\n",
        "    if path_to_save:\n",
        "        plt.savefig(path_to_save, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_L_limit_vs_N_act(params, N_act_range, e_values, path_to_save = None):\n",
        "    data = []\n",
        "    ax = plt.figure(figsize=(10, 6)).gca()\n",
        "\n",
        "\n",
        "    for e, color in zip(e_values, colors):\n",
        "        E_prime = get_E_prime(e, params)\n",
        "        L_limit = params[\"a\"] * (E_prime ** params[\"delta\"]) * (N_range ** (params[\"alpha\"] + params[\"gamma\"] * np.log(E_prime)))\n",
        "\n",
        "        sns.lineplot(data=pd.DataFrame({'N':N_range, 'L': L_limit }), x='N', y='L', label=str(e), color=color)\n",
        "\n",
        "    # Plot\n",
        "    plt.xscale(\"log\")  # Using log scale for D if range is large\n",
        "    plt.yscale(\"log\")  # Optionally, if L values also span large range\n",
        "    plt.title(r\"$L(N_{act}, \\infty, E)$\", fontsize=TITLE_SIZE)\n",
        "    plt.xlabel(\"$N_{act}$\", fontsize=LABEL_SIZE)\n",
        "    plt.ylabel(\"$L - c$\", fontsize=LABEL_SIZE)\n",
        "    plt.legend(fontsize=LABEL_SIZE)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=LABEL_SIZE)  # Adjust 'labelsize' as needed\n",
        "\n",
        "\n",
        "    # Set the x-axis formatter\n",
        "    ax.xaxis.set_major_formatter(ml_formatter)\n",
        "\n",
        "\n",
        "    if path_to_save:\n",
        "        plt.savefig(path_to_save, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "D_range = np.logspace(9, 12, num=100)\n",
        "N_range = np.logspace(7, 11, num=100)\n",
        "e_values = [1, 2, 4, 8, 16, 32]  # Example e values\n",
        "\n",
        "#plot_L_limit_vs_D(params, D_range, e_values, f\"{PLOTS_DIR}/l_of_inf_params.pdf\")\n",
        "#plot_L_limit_vs_N(params, N_range, e_values, f\"{PLOTS_DIR}/l_of_inf_data_total.pdf\")\n",
        "#plot_L_limit_vs_N_act(params, N_range, e_values, f\"{PLOTS_DIR}/l_of_inf_data_active.pdf\")\n",
        "\n",
        "#save_to_zip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmODwnNZOKE4"
      },
      "outputs": [],
      "source": [
        "def plot_memory_flops_optimal_E(res, memory_constraints, flops_budgets,\n",
        "                                n_kv_tokens, n_inference_tokens, Es=[1, 2, 4, 8, 16, 32],\n",
        "                                subtract_dataset_entropy=False, color_map='viridis'):\n",
        "    \"\"\"\n",
        "    Plots a 2D graph with memory constraints on the x-axis, FLOPs budget on the y-axis,\n",
        "    and colors each point based on the optimal expansion rate E.\n",
        "\n",
        "    Parameters:\n",
        "    - res: Dictionary of parameters required for predict_loss_joined.\n",
        "    - memory_constraints: Array-like, memory constraints to iterate over.\n",
        "    - flops_budgets: Array-like, FLOPs budgets to iterate over.\n",
        "    - n_kv_tokens: Integer, number of KV tokens.\n",
        "    - n_inference_tokens: Integer, number of inference tokens.\n",
        "    - Es: List of expansion rates to consider.\n",
        "    - subtract_dataset_entropy: Boolean, whether to subtract dataset entropy.\n",
        "    - color_map: String, matplotlib colormap name for E.\n",
        "\n",
        "    Returns:\n",
        "    - None (displays the plot)\n",
        "    \"\"\"\n",
        "    # Create a meshgrid for memory and FLOPs\n",
        "    memory_grid, flops_grid = np.meshgrid(memory_constraints, flops_budgets)\n",
        "    memory_grid_flat = memory_grid.flatten()\n",
        "    flops_grid_flat = flops_grid.flatten()\n",
        "\n",
        "    # Prepare a DataFrame to store optimal E for each point\n",
        "    optimal_E = np.zeros(\n",
        "        [len(memory_constraints), len(flops_budgets)]\n",
        "    )\n",
        "\n",
        "    for row_idx in range(len(memory_constraints)):\n",
        "        for col_idx in range(len(flops_budgets)):\n",
        "            mem = memory_constraints[row_idx]\n",
        "            flops = flops_budgets[col_idx]\n",
        "            E_list = []\n",
        "            N_list = []\n",
        "            D_list = []\n",
        "            train_budget_list = []\n",
        "            for E in Es:\n",
        "                for model_size in np.logspace(1, np.log10(mem), 10):\n",
        "                    dm = get_dm_from_N_total_with_embeddings_with_kv(model_size, DV, E, n_kv_tokens)\n",
        "                    active_N_nonembed = (13 / 64) * dm**3\n",
        "                    embed_size = 2 * dm * DV\n",
        "                    active_N_with_embed = active_N_nonembed + embed_size\n",
        "                    inference_cost = 2 * n_inference_tokens * active_N_with_embed\n",
        "                    train_budget = flops - inference_cost\n",
        "                    E_list.append(E)\n",
        "                    N_list.append(active_N_nonembed)\n",
        "                    D_list.append(train_budget / (6 * active_N_with_embed))\n",
        "                    train_budget_list.append(train_budget)\n",
        "            losses = predict_loss_joined(\n",
        "                N=torch.tensor(N_list),\n",
        "                D=torch.tensor(D_list),\n",
        "                E=torch.tensor(E_list),\n",
        "                **res\n",
        "            )\n",
        "            df = pd.DataFrame({\n",
        "                'E': E_list,\n",
        "                'N': N_list,\n",
        "                'D': D_list,\n",
        "                'train_budget': train_budget_list,\n",
        "                'loss': losses\n",
        "            })\n",
        "            valid_df = df[(df['train_budget'] > 0) & (df['N'] > 0)]\n",
        "            # reset index\n",
        "            valid_df = valid_df.reset_index(drop=True)\n",
        "            if len(valid_df) == 0:\n",
        "                optimal_E[row_idx, col_idx] = -1\n",
        "            else:\n",
        "                # idxmin =\n",
        "                # print(idxmin)\n",
        "                # print(valid_df)\n",
        "                optimal_E[row_idx, col_idx] = valid_df['E'][valid_df['loss'].idxmin()]\n",
        "\n",
        "            #     # Compute dm based on current memory and E\n",
        "            #     dm = get_dm_from_N_total_with_embeddings_with_kv(mem, DV, E, n_kv_tokens)\n",
        "            #     active_N_nonembed = (13 / 64) * dm**3\n",
        "            #     embed_size = 2 * dm * DV\n",
        "            #     active_N_with_embed = active_N_nonembed + embed_size\n",
        "            #     inference_cost = 2 * n_inference_tokens * active_N_with_embed\n",
        "            #     train_budget = flops - inference_cost\n",
        "            #     if train_budget <= 0:\n",
        "            #         loss = float('inf')\n",
        "            #     else:\n",
        "            #         D = train_budget / (6 * active_N_with_embed)\n",
        "            #         loss = predict_loss_joined(\n",
        "            #             N=torch.tensor([active_N_nonembed]),\n",
        "            #             D=torch.tensor([D]),\n",
        "            #             E=torch.tensor([E]),\n",
        "            #             **res\n",
        "            #         ).item()\n",
        "            #     losses.append(loss)\n",
        "            # min_loss = min(losses)\n",
        "            # if min_loss == float('inf'):\n",
        "            #     optimal_E[row_idx, col_idx] = -1\n",
        "            # else:\n",
        "            #     optimal_E[row_idx, col_idx] = Es[np.argmin(losses)]\n",
        "\n",
        "    print(optimal_E)\n",
        "    extent = [flops_budgets.min(), flops_budgets.max(), memory_constraints.min(), memory_constraints.max()]\n",
        "    optimal_E = np.flip(optimal_E, 0)\n",
        "    extent = [np.log10(num) for num in extent]\n",
        "    print(extent)\n",
        "    plt.imshow(optimal_E,\n",
        "               cmap=color_map,\n",
        "               extent=extent\n",
        "    )\n",
        "    # plt.colorbar(\n",
        "\n",
        "    # Iterate over each (memory, flops) pair\n",
        "    # for mem, flops in zip(memory_grid_flat, flops_grid_flat):\n",
        "    #     losses = []\n",
        "    #     for E in Es:\n",
        "    #         # Compute dm based on current memory and E\n",
        "    #         dm = get_dm_from_N_total_with_embeddings_with_kv(mem, DV, E, n_kv_tokens)\n",
        "    #         # dm = get_dm_from_N_total_with_embeddings_with_kv(\n",
        "    #         #     total_N=None,  # Not used in memory-flops plot\n",
        "    #         #     DV=DV,\n",
        "    #         #     E=E,\n",
        "    #         #     n_kv_tokens=n_kv_tokens,\n",
        "    #         #     memory=mem  # Assuming you modify the function to accept memory\n",
        "    #         # )\n",
        "\n",
        "    #         # Compute active_N_nonembed and active_N_with_embed\n",
        "    #         active_N_nonembed = (13 / 64) * dm**3\n",
        "    #         embed_size = 2 * dm * DV\n",
        "    #         active_N_with_embed = active_N_nonembed + embed_size\n",
        "\n",
        "    #         # Compute inference_cost and train_budget based on flops\n",
        "    #         inference_cost = 2 * n_inference_tokens * active_N_with_embed\n",
        "    #         train_budget = flops - inference_cost\n",
        "\n",
        "    #         if train_budget <= 0:\n",
        "    #             loss = float('inf')\n",
        "    #         else:\n",
        "    #             D = train_budget / (6 * active_N_with_embed)\n",
        "    #             # Compute loss using predict_loss_joined\n",
        "    #             loss_tensor = predict_loss_joined(\n",
        "    #                 N=torch.tensor([active_N_nonembed]),\n",
        "    #                 D=torch.tensor([D]),\n",
        "    #                 E=torch.tensor([E]),\n",
        "    #                 **res\n",
        "    #             )\n",
        "    #             loss = loss_tensor.item()\n",
        "    #             if subtract_dataset_entropy:\n",
        "    #                 loss -= res.get('C', 0)\n",
        "    #         losses.append(loss)\n",
        "\n",
        "    #     # Determine the optimal E (with minimum loss)\n",
        "    #     min_loss = min(losses)\n",
        "    #     if min_loss == float('inf'):\n",
        "    #         optimal_E.append(None)  # No valid E\n",
        "    #     else:\n",
        "    #         optimal_E.append(Es[np.argmin(losses)])\n",
        "    # print(optimal_E)\n",
        "    # Create DataFrame for plotting\n",
        "    # df_plot = pd.DataFrame({\n",
        "    #     'Memory_Constraint': memory_grid_flat,\n",
        "    #     'FLOPs_Budget': flops_grid_flat,\n",
        "    #     'Optimal_E': optimal_E\n",
        "    # })\n",
        "\n",
        "    # Remove points with no valid E\n",
        "    # df_plot = df_plot.dropna(subset=['Optimal_E'])\n",
        "\n",
        "    # Plotting\n",
        "    # plt.figure(figsize=(12, 8))\n",
        "    # scatter = plt.scatter(\n",
        "    #     df_plot['Memory_Constraint'],\n",
        "    #     df_plot['FLOPs_Budget'],\n",
        "    #     c=df_plot['Optimal_E'],\n",
        "    #     cmap=color_map,\n",
        "    #     marker='s',\n",
        "    #     s=50,\n",
        "    #     edgecolor='k',\n",
        "    #     alpha=0.7\n",
        "    # )\n",
        "    # plt.colorbar(scatter, label='Optimal E')\n",
        "    # plt.xscale('log')\n",
        "    # plt.yscale('log')\n",
        "    # plt.xlabel('Memory Constraint')\n",
        "    # plt.ylabel('FLOPs Budget')\n",
        "    # plt.title('Optimal Expansion Rate (E) Based on Memory and FLOPs Constraints')\n",
        "    # plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "resolution = 50\n",
        "memory_constraints = np.logspace(9, 11, num=resolution)  # Example: 1MB to 1TB\n",
        "flops_budgets = np.logspace(19, 22, num=resolution)      # Example: 1 GFLOPs to 1 EFLOPs\n",
        "# memory_grid, flops_grid = np.meshgrid(memory_constraints, flops_budgets)\n",
        "# for m in range(memory_constraints.shape[0]):\n",
        "#     for f in range(memory_constraints.shape[0]):\n",
        "# print(memory_grid.shape)\n",
        "\n",
        "n_kv_tokens = 32000\n",
        "# n_inference_tokens = 1000\n",
        "# n_kv_tokens = 0\n",
        "n_inference_tokens = 0\n",
        "Es = [1, 2, 4, 8, 16, 32]\n",
        "# colors = sns.color_palette(\"viridis\", len(Es))  # Define colors if needed\n",
        "\n",
        "# Call the plotting function\n",
        "plot_memory_flops_optimal_E(\n",
        "    res=res,\n",
        "    memory_constraints=memory_constraints,\n",
        "    flops_budgets=flops_budgets,\n",
        "    n_kv_tokens=n_kv_tokens,\n",
        "    n_inference_tokens=n_inference_tokens,\n",
        "    Es=Es,\n",
        "    subtract_dataset_entropy=True,\n",
        "    color_map='viridis'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQxm74CPNmn8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def format_ml_number(num: float):\n",
        "    if 1e6 <= num < 1e9:\n",
        "        return f\"{num / 1e6:.0f}M\"\n",
        "    elif 1e9 <= num < 1e12:\n",
        "        return f\"{num / 1e9:.0f}B\"\n",
        "    elif 1e12 <= num < 1e15:\n",
        "        return f\"{num / 1e12:.0f}T\"\n",
        "\n",
        "\n",
        "def generate_latex_table(res, flops_constraints, memory_constraints,\n",
        "                         use_active, use_embeddings,\n",
        "                         n_kv_tokens, n_inference_tokens, subtract_dataset_entropy,\n",
        "                         memory_constraints_labels=None, ending_mode='cut',\n",
        "                         Es = [1, 2, 4, 8, 16, 32]):\n",
        "    # Prepare the table data as a dictionary\n",
        "    table_data = {}\n",
        "\n",
        "    # Loop through each memory constraint (row)\n",
        "    for memory in memory_constraints:\n",
        "        row_data = []\n",
        "\n",
        "        # Loop through each FLOPs constraint (column)\n",
        "        for flops in flops_constraints:\n",
        "            optimal_E = None\n",
        "            optimal_dm = None\n",
        "            optimal_L = float('inf')\n",
        "\n",
        "            candidate_Ns = []\n",
        "            candidate_Ds = []\n",
        "            candidate_Es = []\n",
        "\n",
        "            # Find optimal E and L for the current combination\n",
        "\n",
        "            for exp_rate in Es:\n",
        "                for candidate_dm in range(1, 100000):\n",
        "                    # N_total = candidate_dm**3 * (1+3*exp_rate)/16 + candidate_dm**2 * n_kv_tokens / 32 + 2*candidate_dm * DV\n",
        "                    N_total =  candidate_dm**3 * (4 + 9 * exp_rate)/ 64 + candidate_dm**2 * n_kv_tokens / 32 + 2 * candidate_dm * DV\n",
        "                    active_N_with_embeddings = candidate_dm**3 * 13 / 64 + 2*candidate_dm * DV\n",
        "                    if N_total > memory:\n",
        "                        # print(N, candidate_dm)\n",
        "                        break\n",
        "                    inference_cost = 2 * n_inference_tokens * active_N_with_embeddings\n",
        "                    train_budget = flops - inference_cost\n",
        "                    D = train_budget / (6 * active_N_with_embeddings)\n",
        "\n",
        "                    candidate_Ns.append(active_N_with_embeddings)\n",
        "                    candidate_Ds.append(D)\n",
        "                    candidate_Es.append(exp_rate)\n",
        "                    # if L < optimal_L:\n",
        "                    #     optimal_L = L\n",
        "                    #     optimal_E = exp_rate\n",
        "                    #     optimal_dm = candidate_dm\n",
        "                else:\n",
        "                    raise ValueError(\"broaden the candidate_dm search\")\n",
        "\n",
        "            L = predict_loss_joined(N=torch.tensor(candidate_Ns), D=torch.tensor(candidate_Ds), E=torch.tensor(candidate_Es), **res)\n",
        "            optimal_E = candidate_Es[np.argmin(L)]\n",
        "\n",
        "                # L = predict_loss_joined(N=torch.tensor(df[f'active_N_{e}']), D=torch.tensor(df[f'D_{e}']), E=torch.tensor([e] * len(df)), **res)\n",
        "                # if subtract_dataset_entropy:\n",
        "                #     L -= res['C']\n",
        "\n",
        "            # Save the optimal loss for the current combination in the row\n",
        "            # row_data.append((f\"{optimal_L:.2f}\", optimal_E, optimal_dm))\n",
        "            row_data.append(f\"\\\\textcolor{{e{optimal_E}}}{{{optimal_E}}}\")\n",
        "\n",
        "        # Add the row to the table with memory as the index\n",
        "        table_data[memory] = row_data\n",
        "\n",
        "    # print(table_data)\n",
        "    # Convert to DataFrame with flops as columns and memory as index\n",
        "    df = pd.DataFrame(table_data,\n",
        "                      index=[f'{f:.0e}' for f in flops_constraints],\n",
        "                      )\n",
        "    # print(df)\n",
        "    if memory_constraints_labels is None:\n",
        "        df.columns = [format_ml_number(m) for m in memory_constraints]\n",
        "    else:\n",
        "        df.columns = memory_constraints_labels\n",
        "    # df.columns = [format_ml_number(m) for m in memory_constraints]\n",
        "    print(df)\n",
        "\n",
        "    # Generate LaTeX table\n",
        "    latex_table = df.to_latex(index=True, float_format=\"%.0e\", column_format=\"c\" + \"c\" * len(memory_constraints))\n",
        "    latex_table = latex_table.replace('{32}', r'{$\\geq 32$}')\n",
        "\n",
        "    return latex_table\n",
        "\n",
        "\n",
        "# generate_latex_table(data, flops_constraints=[1e20, 1e23, 1e26], memory_constraints=[24e9, 40e9, 640e9],\n",
        "#                      use_active=True, use_embeddings=True,\n",
        "#                      n_kv_tokens=0, n_inference_tokens=0,\n",
        "#                      subtract_dataset_entropy=True, ending_mode='cut')\n",
        "\n",
        "generate_latex_table(res, flops_constraints=[1e21, 1e22, 1e23, 1e24, 1e25], memory_constraints=[12e9, 40e9, 320e9],\n",
        "                     memory_constraints_labels=['24GB', '80GB', '640GB'],\n",
        "                     use_active=True, use_embeddings=True,\n",
        "                     n_kv_tokens=2**14, n_inference_tokens=0,\n",
        "                     # n_inference_tokens=np.float64(15e13),\n",
        "                     subtract_dataset_entropy=True, ending_mode='cut')\n",
        "generate_latex_table(res, flops_constraints=[1e21, 1e22, 1e23, 1e24, 1e25], memory_constraints=[12e9, 40e9, 320e9],\n",
        "                     memory_constraints_labels=['24GB', '80GB', '640GB'],\n",
        "                     use_active=True, use_embeddings=True,\n",
        "                     n_kv_tokens=2**16, n_inference_tokens=0,\n",
        "                     # n_inference_tokens=np.float64(15e13),\n",
        "                     subtract_dataset_entropy=True, ending_mode='cut')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzmCdl1Nbls5"
      },
      "outputs": [],
      "source": [
        "# def custom_formatter(x, _):\n",
        "#     if x >= 1e12:\n",
        "#         return '{:.0f}T'.format(x * 1e-12)\n",
        "#     elif x >= 1e9:\n",
        "#         return '{:.0f}B'.format(x * 1e-9)\n",
        "#     elif x >= 1e6:\n",
        "#         return '{:.0f}M'.format(x * 1e-6)\n",
        "#     else:\n",
        "#         return '{:.0f}'.format(x)\n",
        "def custom_formatter(x):\n",
        "    if x >= 1e12:\n",
        "        return '{:.0f}T'.format(x * 1e-12)\n",
        "    elif x >= 1e9:\n",
        "        return '{:.0f}B'.format(x * 1e-9)\n",
        "    elif x >= 1e6:\n",
        "        return '{:.0f}M'.format(x * 1e-6)\n",
        "    else:\n",
        "        return '{:.0f}'.format(x)\n",
        "\n",
        "def make_latex_appendix(table):\n",
        "    # latex_table = table[['args/n_att_heads']]\n",
        "    # latex_table = latex_table.rename(columns={'args/n_att_heads': 'n_att_heads'})\n",
        "    latex_table = pd.DataFrame(\n",
        "        {\n",
        "            'n\\_att\\_heads': table['args/n_att_heads'].astype(int),\n",
        "            'n\\_blocks': table['args/n_blocks'].astype(int),\n",
        "            'dmodel': table['args/dmodel'].astype(int),\n",
        "            'active params': table['active_with_embed'],\n",
        "            'total params': table['total_with_embed'],\n",
        "            'E': table['args/expansion_rate'].astype(int),\n",
        "            'tokens': table['tokens'],\n",
        "            # 'n_kv_tokens': table['args/n_kv_tokens'],\n",
        "            # 'n_inference_tokens': table['args/n_inference_tokens'],\n",
        "        }\n",
        "    )\n",
        "    # latex_table['n_att_heads'] = latex_table['n_att_heads'].astype(int)\n",
        "    latex = latex_table.to_latex(index=False)\n",
        "    latex = latex.replace('tabular', 'longtable')\n",
        "    print(latex)\n",
        "\n",
        "def group_and_add_tokens(table):\n",
        "    # Add tokens to the grouping and aggregate as lists\n",
        "    table = table.copy()\n",
        "    table['tokens'] = table['tokens'].apply(custom_formatter) # Ensure tokens are strings\n",
        "    table['active_with_embed'] = table['active_with_embed'].apply(custom_formatter) # Ensure tokens are strings\n",
        "    table['total_with_embed'] = table['total_with_embed'].apply(custom_formatter) # Ensure tokens are strings\n",
        "    grouped = table.groupby(\n",
        "        ['args/n_att_heads', 'args/n_blocks', 'args/dmodel', 'active_with_embed', 'total_with_embed', 'args/expansion_rate']\n",
        "    )['tokens'].apply(list).reset_index()\n",
        "\n",
        "    return grouped\n",
        "\n",
        "# make_latex_appendix(table.copy())\n",
        "make_latex_appendix(group_and_add_tokens(table.copy()))\n",
        "# table.columns\n",
        "# table['tokens']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdTUD1OkEb4j"
      },
      "outputs": [],
      "source": [
        "Fs = [1e20, 1e21, 1e22, 1e23]  # example FLOPs budgets\n",
        "ratios = []\n",
        "\n",
        "def compute_losses_for_budget(budget, res, n_kv_tokens=0, n_inference_tokens=0):\n",
        "    import numpy as np\n",
        "    budget_exponent = np.log10(budget)\n",
        "    total_N = np.logspace(budget_exponent/2 - 2, budget_exponent/2, num=1000)\n",
        "\n",
        "    # Arrays to hold the losses\n",
        "    L_1 = np.zeros_like(total_N)\n",
        "    L_2 = np.zeros_like(total_N)\n",
        "\n",
        "    # Compute D and loss for E=1, E=2\n",
        "    for i, N_val in enumerate(total_N):\n",
        "        # same logic as in plot_L_N_kvcache_inference_optimal\n",
        "        dm_1 = get_dm_from_N_total_with_embeddings_with_kv(N_val, DV, 1, n_kv_tokens)\n",
        "        active_N_1 = (13/64)*dm_1**3 + 2*dm_1*DV\n",
        "        inference_cost_1 = 2*n_inference_tokens*active_N_1\n",
        "        D_1 = (budget - inference_cost_1)/(6*active_N_1)\n",
        "        L_1[i] = predict_loss_joined(\n",
        "            N=torch.tensor([active_N_1]),\n",
        "            D=torch.tensor([D_1]),\n",
        "            E=torch.tensor([1]),\n",
        "            **res\n",
        "        ).item()\n",
        "\n",
        "        dm_2 = get_dm_from_N_total_with_embeddings_with_kv(N_val, DV, 4, n_kv_tokens)\n",
        "        active_N_2 = (13/64) * dm_2**3 + 2*dm_2*DV\n",
        "        inference_cost_2 = 2*n_inference_tokens*active_N_2\n",
        "        D_2 = (budget - inference_cost_2)/(6*active_N_2)\n",
        "        L_2[i] = predict_loss_joined(\n",
        "            N=torch.tensor([active_N_2]),\n",
        "            D=torch.tensor([D_2]),\n",
        "            E=torch.tensor([4]),\n",
        "            **res\n",
        "        ).item()\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"total_N\": total_N,\n",
        "        \"L_1\": L_1,\n",
        "        \"L_2\": L_2\n",
        "    })\n",
        "\n",
        "for F in Fs:\n",
        "    df = compute_losses_for_budget(F, res=res)\n",
        "    px.line(df, x='total_N', y=['L_1', 'L_2']).show()\n",
        "    # Size at which E=1 is optimal\n",
        "    i_opt_1 = df['L_1'].idxmin()\n",
        "    N_opt_1 = df.loc[i_opt_1, 'total_N']\n",
        "\n",
        "    # Find intersection via sign change in (L_1 - L_2)\n",
        "    diff = df['L_1'] - df['L_2']\n",
        "    # px.line(diff).show()\n",
        "    sign_changes = np.where(np.diff(np.sign(diff)) != 0)[0]\n",
        "    print(f'Total N at sign changes: {df.loc[sign_changes, \"total_N\"]}')\n",
        "    if len(sign_changes) == 0:\n",
        "        # If no crossing, set intersection_N = NaN\n",
        "        intersection_N = np.nan\n",
        "    else:\n",
        "        i0 = sign_changes[0]\n",
        "        x0, x1 = df['total_N'].iloc[i0], df['total_N'].iloc[i0+1]\n",
        "        y0, y1 = diff.iloc[i0], diff.iloc[i0+1]\n",
        "        # Linear interpolation for root\n",
        "        intersection_N = x0 + (x1 - x0)*(-y0)/(y1 - y0)\n",
        "        print(intersection_N)\n",
        "\n",
        "    ratios.append(intersection_N / N_opt_1)\n",
        "\n",
        "plt.plot(Fs, ratios, \"o-\")\n",
        "plt.xscale('log')\n",
        "plt.xlabel(\"FLOPs budget\")\n",
        "plt.ylabel(\"Intersection / Optimal(E=1) size\")\n",
        "plt.title(\"Ratio of E=1/E=2 intersection to E=1 optimal size\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3M_0YMJbSxn"
      },
      "source": [
        "# End"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SEVom8OY77gu",
        "QRSllIAXOlyb",
        "eP4ghZpYIY92",
        "ehPOAkh4pCh4",
        "37kM-mWrpE-c",
        "KFjz6kiMUBk0",
        "x0TlX_WjUNGG",
        "QGM-U4nTlye6",
        "zP8rnDeSIysg",
        "cYnd-g4lLCHF",
        "XnlTDlysIEMb",
        "BZMWftEidden",
        "kMUtWSLNpv4l",
        "xA7PtZbo7tu2",
        "a3lqfBNzIC1q"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llmrandom",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1224d4ca62df4f25a371f72d532b2335": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66245d45d99648b386ade7370d3cf28b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f794b4dbdf3f40caacf0010c7ea80fa3",
            "value": 1
          }
        },
        "2724ebddf2d04c6da3544ce7141b9b95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3651325ade9048208a671b3c4a39684d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66245d45d99648b386ade7370d3cf28b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "72cdcfc03440460582d36a6e9061757a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2724ebddf2d04c6da3544ce7141b9b95",
            "placeholder": "",
            "style": "IPY_MODEL_a477bb3e2ba44ffeb04630de6ee8ebaf",
            "value": "2496/2496[01:27&lt;00:00,42.76/s]"
          }
        },
        "8e5d34f526604ad399f4421b25b0046e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bad72c67668b4b4fb98d82c2414016e7",
            "placeholder": "",
            "style": "IPY_MODEL_3651325ade9048208a671b3c4a39684d",
            "value": "Fetchingtable...:100%"
          }
        },
        "925ef98985ac444f924fc106df54c484": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e5d34f526604ad399f4421b25b0046e",
              "IPY_MODEL_1224d4ca62df4f25a371f72d532b2335",
              "IPY_MODEL_72cdcfc03440460582d36a6e9061757a"
            ],
            "layout": "IPY_MODEL_d31935f7d621472296919548f100deaf"
          }
        },
        "a477bb3e2ba44ffeb04630de6ee8ebaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bad72c67668b4b4fb98d82c2414016e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d31935f7d621472296919548f100deaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f794b4dbdf3f40caacf0010c7ea80fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
